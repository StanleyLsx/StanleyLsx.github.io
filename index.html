<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-The probability of a sentence Recurrent Neural Networks and Language Models" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/" class="article-date">
  <time datetime="2020-04-30T15:54:16.022Z" itemprop="datePublished">2020-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/">06 The probability of a sentence? Recurrent Neural Networks and Language Models</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h2><p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image01.png" alt></p>
<p>语言模型就是预测一个句子中下一个词的概率分布。如上图所示，假设给定一个句子前缀是the students opened their，语言模型预测这个句子片段下一个词是books、laptops、exams、minds或者其他任意一个词的概率。形式化表示就是计算。</p>
<script type="math/tex; mode=display">
P(x^{(t+1)}|x^{(t)},...,x^{(1)})</script><p>$x^{(t+1)}$表示第$t+1$个位置(时刻)的词是$x$，$x$可以是词典$V$中任意的一个词。</p>
<p>例如有一段文本$x^{(1)}$,…,$x^{(T)}$，则这段文本的概率(根据语言模型)为</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(x^{(1)},...,x^{(T)})&=P(x^{(1)})\times P(x^{(2)}|x^{(1)})\times...\times P(x^{(T)}|x^{(T-1)},...,x^{(1)})
\\&=\prod ^T_{t=1}P(x^{(T)}|x^{(T-1)},...,x^{(1)})
\end{aligned}</script><p>语言模型可以用在输入法中预测下一个将要输入的词，在谷歌搜索中输入前几个关键词，搜索引擎会自动预测接下来可能的几个词。网上有很多智能AI自动生成新闻、诗歌等等。可以说语言模型是很多NLP任务的基础模块，具有非常重要的作用。</p>
<h3 id="n-gram-Language-Models"><a href="#n-gram-Language-Models" class="headerlink" title="n-gram Language Models"></a>n-gram Language Models</h3><p>在前-深度学习时代，人们使用n-gram方法来学习语言模型。对于一个句子，n-gram表示句子中连续的n个词，n-gram对于n=1,2,3,4的结果是：</p>
<ul>
<li>unigrams: “the”, “students”, “opened”, ”their”</li>
<li>bigrams: “the students”, “students opened”, “opened their”</li>
<li>trigrams: “the students opened”, “students opened their”</li>
<li>4-grams: “the students opened their”</li>
</ul>
<p>n-gram方法有一个前提假设，即假设每个词出现的概率只和前n-1个词有关。n-gram的计算方法就是，统计语料库中出现$x^{(t)},…,x^{(t-n+2)}$的次数作为分母，以及在这个基础上再接一个词$x^{(t+1)}$的次数$x^{(t+1)},x^{(t)},…,x^{(t-n+2)}$作为分子，用后者除以前者来近似这个条件概率。</p>
<p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image02.png" alt></p>
<p>举个例子，假设完整的句子是as the proctor started the clock, the students opened their，需要预测下一个词的概率分布。对于4-gram方法，则只有students opened their对下一个词有影响，前面的词都没有影响。然后我们统计训练集语料库中发现，分母students opened their出现1000次，其后接books即students opened their books出现了400次，所以P(books|students opened their)=400/1000=0.4。类似的，可以算出下一个词为exams的概率是0.1。所以4-gram方法认为下一个词是books的概率更大。</p>
<p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image03.png" alt></p>
<p>n-gram方法在统计语料库中的n-gram时，对词的顺序是有要求的，即必须要和给定的n-gram的顺序一样才能对频数加1，比如这个例子中只有出现和students opened their顺序一样才行，如果是their students opened则不行。</p>
<p>n-gram方法虽然能够有效，比如对于上面的例子，预测出books和exams看起来和前面几个词搭配得很好；但是，它有不少的问题，还是上面的例子，其实考虑更前面的词proctor以及clock的话，这很明显是考试场景，后面出现exams的概率应该比books更高才对。</p>
<p>具体来说，n-gram方法有以下不足：</p>
<ul>
<li>考虑的状态有限。n-gram只能看到前n-1个词，无法建模长距离依赖关系，上面就是一个很好的例子。</li>
<li>稀疏性问题。对于一个稀有的(不常见的)词w，如果他的词组没有在语料库中出现，则分子为0，但w很有可能是正确的，概率至少不是0。比如students opened their petri dishes，对于学生物的学生来说是有可能的，但如果students opened their petri dishes没有在语料库中出现的话，petri dishes的概率就被预测为0了，这是不合理的。当然这个问题可以通过对词典中所有可能的词组频率+1平滑来部分解决。</li>
<li>更严重的稀疏性问题，如果分母的词组频率在语料库中是0，那么所有词w对应的分子的词组频率就是0了，根本就没法计算概率。这种情况只能使用back-off策略，即如果4-gram太过于稀疏了，则降到3-gram，分母只统计opened their的频率。一般的，虽然n-gram中的n越大，语言模型预测越准确，但其稀疏性越严重。n其实就相当于维度，我们知道在空间中，维度越高越稀疏，高维空间非常稀疏。对于n-gram，一般取n&lt;=5。</li>
<li>存储问题，需要存储所有n-gram的频率，如果n越大，这种n-gram的组合越多，所以存储空间呈幂次上升。</li>
</ul>
<p>下面是一个更直观的trigram稀疏性问题的例子，由于语料库中统计到的today the company和today the bank的词组频率相同，导致company和bank算出来的概率相等，无法区分。就是因为这两个trigram在预料中出现都比较少，很稀疏，导致统计数据难以把他们区分开来。</p>
<p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image04.png" alt></p>
<h3 id="A-fixed-window-neural-Language-Model"><a href="#A-fixed-window-neural-Language-Model" class="headerlink" title="A fixed-window neural Language Model"></a>A fixed-window neural Language Model</h3><p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image05.png" alt></p>
<p>window-based neural model在第三讲中被用于NER问题，方法是对一个词开一个小窗口，然后利用词向量和全连接网络识别词的类别。仿照这个方法，也可以用基于窗口的方法来学习语言模型。</p>
<p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image06.png" alt></p>
<p>超越n-gram语言模型的改进有： </p>
<ul>
<li>没有稀疏性问题，它不要求语料库中出现n-gram的词组，它仅仅是把每个独立的单词的词向量组合起来。只要有词向量，就有输入，至少整个模型能顺利跑通。</li>
<li>不需要观察到所有的n-grams，节省存储空间，只需要存储每个独立的词的词向量。</li>
</ul>
<p>存在的问题： </p>
<ul>
<li>固定窗口太小，受限于窗口大小，不能感知远距离的关系。</li>
<li>扩大窗口就需要扩大权重矩阵$W$，导致网络变得复杂。</li>
<li>输入$e^{(1)},…,e^{(4)}$对应$W$的不同列，每个$e$对应的权重完全是独立的，没有共享关系，导致训练效率比较低。</li>
</ul>
<p>我们需要一个神经结构，可以处理任何长度的输入</p>
<h3 id="RNN-Language-Model"><a href="#RNN-Language-Model" class="headerlink" title="RNN Language Model"></a>RNN Language Model</h3><h3 id="Evaluating-Language-Models"><a href="#Evaluating-Language-Models" class="headerlink" title="Evaluating Language Models"></a>Evaluating Language Models</h3><h3 id="Why-should-we-care-about-Language-Modeling"><a href="#Why-should-we-care-about-Language-Modeling" class="headerlink" title="Why should we care about Language Modeling?"></a>Why should we care about Language Modeling?</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/" data-id="ck9myomwc0003hov8356k5hpk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" rel="tag">cs224n课程</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Linguistic Structure Dependency Parsing" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/" class="article-date">
  <time datetime="2020-04-30T15:52:48.492Z" itemprop="datePublished">2020-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/">05 Linguistic Structure Dependency Parsing</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>对于句法结构(syntactic structure)分析，主要有两种方式：Constituency Parsing与Dependency Parsing</p>
<h2 id="Constituency-Parsing"><a href="#Constituency-Parsing" class="headerlink" title="Constituency Parsing"></a>Constituency Parsing</h2><p>Constituency Parsing主要用phrase structure grammer即短语语法来不断的将词语整理成嵌套的组成成分，又被称为context-free grammers，简写做CFG<br>其主要步骤是先对每个词做词性分析part of speech, 简称POS，然后再将其组成短语，再将短语不断递归构成更大的短语</p>
<p><img src="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/image01.png" alt></p>
<p>例如，对于 the cuddly cat by the door, 先做POS分析，the是限定词，用Det(Determiner)表示，cuddly是形容词，用Adj(Adjective)代表，cat和door是名词，用N(Noun)表示, by是介词，用P(Preposition)表示。<br>然后the cuddly cat构成名词短语NP(Noun Phrase)，这里由Det(the)+Adj(cuddly)+N(cat)构成，by the door构成介词短语PP(Preposition Phrase), 这里由P(by)+NP(the door)构成。<br>最后，整个短语the cuddly cat by the door 是NP，由NP（the cuddly cat）+ PP(by the door)构成。</p>
<h2 id="Dependency-Parsing"><a href="#Dependency-Parsing" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h2><p>Dependency Structure展示了词语之前的依赖关系,通常用箭头表示其依存关系，有时也会在箭头上标出其具体的语法关系，如是主语还是宾语关系等。<br>Dependency Structure有两种表现形式，一种是直接在句子上标出依存关系箭头及语法关系，如 ：  </p>
<p><img src="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/image02.png" alt></p>
<p>另一种是将其做成树状机构（Dependency Tree Graph）</p>
<p>Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas<br>堪萨斯州共和党参议员布朗巴克(Brownback)提交了有关港口和移民的法案</p>
<p><img src="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/image03.png" alt></p>
<p>Dependency Parsing可以看做是给定输入句子$S=w_0w_1…w_n$（其中$w_0$常常是fake ROOT，使得句子中每一个词都依赖于另一个节点）构建对应的Dependency Tree Graph的任务。而这个树如何构建呢？一个有效的方法是Transition-based Dependency Parsing</p>
<h3 id="Transition-based-Dependency-Parsing"><a href="#Transition-based-Dependency-Parsing" class="headerlink" title="Transition-based Dependency Parsing"></a>Transition-based Dependency Parsing</h3><p>Transition-based Dependency Parsing可以看做是状态机，对于$S=w_0w_1…w_n$，其状态由三部分构成$(\sigma,\beta,A)$<br>$\sigma$是$S$中若干$w_i$构成的堆(stack)<br>$\beta$是$S$中若干$w_i$构成的缓冲(buffer)<br>$A$是$w_i$之间的关系依存弧构成的集合，每一条边的形式是$(w_i,r,w_j)$，其中$r$描述了节点的依存关系(如动宾关系等)<br>初始状态时，$\sigma$仅包含ROOT$w_0$，$\beta$包含了所有的单词$w_1…w_n$，而$A$是空集$\varnothing$。最终的目标是$\sigma$包含ROOTT$w_0$，$\beta$清空，而$A$包含了所有关系， $A$就是我们想要的描述Dependency的结果<br>其含义是对于$S$中所有的单词都找到了相互的关系<br>状态之间的转移有三类 ： </p>
<ul>
<li>移除在缓冲区的第一个单词，然后将其放在堆的顶部（前提条件：缓冲区不能为空）。</li>
<li>LEFT-ARC：向依存弧集合$A$中加入一个依存弧$(w_j,r,w_i)$，其中$w_i$是堆顶的第二个单词，$w_j$堆顶部的单词。从堆栈中移除$w_i$。</li>
<li>RIGHT-ARC:向依存弧集合$A$中加入一个依存弧$(w_i,r,w_j)，其中$w_i$是堆顶的第二个单词，$w_j$堆顶部的单词。从堆栈中移除$w_j$。<br>我们不断的进行上述三类操作，直到从初始态达到最终态。    </li>
</ul>
<p>在每个状态下如何选择哪种操作呢？  </p>
<ul>
<li>当我们考虑到LEFT-ARC与RIGHT-ARC各有|R|(|R|为r的类的个数)种类别，我们可以将其看做是类别数为2|R|+1的分类问题。</li>
<li>每一个stack+buffer的状态相当于输入，3种操作相当于输出，把这个问题建模成分类问题。于是Nivre等人对每一个stack+buffer的状态，人工抽取出很多的特征，然后使用logistic或者svm进行分类。但是，当时的特征设计都是0/1状态的，特征向量很稀疏；特征又多，抽取特征很花时间。</li>
</ul>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>当我们有了Dependency Parsing的模型后，我们如何对其准确性进行评估呢？<br>我们有两个metric，一个是<strong>LAS(labeled attachment score)</strong>即只有arc的箭头方向以及语法关系均正确时才算正确，以及<strong>UAS(unlabeled attachment score)</strong>即只要arc的箭头方向正确即可。</p>
<p><img src="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/image04.png" alt></p>
<h3 id="Neural-Dependency-Parsing"><a href="#Neural-Dependency-Parsing" class="headerlink" title="Neural Dependency Parsing"></a>Neural Dependency Parsing</h3><p>传统的Transition-based Dependency Parsing对特征工程要求较高，我们可以用神经网络来减少人力劳动</p>
<p>对于Neural Dependency Parser，其输入特征通常包含三种：</p>
<ul>
<li>stack和buffer中的单词及其dependent word。</li>
<li>单词的Part-of-Speech tag。</li>
<li>描述语法关系的arc label。</li>
</ul>
<p><img src="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/image05.png" alt></p>
<p>当神经网络火了之后，人们自然想到了用神经网络替代logistic或svm，提出了新的句法分析器。他们对于每一个stack+buffer的状态，抽取出words、POS tags和arc labels三种不同类型的特征，都用词向量来表示。然后输入只有一个隐层的全连接网络，效果立马超过了之前所有人工设计的特征和方法。基于这个工作，后续又有很多改进版本。</p>
<p><img src="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/image06.png" alt></p>
<p>利用这样简单的前置神经网络，我们就可以减少特征工程并提高准确度，当然，RNN模型也可以应用到Dependency Parsing任务中。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/" data-id="ck9myomw40000hov86y5337an" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" rel="tag">cs224n课程</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Matrix Calculus and Backpropagation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/" class="article-date">
  <time datetime="2020-04-30T15:50:12.656Z" itemprop="datePublished">2020-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/">04 Matrix Calculus and Backpropagation</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h2><p>该部分介绍了雅可比矩阵、链式求导法则，最后推算出下图的输出得分$s$对$W$和$b$的求导结果。</p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image01.png" alt></p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image02.png" alt></p>
<p>根据链式求导法则 : </p>
<script type="math/tex; mode=display">\frac{\partial s}{\partial b}=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial b}=u^Tdiag(f'(z))I=u^T \cdot f'(z)</script><script type="math/tex; mode=display">\frac{\partial s}{\partial W}=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial W}</script><p>令$\delta=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}=u^T \cdot f’(z)$，$\delta$是局部误差符号，则</p>
<script type="math/tex; mode=display">\frac{\partial s}{\partial W}=\delta\frac{\partial z}{\partial W}=\delta^T x^T</script><script type="math/tex; mode=display">\frac{\partial s}{\partial b}=\delta\frac{\partial z}{\partial b}=\delta</script><h3 id="Derivative-with-respect-to-Matrix-Output-shape"><a href="#Derivative-with-respect-to-Matrix-Output-shape" class="headerlink" title="Derivative with respect to Matrix: Output shape"></a>Derivative with respect to Matrix: Output shape</h3><p>$W \in \mathbb{R}^{n \times m},\frac{\partial s}{\partial W}$的形状是啥?<br>我们遵循导数的形状是参数的形状的规则，可以看到它是一个$n \times m$的雅可比矩阵</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
\frac{\partial s}{\partial W_{11}} & \cdots  & \frac{\partial s}{\partial W_{1m}}\\ 
\vdots  & \ddots  & \vdots \\ 
\frac{\partial s}{\partial W_{n1}} & \cdots  & \frac{\partial s}{\partial W_{nm}}
\end{bmatrix}</script><h3 id="Why-the-Transposes"><a href="#Why-the-Transposes" class="headerlink" title="Why the Transposes?"></a>Why the Transposes?</h3><p>为什么$\frac{\partial s}{\partial W}=\delta^T x^T$中$\delta$是转置?</p>
<script type="math/tex; mode=display">
\frac{\partial s}{\partial W}=\delta^T x^T=\begin{bmatrix}
\delta_1 \\ 
\vdots \\ 
\delta_n
\end{bmatrix}
[x_1,...,x_m]=\begin{bmatrix}
\delta_1x_1 & \cdots  & \delta_1x_m\\ 
\vdots  & \ddots  & \vdots \\ 
\delta_nx_1 & \cdots  & \delta_nx_m
\end{bmatrix}</script><h2 id="Deriving-local-input-gradient-in-backprop"><a href="#Deriving-local-input-gradient-in-backprop" class="headerlink" title="Deriving local input gradient in backprop"></a>Deriving local input gradient in backprop</h2><p>通过神经网络展开的图计算$\frac{\partial s}  {\partial W}$</p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image03.png" alt></p>
<script type="math/tex; mode=display">\frac{\partial s}{\partial W}=\delta\frac{\partial z}{\partial W}=\delta  \frac{\partial}{\partial W}Wx+b</script><p>我们只考虑$W_{ij}$的导数，$W_{ij}$只对$z_i$有贡献，例如$W_{23}$只对$z_2$有贡献，对$z_1$没有贡献，可推导出 ： </p>
<script type="math/tex; mode=display">
\frac{\partial z_i}{\partial W_{ij}}=\delta  \frac{\partial}{\partial W_{ij}}W_ix+b_i=\frac{\partial}{\partial W_{ij}}\sum^d_{k=1}W_{ik}x_k=x_j</script><h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><p>前向传播(从左至右计算)</p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image04.png" alt></p>
<script type="math/tex; mode=display">s=u^Th</script><script type="math/tex; mode=display">h=f(z)</script><script type="math/tex; mode=display">z=Wx+b</script><script type="math/tex; mode=display">x(input)</script><p>后向传播(从右至左传递导数)</p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image05.png" alt></p>
<h3 id="Backpropagation-Single-Node"><a href="#Backpropagation-Single-Node" class="headerlink" title="Backpropagation: Single Node"></a>Backpropagation: Single Node</h3><p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image06.png" alt></p>
<ul>
<li>节点接收“上游梯度”</li>
<li>目标是传递正确的“下游梯度”</li>
<li>每个节点都有<strong>局部梯度local gradient</strong>，它输出的梯度是与它的输入有关</li>
<li>[downstream gradient] = [upstream gradient] x [local gradient]</li>
</ul>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image07.png" alt></p>
<ul>
<li>多个输入对应多个局部梯度<h3 id="An-Example"><a href="#An-Example" class="headerlink" title="An Example"></a>An Example</h3></li>
</ul>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image08.png" alt></p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image09.png" alt></p>
<script type="math/tex; mode=display">\frac{\partial f}{\partial x}=\frac{\partial f}{\partial a}\frac{\partial a}{\partial x}=2</script><script type="math/tex; mode=display">\frac{\partial f}{\partial y}=\frac{\partial f}{\partial a}\frac{\partial a}{\partial y}+\frac{\partial f}{\partial b}\frac{\partial b}{\partial y}=2+3=5</script><script type="math/tex; mode=display">\frac{\partial f}{\partial z}=0</script><h3 id="Efficiency-compute-all-gradients-at-once"><a href="#Efficiency-compute-all-gradients-at-once" class="headerlink" title="Efficiency: compute all gradients at once"></a>Efficiency: compute all gradients at once</h3><p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image10.png" alt></p>
<ul>
<li>绿的线的部分导数可以共用以此减少计算</li>
</ul>
<h3 id="Back-Prop-in-General-Computation-Graph"><a href="#Back-Prop-in-General-Computation-Graph" class="headerlink" title="Back-Prop in General Computation Graph"></a>Back-Prop in General Computation Graph</h3><p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image11.png" alt></p>
<ol>
<li>Fprop：按拓扑排序顺序访问节点<ul>
<li>计算给定父节点的节点的值</li>
</ul>
</li>
<li>Bprop：<ul>
<li>初始化输出梯度为 1</li>
<li>以相反的顺序方位节点，使用节点的后继的梯度来计算每个节点的梯度</li>
<li>$\{y_1,y_2,…,y_n\}$是$x$的后继</li>
<li>$\frac{\partial z}{\partial x}=\sum^n_1\frac{\partial z}{\partial y_i}\frac{\partial y_i}{\partial x}$</li>
<li>正确地说，Fprop 和 Bprop 的计算复杂度是一样的</li>
<li>一般来说，我们的网络有固定的层结构，所以我们可以使用矩阵和雅可比矩阵</li>
</ul>
</li>
</ol>
<h3 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a>Automatic Differentiation</h3><p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image12.png" alt></p>
<ul>
<li>梯度计算可以从 Fprop 的符号表达式中自动推断</li>
<li>每个节点类型需要知道如何计算其输出，以及如何在给定其输出的梯度后计算其输入的梯度</li>
<li>现代DL框架(Tensorflow, Pytoch)为您做反向传播，但主要是令作者手工计算层/节点的局部导数</li>
</ul>
<h2 id="Notes-03-Neural-Networks-Backpropagation"><a href="#Notes-03-Neural-Networks-Backpropagation" class="headerlink" title="Notes 03 Neural Networks, Backpropagation"></a>Notes 03 Neural Networks, Backpropagation</h2><h3 id="Neural-Networks-Foundations"><a href="#Neural-Networks-Foundations" class="headerlink" title="Neural Networks: Foundations"></a>Neural Networks: Foundations</h3><p>该部分涉及到DL中的W和b的更新，已经掌握</p>
<h3 id="Neural-Networks-Tips-and-Tricks"><a href="#Neural-Networks-Tips-and-Tricks" class="headerlink" title="Neural Networks: Tips and Tricks"></a>Neural Networks: Tips and Tricks</h3><p>此部分已经在我的hit里面的<a href="https://github.com/StanleyLsx/image_classification" target="_blank" rel="noopener">image_classification</a>中学习到</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/" data-id="ck9myomw90001hov8fw9i92av" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" rel="tag">cs224n课程</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Word Window Classification, Neural Networks, and PyTorch" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/" class="article-date">
  <time datetime="2020-04-30T15:46:55.264Z" itemprop="datePublished">2020-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/">03 Word Window Classification, Neural Networks, and PyTorch</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Classification-setup-and-notation"><a href="#Classification-setup-and-notation" class="headerlink" title="Classification setup and notation"></a>Classification setup and notation</h2><p>通常我们有由样本组成的训练数据集</p>
<script type="math/tex; mode=display">
\{x_i,y_i\}_{i=1}^N</script><p>$x_i$是输入，例如单词(索引或是向量)，句子，文档等，维度为$d$<br>$y_i$是我们尝试预测的标签($C$各类别中的一个)，例如 :   </p>
<ul>
<li>类别：感情，命名实体，购买/售出的决定</li>
<li>其他单词</li>
<li>之后：多词序列的<h3 id="Classification-intuition"><a href="#Classification-intuition" class="headerlink" title="Classification intuition"></a>Classification intuition</h3></li>
</ul>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image01.png" alt></p>
<p>训练数据 : $\{x_i,y_i\}_{i=1}^N$<br>简单的说明情况   </p>
<ul>
<li>固定的二维单词向量分类</li>
<li>使用softmax/logistic回归</li>
<li>线性决策边界</li>
</ul>
<p><strong>传统的机器学习/统计学方法：</strong> 假设$x_i$是固定的，训练softmax/logistic回归的权重$W\in \mathbb{R}^{C\times d}$来决定决策边界(超平面)<br>方法 : 对每个$x$，预测</p>
<script type="math/tex; mode=display">
p(y|x)=\frac{exp(W_y\cdot x)}{\sum_{c=1}^Cexp(W_c\cdot x)}</script><p>我们可以将预测函数分为两个步骤：  </p>
<ol>
<li>将$W$的第$y$行和$x$中的对应行相乘得到分数，计算所有的$fc,for  c=1,…,C$<script type="math/tex; mode=display">
W_y\cdot x=\sum^d_{i=1}W_{yi}x_i=f_y</script></li>
<li>使用softmax函数获得归一化的概率<script type="math/tex; mode=display">
p(y|x)=\frac{exp(f_y)}{\sum^C_{c=1}exp(f_c)}</script><h3 id="Training-with-softmax-and-cross-entropy-loss"><a href="#Training-with-softmax-and-cross-entropy-loss" class="headerlink" title="Training with softmax and cross-entropy loss"></a>Training with softmax and cross-entropy loss</h3>对于每个训练样本$(x,y)$，我们的目标是最大化正确类$y$的概率，或者我们可以最小化该类的负对数概率  <script type="math/tex; mode=display">
-logp(y|x)=-log(\frac{exp(f_y)}{})</script><h3 id="Background-What-is-“cross-entropy”-loss-error"><a href="#Background-What-is-“cross-entropy”-loss-error" class="headerlink" title="Background: What is “cross entropy” loss/error?"></a>Background: What is “cross entropy” loss/error?</h3></li>
</ol>
<ul>
<li>交叉熵的概念来源于信息论，衡量两个分布之间的差异</li>
<li>令真实概率分布为$p$</li>
<li>令我们计算的模型概率为$q$</li>
<li>交叉熵为  <script type="math/tex; mode=display">
H(p,q)=-\sum^C_{c=1}p(c)logq(c)</script></li>
<li>假设groud truth(or true or gold or target)的概率分布在正确的类上为1，在其他任何地方为0 ：$p=[0,…,0,1,0,…0]$ </li>
<li>因为p是one-hot向量，所以唯一剩下的项是真实类的负对数概率</li>
</ul>
<h3 id="Classification-over-a-full-dataset"><a href="#Classification-over-a-full-dataset" class="headerlink" title="Classification over a full dataset"></a>Classification over a full dataset</h3><p>在整个数据集$\{x_i,y_i\}_{i=1}^N$上的交叉熵损失函数，是所有样本的交叉熵的均值  </p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{N}\sum^N_{i=1}-log(\frac{e^{f_{yi}}}{\sum^C_{c=1}e^{f_c}})</script><p>我们不使用</p>
<script type="math/tex; mode=display">
f_y=f_y(x)=W_y\cdot x=\sum^d_{j=1}W_{yj}x_j</script><p>我们使用矩阵来表示$f$</p>
<script type="math/tex; mode=display">f=Wx</script><h3 id="Traditional-ML-optimization"><a href="#Traditional-ML-optimization" class="headerlink" title="Traditional ML optimization"></a>Traditional ML optimization</h3><ul>
<li>一般机器学习的参数$\theta$通常只由W的列组成<script type="math/tex; mode=display">
\theta=[W_1,...,W_d]=W(:)^T\in \mathbb{R}^{Cd}</script></li>
<li>因此，我们只通过以下方式更新决策边界<script type="math/tex; mode=display">
\bigtriangledown_{\theta}J(\theta)=[\bigtriangledown_{W_1},...,\bigtriangledown_{W_d}]^T\in \mathbb{R}^{Cd}</script><h2 id="Neural-Network-Classifiers"><a href="#Neural-Network-Classifiers" class="headerlink" title="Neural Network Classifiers"></a>Neural Network Classifiers</h2></li>
</ul>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image02.png" alt></p>
<ul>
<li>单独使用Softmax(≈logistic回归)并不十分强大</li>
<li>Softmax只给出线性决策边界  <ul>
<li>这可能是相当有限的，当问题很复杂时是无用的</li>
<li>纠正这些错误不是很酷吗?</li>
</ul>
</li>
</ul>
<h3 id="Neural-Nets-for-the-Win"><a href="#Neural-Nets-for-the-Win" class="headerlink" title="Neural Nets for the Win!"></a>Neural Nets for the Win!</h3><p>神经网络可以学习更复杂的函数和非线性决策边界</p>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image03.png" alt></p>
<p>更高级的分类需要</p>
<ul>
<li>词向量</li>
<li>更深层次的深层神经网络<h3 id="Classification-difference-with-word-vectors"><a href="#Classification-difference-with-word-vectors" class="headerlink" title="Classification difference with word vectors"></a>Classification difference with word vectors</h3>一般在NLP深度学习中</li>
<li>我们学习了矩阵$W$和词向量$x$</li>
<li>我们学习传统参数和表示</li>
<li>词向量是对one-hot向量的重新表示，在中间层向量空间中移动它们，以便使用(线性)softmax分类器通过x = Le层进行分类  <ul>
<li>即将词向量理解为一层神经网络，输入单词的one-hot向量并获得单词的词向量表示，并且我们需要对其进行更新。其中，$Vd$是数量很大的参数<script type="math/tex; mode=display">
\bigtriangledown_{\theta}J(\theta)=[\bigtriangledown_{W_1},...,\bigtriangledown_{dardvark},...\bigtriangledown_{W_d}]^T\in \mathbb{R}^{Cd+Vd}</script><h3 id="Neural-computation"><a href="#Neural-computation" class="headerlink" title="Neural computation"></a>Neural computation</h3></li>
</ul>
</li>
</ul>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image04.png" alt></p>
<h3 id="A-neuron-can-be-a-binary-logistic-regression-unit"><a href="#A-neuron-can-be-a-binary-logistic-regression-unit" class="headerlink" title="A neuron can be a binary logistic regression unit"></a>A neuron can be a binary logistic regression unit</h3><p>$f$为非线性激活函数(例如sigmoid函数)，$w$为权重向量，$b$为偏置向量，$h$为隐藏层变量对应的向量，$x$为输入向量</p>
<script type="math/tex; mode=display">
h_{w,b}=f(w^Tx+b)</script><script type="math/tex; mode=display">
f(z)=\frac{1}{1+e^{-z}}</script><p>$b$ : 我们可以有一个“总是打开”的特性，它给出一个先验类，或者将它作为一个偏向项分离出来<br>$w,b$是神经元的参数</p>
<h3 id="A-neural-network-running-several-logistic-regressions-at-the-same-time"><a href="#A-neural-network-running-several-logistic-regressions-at-the-same-time" class="headerlink" title="A neural network = running several logistic regressions at the same time"></a>A neural network = running several logistic regressions at the same time</h3><p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image05.png" alt></p>
<p>如果我们输入一个向量通过一系列逻辑回归函数，那么我们得到一个输出向量，但是我们不需要提前决定这些逻辑回归试图预测的变量是什么。</p>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image06.png" alt></p>
<p>我们可以输入另一个logistic回归函数。损失函数将指导中间隐藏变量应该是什么，以便更好地预测下一层的目标。我们当然可以使用更多层的神经网络。</p>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image07.png" alt></p>
<h3 id="Matrix-notation-for-a-layer"><a href="#Matrix-notation-for-a-layer" class="headerlink" title="Matrix notation for a layer"></a>Matrix notation for a layer</h3><p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image08.png" alt></p>
<p>我们有</p>
<script type="math/tex; mode=display">a_1=f(W_11x_1+W_12x_2+W_13x_3+b_1)</script><script type="math/tex; mode=display">a_2=f(W_21x_1+W_22x_2+W_23x_3+b_2)</script><p>通过矩阵表示的运算有</p>
<script type="math/tex; mode=display">z=Wx+b</script><script type="math/tex; mode=display">a=f(z)</script><p>激活函数$f$在运算时是element-wise逐元素的</p>
<script type="math/tex; mode=display">f([z_1,z_2,z_3])=[f(z_1),f(z_2),f(z_3)]</script><h3 id="Non-linearities-aka-“f-”-Why-they’re-needed"><a href="#Non-linearities-aka-“f-”-Why-they’re-needed" class="headerlink" title="Non-linearities (aka “f ”): Why they’re needed"></a>Non-linearities (aka “f ”): Why they’re needed</h3><p>例如：函数近似，如回归或分类</p>
<ul>
<li>没有非线性，深度神经网络只能做线性变换</li>
<li>多个线性变换可以组成一个的线性变换$W_1W_2x=Wx$，因为线性变换是以某种方式旋转和拉伸空间，多次的旋转和拉伸可以融合为一次线性变换</li>
<li>对于非线性函数而言，使用更多的层，他们可以近似更复杂的函数</li>
</ul>
<h2 id="Named-Entity-Recognition-NER"><a href="#Named-Entity-Recognition-NER" class="headerlink" title="Named Entity Recognition (NER)"></a>Named Entity Recognition (NER)</h2><ul>
<li>任务：例如，查找和分类文本中的名称</li>
</ul>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image09.png" alt></p>
<ul>
<li>可能的用途 : <ul>
<li>跟踪文档中提到的特定实体（组织、个人、地点、歌曲名、电影名等）</li>
<li>对于问题回答，答案通常是命名实体</li>
<li>许多需要的信息实际上是命名实体之间的关联</li>
<li>同样的技术可以扩展到其他 slot-filling 槽填充分类</li>
</ul>
</li>
<li>通常后面是命名实体链接/规范化到知识库</li>
</ul>
<h3 id="Named-Entity-Recognition-on-word-sequences"><a href="#Named-Entity-Recognition-on-word-sequences" class="headerlink" title="Named Entity Recognition on word sequences"></a>Named Entity Recognition on word sequences</h3><p>我们通过在上下文中对单词进行分类，然后将实体提取为单词子序列来预测实体</p>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image10.png" alt></p>
<h3 id="Why-might-NER-be-hard"><a href="#Why-might-NER-be-hard" class="headerlink" title="Why might NER be hard?"></a>Why might NER be hard?</h3><ul>
<li><p>很难计算出实体的边界</p>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image11.png" alt><br>第一个实体是 “First National Bank” 还是 “National Bank”</p>
</li>
<li>很难知道某物是否是一个实体，是一所名为“Future School” 的学校，还是这是一所未来的学校？</li>
<li><p>很难知道未知/新奇实体的类别</p>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image12.png" alt><br>“Zig Ziglar” ? 一个人</p>
</li>
<li><p>实体类是模糊的，依赖于上下文</p>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image13.png" alt><br>这里的“Charles Schwab” 是 PER 不是 ORG</p>
</li>
</ul>
<h2 id="Word-window-classification"><a href="#Word-window-classification" class="headerlink" title="Word window classification"></a>Word window classification</h2><ul>
<li>思想：在<strong>相邻词的上下文窗口</strong>中对一个词进行分类</li>
<li>例如，上下文中一个单词的命名实体分类(人、地点、组织、NONE)</li>
<li>在上下文中对单词进行分类的一个简单方法可能是对窗口中的单词向量进行<strong>平均</strong>，并对平均向量进行分类(问题是这会丢失单词位置信息)</li>
</ul>
<h3 id="Window-classification-Softmax"><a href="#Window-classification-Softmax" class="headerlink" title="Window classification: Softmax"></a>Window classification: Softmax</h3><ul>
<li>训练softmax分类器对中心词进行分类，方法是在一个窗口内<strong>将中心词周围的词向量串联起来</strong></li>
<li>例子：在这句话的上下文中对“Paris”进行分类，窗口长度为2</li>
</ul>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image14.png" alt></p>
<ul>
<li>结果$x_{window}=x\in \mathbb{R}^{5d}$是一个列向量</li>
</ul>
<h3 id="Simplest-window-classifier-Softmax"><a href="#Simplest-window-classifier-Softmax" class="headerlink" title="Simplest window classifier: Softmax"></a>Simplest window classifier: Softmax</h3><p>对于$x=x_{window}$，我们可以使用与之前相同的softmax分类器</p>
<p><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image15.png" alt></p>
<ul>
<li>如何更新向量？</li>
<li>简而言之：求导和优化</li>
</ul>
<h3 id="Slightly-more-complex-Multilayer-Perceptron"><a href="#Slightly-more-complex-Multilayer-Perceptron" class="headerlink" title="Slightly more complex: Multilayer Perceptron"></a>Slightly more complex: Multilayer Perceptron</h3><ul>
<li>在我们的softmax分类器中引入一个附加的非线性层。</li>
<li>Multilayer Perceptron(mlp)是更复杂的神经系统的基本构件!</li>
<li>假设我们要对中心词是否为一个地点，进行分类</li>
<li>与word2vec类似，我们将遍历语料库中的所有位置。但这一次，它将受到监督，只有一些位置能够得到高分，在他们的中心有一个实际的NER Location的位置是“真实的”位置会获得高分<h3 id="Neural-Network-Feed-forward-Computation"><a href="#Neural-Network-Feed-forward-Computation" class="headerlink" title="Neural Network Feed-forward Computation"></a>Neural Network Feed-forward Computation</h3>使用神经激活$a$简单地给出一个非标准化的分数<script type="math/tex; mode=display">score(x)=U^Ta \in R</script>我们用一个三层神经网络计算一个窗口的得分<script type="math/tex; mode=display">s=score("museums in Paris are amazing”)</script><script type="math/tex; mode=display">s=U^Tf(Wx+b)</script><script type="math/tex; mode=display">x\in \mathbb{R}^{20 \times 1},W\in \mathbb{R}^{8 \times 20},U\in \mathbb{R}^{8 \times 1}</script><img src="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image16.png" alt></li>
</ul>
<h3 id="Main-intuition-for-extra-layer"><a href="#Main-intuition-for-extra-layer" class="headerlink" title="Main intuition for extra layer"></a>Main intuition for extra layer</h3><p>中间层学习输入词向量之间的<strong>非线性交互</strong><br>例如：只有当“museum”是第一个向量时，“in”放在第二个位置才重要</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/" data-id="ck9myomwm0005hov8af5oaefd" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" rel="tag">cs224n课程</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Introduction and Word Vectors" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/30/Introduction%20and%20Word%20Vectors/" class="article-date">
  <time datetime="2020-04-30T15:43:27.815Z" itemprop="datePublished">2020-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/30/Introduction%20and%20Word%20Vectors/">01 Introduction and Word Vectors</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Human-language-and-word-meaning"><a href="#Human-language-and-word-meaning" class="headerlink" title="Human language and word meaning"></a>Human language and word meaning</h2><h3 id="How-do-we-represent-the-meaning-of-a-word"><a href="#How-do-we-represent-the-meaning-of-a-word" class="headerlink" title="How do we represent the meaning of a word?"></a>How do we represent the meaning of a word?</h3><ul>
<li>用一个词、词组等表示的概念</li>
<li>一个人想用语言、符号等来表达的想法</li>
<li>表达在作品、艺术等方面的思想</li>
</ul>
<p>理解意义的最普遍的语言方式(<strong>linguistic way</strong>) : 语言符号与语言符号的意义的转化</p>
<h3 id="How-do-we-have-usable-meaning-in-a-computer"><a href="#How-do-we-have-usable-meaning-in-a-computer" class="headerlink" title="How do we have usable meaning in a computer?"></a>How do we have usable meaning in a computer?</h3><h4 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h4><p>它是一个包含同义词集和上位词(“is a”关系)<strong>synonym sets and hypernyms</strong>的列表的辞典  </p>
<p><img src="/2020/04/30/Introduction%20and%20Word%20Vectors/image01.png" alt>  </p>
<p>缺点 : 这种表达方式忽略了词在上下文中的语境，缺少新的含义，判断比较主观且无法计算单词间的相似度。</p>
<h4 id="离散向量"><a href="#离散向量" class="headerlink" title="离散向量"></a>离散向量</h4><p>最常见的入one-hot编码，传统的自然语言处理中，我们把词语看作离散的符号。单词可以通过独热向量(one-hot vectors，只有一个1，其余均为0的稀疏向量) 。向量维度=词汇量(如500,000)。  </p>
<script type="math/tex; mode=display">
motel = [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]</script><script type="math/tex; mode=display">
hotel = [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0]</script><p>缺点 :  所有向量是正交的。对于独热向量，没有关于相似性概念，并且向量维度过大。</p>
<h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><ul>
<li>一个单词的意思是由经常出现在它附近的单词给出的。</li>
<li>当一个单词$w$出现在文本中时，它的上下文是出现在其附近的一组单词(在一个固定大小的窗口中)。</li>
<li>使用$w$的上下文来构建$w$的表示</li>
</ul>
<p><img src="/2020/04/30/Introduction%20and%20Word%20Vectors/image02.png" alt></p>
<h2 id="Word2vec-introduction"><a href="#Word2vec-introduction" class="headerlink" title="Word2vec introduction"></a>Word2vec introduction</h2><p>为每个单词构建一个密集的向量，使其与出现在相似上下文中的单词向量相似，词向量<strong>word vectors</strong>有时被称为词嵌入<strong>word embeddings</strong>或词表示<strong>word representations</strong>  </p>
<p><img src="/2020/04/30/Introduction%20and%20Word%20Vectors/formula01.png" alt></p>
<p>Word2vec是一个学习单词向量的框架，它的思路如下 :</p>
<ul>
<li>有大量的文本</li>
<li>固定词汇表中的每个单词都由一个向量表示</li>
<li>文本中的每个位置$t$，其中有一个中心词$c$和上下文(“外部”)单词$o$</li>
<li>使用$c$和$o$的词向量的相似性来计算给定$c$的$o$的概率(反之亦然)</li>
<li>不断调整词向量来最大化这个概率</li>
</ul>
<p>下图是窗口大小为$j=2$时的$P(w_{t+j}|w_t)$计算过程，其中center word分别为<strong><em>into</em></strong>和<strong><em>banking</em></strong></p>
<p><img src="/2020/04/30/Introduction%20and%20Word%20Vectors/image03.png" alt></p>
<p><img src="/2020/04/30/Introduction%20and%20Word%20Vectors/image04.png" alt></p>
<h2 id="Word2vec-objective-function"><a href="#Word2vec-objective-function" class="headerlink" title="Word2vec objective function"></a>Word2vec objective function</h2><p>对于每个位置$t=1,…,T$，在大小为m的固定窗口内预测上下文单词，给定中心词$w_j$</p>
<script type="math/tex; mode=display">
L(\theta)=\prod_{t=1}^{T}\prod_{-m\leq j\leq m,j\neq 0}P(w_{t+j}|w_{t};\theta)</script><blockquote>
<p>其中，$\theta$为所有需要优化的变量</p>
</blockquote>
<p>损失函数$J(\theta)$是平均负对数似然</p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{T}logL(\theta)=-\frac{1}{T}\prod_{t=1}^{T}\prod_{-m\leq j\leq m,j\neq 0}logP(w_{t+j}|w_{t};\theta)</script><p>其中log形式是方便将连乘转化为求和，负号是希望将极大化似然率转化为极小化损失函数的等价问题。</p>
<blockquote>
<p>连乘转求和 : </p>
<script type="math/tex; mode=display">
log\prod_{i}x_{i}=\sum_{i}logx_{i}</script></blockquote>
<p>上述的$J(\theta)$可以看到我们要让损失函数最小则是让预测更准，即$P(w_{t+j}|w_{t};\theta)$越大</p>
<p>怎么计算$P(w_{t+j}|w_{t};\theta)$？<br>对于每个单词都是用两个向量，设当单词$w$为中心词的时候的向量为$v_w$，当单词$w$为上下文的某个词的时候向量为$u_w$，$V$为vocab，于是对于一个中心词$c$和一个上下文词$o$有 : </p>
<script type="math/tex; mode=display">
P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}</script><blockquote>
<p>公式中，向量$u_o$和向量$v_c$进行点乘。向量之间越相似，点乘结果越大，从而归一化后得到的概率值也越大。模型的训练正是为了使得具有相似上下文的单词，具有相似的向量。</p>
</blockquote>
<h2 id="Word2vec-prediction-function"><a href="#Word2vec-prediction-function" class="headerlink" title="Word2vec prediction function"></a>Word2vec prediction function</h2><script type="math/tex; mode=display">
P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}</script><p>上述公式公式中 :   </p>
<ul>
<li>取幂使任何数都为正</li>
<li>点积比较$o$和$c$的相似性，点积越大则概率越大</li>
<li>分母对整个词汇表进行标准化，从而给出概率分布</li>
</ul>
<p>将公式做实数集的同等映射$\mathbb{R}^n\rightarrow (0,1)^n$<br><strong>softmax function</strong> : </p>
<script type="math/tex; mode=display">
softmax(x_i)=\frac{exp(x_i)}{\sum_{j=1}^nexp(x_j)}=p_i</script><p>将任意值$x_i$映射到概率分布$p_i$</p>
<ul>
<li>max : 因为放大了最大的概率</li>
<li>soft : 因为仍然为较小的$x_i$赋予了一定概率</li>
<li>在深度学习中常用</li>
</ul>
<p><img src="/2020/04/30/Introduction%20and%20Word%20Vectors/image05.png" alt></p>
<p>随机初始化$u_x\in \mathbb{R}^d$和$v_w\in \mathbb{R}^d$，使用梯度下降法进行公式推导</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial}{\partial v_clog(o|c)}&=\frac{\partial}{\partial v_c}log\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}
\\&=\frac{\partial}{\partial v_c}(logexp(u_o^Tv_c)-log\sum_{w\in V}exp(u_w^Tv_c))
\\&=\frac{\partial}{\partial v_c}(u_o^Tv_c-log\sum_{w\in V}exp(u_w^Tv_c))\\&=u_o-\frac{\sum_{w\in V}exp(u_w^Tv_c)u_w}{\sum_{w\in V}exp(u_w^Tv_c)}
\end{aligned}</script><blockquote>
<p>偏导数可以移进求和中，对应上方公式的最后两行的推导 : </p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial x}\sum_iy_i=\sum_i\frac{\partial}{\partial x}y_i</script></blockquote>
<p>对上述结果重新排列如下 : </p>
<script type="math/tex; mode=display">
\begin{aligned}
u_o-\frac{\sum_{w\in V}exp(u_w^Tv_c)u_w}{\sum_{w\in V}exp(u_w^Tv_c)}&=u_o-\sum_{w\in V} \frac{exp(u_w^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}u_w\\&=u_o-\sum_{w\in V}P(w|c)u_w
\end{aligned}</script><p>第一项$u_0$是真正的上下文单词，第二项是预测的上下文单词。使用梯度下降法，模型的预测上下文将逐步接近真正的上下文。</p>
<p>对$u_0$进行偏微分计算(其中$u_o$是$u_w=o$的简写)，可知 : </p>
<blockquote>
<script type="math/tex; mode=display">\frac{\partial}{\partial u_o}\sum_{w\in V}u_w^Tv_c=\frac{\partial}{\partial u_o}u_o^Tv_c=v_c</script></blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial}{\partial u_o}logP(o|c)&=\frac{\partial}{\partial u_o}log\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}
\\&=\frac{\partial}{\partial u_o}(logexp(u_o^Tv_c)-log\sum_{w\in V}exp(u_w^Tv_c))
\\&=\frac{\partial}{\partial u_o}(u_o^Tv_c-log\sum_{w\in V}exp(u_w^Tv_c))
\\&=v_c-\frac{\sum_{w\in V}\frac{\partial}{\partial u_o}exp(u_w^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}
\\&=v_c-\frac{exp(u_o^Tv_c)v_c}{\sum_{w\in V}exp(u_w^Tv_c)}
\\&=v_c-\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}v_c
\\&=v_c-P(o|c)v_c
\\&=(1-P(o|c))v_c
\end{aligned}</script><p>从以上式子可看出，当$P(o|c) \rightarrow1$，即通过中心词$c$我们可以正确预测上下文词$o$，此时不需要调整$u_o$，反之，则要调整。</p>
<h2 id="Notes-01-Introduction-SVD-and-Word2Vec"><a href="#Notes-01-Introduction-SVD-and-Word2Vec" class="headerlink" title="Notes 01 Introduction, SVD and Word2Vec"></a>Notes 01 Introduction, SVD and Word2Vec</h2><h3 id="How-to-represent-words"><a href="#How-to-represent-words" class="headerlink" title="How to represent words?"></a>How to represent words?</h3><h4 id="Word-Vectors"><a href="#Word-Vectors" class="headerlink" title="Word Vectors"></a>Word Vectors</h4><p>使用词向量编码单词，N维空间足够我们编码语言的所有语义，每一维度都会编码一些我们使用语言传递的信息。简单的one-hot向量无法给出单词间的相似性，我们需要将维度$|V|$减少至一个低纬度的子空间，来获得稠密的词向量，获得词之间的关系</p>
<h4 id="SVD-Based-Methods"><a href="#SVD-Based-Methods" class="headerlink" title="SVD Based Methods"></a>SVD Based Methods</h4><p>首先遍历一个很大的数据集和统计词的共现计数矩阵$X$，然后对矩阵$X$进行SVD分解得到$USV^T$。然后使用$U$的行来作为字典中所有词的词向量。<br>缺点 : </p>
<ul>
<li>增加新的单词和语料库的大小会改变矩阵的维度</li>
<li>矩阵会非常的稀疏，因为很多词不会共现</li>
<li>矩阵维度会非常高</li>
<li>基于 SVD 的方法的计算复杂度很高</li>
<li>需要对$X$加入一些技巧处理来解决词频的极剧的不平衡<h4 id="Iteration-Based-Methods-Word2vec"><a href="#Iteration-Based-Methods-Word2vec" class="headerlink" title="Iteration Based Methods - Word2vec"></a>Iteration Based Methods - Word2vec</h4>我们创建一个模型，该模型能够一次学习一个迭代，并最终对给定上下文的单词的概率进行编码，而不是像上面的方法存储一些大型数据集的全局信息。<br>我们设计一个模型，该模型的参数就是词向量。然后根据一个目标函数训练模型，在每次模型的迭代计算误差，并遵循一些更新规则，该规则具有惩罚造成错误的模型参数的作用，从而可以学习到词向量。这个方法可以追溯到1986年，我们称这个方法为“反向传播”，模型和任务越简单，训练它的速度就越快。基于迭代的方法一次捕获一个单词的共现情况，而不是像SVD方法那样直接捕获所有的共现计数。<br>已经很多人按照这个思路测试了不同的方法。[Collobert et al., 2011]设计的模型首先将每个单词转换为向量。对每个特定的任务（命名实体识别、词性标注等等），他们不仅训练模型的参数，同时也训练单词向量，计算出了非常好的词向量的同时取得了很好的性能。<br>Word2vec 是一个软件包实际上包含 : </li>
<li><strong>两个算法</strong> ：continuous bag-of-words（CBOW）和skip-gram。CBOW是根据中心词周围的上下文单词来预测该词的词向量。skip-gram则相反，是根据中心词预测周围上下文的词的概率分布。</li>
<li><strong>两个训练方法</strong> ：negative sampling和hierarchical softmax。Negative sampling通过抽取负样本来定义目标，hierarchical softmax通过使用一个有效的树结构来计算所有词的概率来定义目标。</li>
</ul>
<p>该部分引出了Word2vec相关的知识点，基础知识通过<a href="https://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="noopener">刘建平博客</a>学习过。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/30/Introduction%20and%20Word%20Vectors/" data-id="ck9myomwq0007hov8at2x1chn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" rel="tag">cs224n课程</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Word Vectors 2 and Word Senses" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/" class="article-date">
  <time datetime="2020-04-30T14:48:01.055Z" itemprop="datePublished">2020-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/">02 Word Vectors 2 and Word Senses</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Review-Main-idea-of-word2vec"><a href="#Review-Main-idea-of-word2vec" class="headerlink" title="Review: Main idea of word2vec"></a>Review: Main idea of word2vec</h2><p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image01.png" alt></p>
<script type="math/tex; mode=display">
P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}</script><ul>
<li>随机的单词开始</li>
<li>历整个语料库中的每个单词</li>
<li>使用单词向量预测周围的单词</li>
<li>更新向量以便更好地预测</li>
<li>学习到词之间的相似关系和在整个词向量空间中的语义倾向<h3 id="Word2vec-parameters-and-computations"><a href="#Word2vec-parameters-and-computations" class="headerlink" title="Word2vec parameters and computations"></a>Word2vec parameters and computations</h3></li>
</ul>
<p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image02.png" alt></p>
<ul>
<li>每行代表一个单词的词向量，点乘后得到的分数通过softmax映射为概率分布，并且我们得到的概率分布是对于该中心词而言的上下文中单词的概率分布，该分布于上下文所在的具体位置无关，所以在每个位置的预测都是一样的</li>
<li>我们希望模型对上下文中(相当频繁)出现的所有单词给出一个合理的高概率估计</li>
<li>the, and, that, of 这样的停用词，是每个单词点乘后得到的较大概率的单词(去掉这一部分可以使词向量效果更好)<h2 id="Optimization-Gradient-Descent"><a href="#Optimization-Gradient-Descent" class="headerlink" title="Optimization: Gradient Descent"></a>Optimization: Gradient Descent</h2></li>
<li>Gradient Descent每次使用全部样本进行更新，计算成本巨大</li>
<li>Stochastic Gradient Descent每次只是用单个样本进行更新<h3 id="Stochastic-gradients-with-word-vectors"><a href="#Stochastic-gradients-with-word-vectors" class="headerlink" title="Stochastic gradients with word vectors"></a>Stochastic gradients with word vectors</h3><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image03.png" alt></li>
</ul>
<p>我们正在一个window里面计算SGD，但是在一个window里面最多只出现$2m+1$个次，所以$\triangledown_\theta J_t(\theta)$非常稀疏<br>解决办法 : </p>
<ul>
<li>需要稀疏矩阵更新操作来只更新矩阵U和V中的特定行</li>
<li>需要保留单词向量的散列  </li>
</ul>
<p>如果有数百万个单词向量，并且进行分布式计算，那么重要的是不必到处发送巨大的更新  </p>
<h3 id="Word2vec-More-details"><a href="#Word2vec-More-details" class="headerlink" title="Word2vec: More details"></a>Word2vec: More details</h3><p>为什么两个向量？</p>
<ul>
<li>更容易优化，最后都取平均值</li>
<li>可以每个单词只用一个向量</li>
</ul>
<p>两个模型变体</p>
<ul>
<li>Skip-grams(SG)，输入中心词并预测上下文中的单词</li>
<li>Continuous Bag of Words(CBOW)，输入上下文中的单词并预测中心词  </li>
</ul>
<p>之前一直使用naive的softmax(简单但代价很高的训练方法)，接下来使用负采样方法加快训练速率</p>
<h2 id="The-skip-gram-model-with-negative-sampling-HW2"><a href="#The-skip-gram-model-with-negative-sampling-HW2" class="headerlink" title="The skip-gram model with negative sampling(HW2)"></a>The skip-gram model with negative sampling(HW2)</h2><p>softmax中用于归一化的分母的计算代价太高</p>
<script type="math/tex; mode=display">
P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}</script><p>负采样的基本思路是使用一个true pair(中心词及其上下文窗口中的词)与几个noise pair(中心词与随机词搭配) 形成的样本，训练二元逻辑回归<br>原文中的(最大化)目标函数是 : </p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{T}\sum^T_{t=1}J_t(\theta)</script><script type="math/tex; mode=display">
J_t(\theta)=log\sigma(u^T_ov_c)+\sum^k_{i=1}E_{j\sim P(w)}[log\Sigma(-u^T_jv_c)]</script><p>课程中的目标函数是 : </p>
<script type="math/tex; mode=display">
J_{neg-sample}(O,v_c,U)=-log(\sigma(u^T_jv_c))-\sum^K_{k=1}log(\sigma(u^T_kv_c))</script><ul>
<li>我们希望中心词与真实上下文单词的向量点积更大，中心词与随机单词的点积更小</li>
<li>k是我们负采样的样本数目<script type="math/tex; mode=display">
P(w)=U(w)^{3/4}/Z</script>使用上式作为抽样的分布，$U(w)$是unigram分布，通过$\frac{3}{4}$次方，相对减少常见单词的频率，增大稀有词的概率。$Z$用于生成概率分布。<h2 id="Why-not-capture-co-occurrence-counts-directly"><a href="#Why-not-capture-co-occurrence-counts-directly" class="headerlink" title="Why not capture co-occurrence counts directly?"></a>Why not capture co-occurrence counts directly?</h2>共现矩阵$X$</li>
<li>两个选项 ：windows和full document</li>
<li>Window ：与word2vec类似，在每个单词周围都使用Window，包括语法(POS)和语义信息</li>
</ul>
<p>Word-document共现矩阵的基本假设是在同一篇文章中出现的单词更有可能相互关联。假设单词$i$出现在文章$j$中，则矩阵元素$X_{ij}$加1，当我们处理完数据库中的所有文章后，就得到了矩阵$X$，其大小为$|V|×M$，其中$|V|$为词汇量，而M为文章数。这一构建单词文章co-occurrence matrix的方法也是经典的Latent Semantic Analysis所采用的。</p>
<p>利用某个定长窗口中单词与单词同时出现的次数来产生window-based (word-word) co-occurrence matrix。下面以窗口长度为1来举例，假设我们的数据包含以下几个句子 ：</p>
<ul>
<li>I like deep learning.</li>
<li>I like NLP.</li>
<li>I enjoy flying.</li>
</ul>
<p>则我们可以得到如下的word-word co-occurrence matrix:</p>
<p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image04.png" alt></p>
<p>使用共现次数衡量单词的相似性，但是会随着词汇量的增加而增大矩阵的大小，并且需要很多空间来存储这一高维矩阵，后续的分类模型也会由于矩阵的稀疏性而存在稀疏性问题，使得效果不佳。我们需要对这一矩阵进行降维，获得低维(25-1000)的稠密向量。</p>
<h3 id="Method-1-Dimensionality-Reduction-on-X-HW1"><a href="#Method-1-Dimensionality-Reduction-on-X-HW1" class="headerlink" title="Method 1: Dimensionality Reduction on X(HW1)"></a>Method 1: Dimensionality Reduction on X(HW1)</h3><p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image05.png" alt><br>使用SVD方法将共现矩阵$X$分解为$UΣV⊤$，$∑$是对角线矩阵，对角线上的值是矩阵的奇异值。$U$,$V$是对应于行和列的正交基。<br>为了减少尺度同时尽量保存有效信息，可保留对角矩阵的最大的k个值，并将矩阵$U$,$V$的相应的行列保留。这是经典的线性代数算法，对于大型矩阵而言，计算代价昂贵。</p>
<h3 id="Hacks-to-X-several-used-in-Rohde-et-al-2005"><a href="#Hacks-to-X-several-used-in-Rohde-et-al-2005" class="headerlink" title="Hacks to X (several used in Rohde et al. 2005)"></a>Hacks to X (several used in Rohde et al. 2005)</h3><p>按比例调整 counts 会很有效  </p>
<ul>
<li>对诸如the、he、has等高频词进行缩放(语法有太多的影响)，可以全部忽略掉，也可以按照$min(X,t),t\approx100$规则忽略</li>
<li>在基于window的计数中，提高更加接近的单词的计数</li>
<li>使用Person相关系数代替计数</li>
</ul>
<blockquote>
<p>Conclusion：对计数进行处理是可以得到有效的词向量的</p>
</blockquote>
<p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image06.png" alt></p>
<h3 id="Towards-GloVe-Count-based-vs-direct-prediction"><a href="#Towards-GloVe-Count-based-vs-direct-prediction" class="headerlink" title="Towards GloVe: Count based vs. direct prediction"></a>Towards GloVe: Count based vs. direct prediction</h3><p>基于计数(使用整个矩阵的全局统计数据来直接估计)</p>
<ul>
<li>优点 ：训练快速、统计数据高效利用</li>
<li>缺点 ：主要用于捕捉单词相似性、对大量数据给予比例失调的重视  </li>
</ul>
<p>转换计数(定义概率分布并试图预测单词)</p>
<ul>
<li>优点 ：提高其他任务的性能、能捕获除了单词相似性以外的复杂的模式</li>
<li>缺点 ：与语料库大小有关的量表、统计数据的低效使用<h2 id="Encoding-meaning-in-vector-differences"><a href="#Encoding-meaning-in-vector-differences" class="headerlink" title="Encoding meaning in vector differences"></a>Encoding meaning in vector differences</h2>将两个流派的想法结合起来，在神经网络中使用计数矩阵<blockquote>
<p>关于Glove的理论分析需要阅读原文，也可以阅读<a href="https://zhuanlan.zhihu.com/p/60208480" target="_blank" rel="noopener">CS224N笔记(二)：GloVe</a></p>
</blockquote>
</li>
</ul>
<p><strong>关键思想</strong> ：共现概率的比值可以对meaning component进行编码</p>
<p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image07.png" alt></p>
<p>重点不是单一的概率大小，重点是他们之间的比值，其中蕴含着meaning component</p>
<p>例如我们想区分热力学上两种不同状态ice冰与蒸汽steam，它们之间的关系可通过与不同的单词 x 的co-occurrence probability 的比值来描述。<br>例如对于solid固态，虽然$P(solid|ice)$与$P(solid|steam)$本身很小，不能透露有效的信息，但是它们的比值$\frac{P(solid|ice)}{P(solid|steam)}$却较大，因为solid更常用来描述ice的状态而不是steam的状态，所以在ice的上下文中出现几率较大<br>对于gas则恰恰相反，而对于water这种描述ice与steam均可或者fashion这种与两者都没什么联系的单词，则比值接近于1。所以相较于单纯的co-occurrence probability，实际上co-occurrence probability的相对比值更有意义</p>
<p>我们如何在词向量空间中以线性meaning component的形式捕获共现概率的比值？<br>log-bilinear 模型 : </p>
<script type="math/tex; mode=display">
w_i\cdot w_j=logP(i|j)</script><p>向量差异 : </p>
<script type="math/tex; mode=display">
w_x\cdot (w_a-w_b)=log\frac{P(x|a)}{P(x|b)}</script><ul>
<li>如果使向量点积等于共现概率的对数，那么向量差异变成了共现概率的比率<script type="math/tex; mode=display">
J=\sum^T_{i,j=1}f(X_{ij})(w^T_i\tilde{w}_j+b_i+\tilde{b}_j-logX_{ij})^2</script></li>
<li>使用平方误差促使点积尽可能得接近共现概率的对数</li>
<li><p>使用$f(x)$对常见单词进行限制</p>
<p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image08.png" alt></p>
</li>
<li><p>优点 : 训练快速、可以扩展到大型语料库、即使是小语料库和小向量，性能也很好</p>
</li>
</ul>
<h2 id="How-to-evaluate-word-vectors"><a href="#How-to-evaluate-word-vectors" class="headerlink" title="How to evaluate word vectors?"></a>How to evaluate word vectors?</h2><p>与NLP的一般评估相关分为内在与外在(intrinsic vs. extrinsic)<br>内在 : </p>
<ul>
<li>对特定/中间子任务进行评估</li>
<li>计算速度快</li>
<li>有助于理解这个系统  </li>
<li>不清楚是否真的有用，除非与实际任务建立了相关性</li>
</ul>
<p>外在 :   </p>
<ul>
<li>对真实任务的评估</li>
<li>计算精确度可能需要很长时间</li>
<li>不清楚子系统是问题所在，是交互问题，还是其他子系统</li>
<li>如果用另一个子系统替换一个子系统可以提高精确度<h3 id="Intrinsic-word-vector-evaluation"><a href="#Intrinsic-word-vector-evaluation" class="headerlink" title="Intrinsic word vector evaluation"></a>Intrinsic word vector evaluation</h3>一个比较常用的内部评估的方法是词向量的类比。在词向量类比中，给定以下形式的不完整类比：  <script type="math/tex; mode=display">a:b :: c:?</script>形如 : <script type="math/tex; mode=display">man:woman :: king:?</script><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image09.png" alt></li>
</ul>
<p>然后内部评估系统计算词向量的最大余弦相似度：</p>
<script type="math/tex; mode=display">
d=arg\underset{i}{max}\frac{(x_b-x_a+x_c)^Tx_i}{||x_b-x_a+x_c||}</script><p>这个指标有直观的解释。理想的情况下，我们希望$x_b-x_a=x_d-x_c$(例如，queen-king=actress-actor)。这就暗含着我们希望$x_b-x_a+x_c=x_d$。因此，我们确定可以最大化两个词向量之间的归一化点积的向量$x_d$即可（即余弦相似度）。<br>它需要面对的问题 :  </p>
<ul>
<li>通过加法后的余弦距离是否能很好地捕捉到直观的语义和句法类比问题来评估单词向量</li>
<li>从搜索中丢弃输入的单词</li>
<li>如果有信息但不是线性的怎么办？</li>
</ul>
<p>下面是Glove可视化效果 ： </p>
<p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image10.png" alt></p>
<p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image11.png" alt></p>
<p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image12.png" alt></p>
<blockquote>
<p>可以使用数据集评估语法和语义上的效果</p>
<h3 id="Analogy-evaluation-and-hyperparameters"><a href="#Analogy-evaluation-and-hyperparameters" class="headerlink" title="Analogy evaluation and hyperparameters"></a>Analogy evaluation and hyperparameters</h3><p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image13.png" alt></p>
<ul>
<li>300是一个很好的词向量维度</li>
<li>不对称上下文(只使用单侧的单词)不是很好，但是这在下游任务重可能不同</li>
<li>window size设为8对Glove向量来说比较好</li>
<li>分析 : window size设为2的时候实际上有效的，并且对于句法分析是更好的，因为句法效果非常局部</li>
</ul>
</blockquote>
<p>关于字嵌入的维度 :<br>利用矩阵摄动理论，揭示了词嵌入维数选择的基本的偏差与方法的权衡，当持续增大词向量维度的时候，词向量的效果不会一直变差并且会保持平稳</p>
<p><img src="/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/image14.png" alt></p>
<ul>
<li>训练时间越长越好</li>
<li>数据集越大越好，并且维基百科数据集比新闻文本数据集要好(因为维基百科就是在解释概念以及他们之间的相互关联，更多的说明性文本显示了事物之间的所有联系，而新闻并不去解释，而只是去阐述一些事件)</li>
</ul>
<h2 id="Word-senses-and-word-sense-ambiguity"><a href="#Word-senses-and-word-sense-ambiguity" class="headerlink" title="Word senses and word sense ambiguity"></a>Word senses and word sense ambiguity</h2><p>大多数单词都是多义的  </p>
<ul>
<li>特别是常见单词</li>
<li>特别是存在已久的单词</li>
</ul>
<h3 id="Improving-Word-Representations-Via-Global-Context-And-Multiple-Word-Prototypes-Huang-et-al-2012"><a href="#Improving-Word-Representations-Via-Global-Context-And-Multiple-Word-Prototypes-Huang-et-al-2012" class="headerlink" title="Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)"></a>Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)</h3><p>将常用词的所有上下文进行聚类，通过该词得到一些清晰的簇，从而将这个常用词分解为多个单词，例如bank_1, bank_2, bank_3<br>虽然这很粗糙，并且有时sensors之间的划分也不是很明确甚至相互重叠</p>
<h3 id="Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy"><a href="#Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy" class="headerlink" title="Linear Algebraic Structure of Word Senses, with Applications to Polysemy"></a>Linear Algebraic Structure of Word Senses, with Applications to Polysemy</h3><p>单词在标准单词嵌入(如word2vec)中的不同含义以线性叠加(加权和)的形式存在，$f$指频率 : </p>
<script type="math/tex; mode=display">
v_{pike}=\alpha_1v_{pike_1}+\alpha_2v_{pike_2}+\alpha_3v_{pike_3}</script><script type="math/tex; mode=display">
\alpha_1=\frac{f_1}{f_1+f_2+f_3}</script><p>令人惊讶的结果，只是加权平均值就已经可以获得很好的效果 : </p>
<ul>
<li>由于从稀疏编码中得到的概念，你实际上可以将感官分离出来(前提是它们相对比较常见)</li>
<li>可以理解为由于单词存在于高维的向量空间之中，不同的纬度所包含的含义是不同的，所以加权平均值并不会损害单词在不同含义所属的纬度上存储的信息</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/30/Word%20Vectors%202%20and%20Word%20Senses/" data-id="ck9myomwf0004hov80pg32wzh" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" rel="tag">cs224n课程</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>

    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" rel="tag">cs224n课程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" style="font-size: 10px;">cs224n课程</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/">06 The probability of a sentence? Recurrent Neural Networks and Language Models</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/">05 Linguistic Structure Dependency Parsing</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/">04 Matrix Calculus and Backpropagation</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/">03 Word Window Classification, Neural Networks, and PyTorch</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Introduction%20and%20Word%20Vectors/">01 Introduction and Word Vectors</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>