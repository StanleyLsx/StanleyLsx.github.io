<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="02 Word Vectors 2 and Word Senses"><meta name="keywords" content="cs224n课程,nlp,word2vec"><meta name="author" content="Stanleylsx"><meta name="copyright" content="Stanleylsx"><title>02 Word Vectors 2 and Word Senses | LSXのBlog</title><link rel="shortcut icon" href="/umbrella.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?3b6aa10c68000f1e94f4d4b68f24e650";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Review-Main-idea-of-word2vec"><span class="toc-number">1.</span> <span class="toc-text">Review: Main idea of word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Word2vec-parameters-and-computations"><span class="toc-number">1.1.</span> <span class="toc-text">Word2vec parameters and computations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Optimization-Gradient-Descent"><span class="toc-number">2.</span> <span class="toc-text">Optimization: Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-gradients-with-word-vectors"><span class="toc-number">2.1.</span> <span class="toc-text">Stochastic gradients with word vectors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Word2vec-More-details"><span class="toc-number">2.2.</span> <span class="toc-text">Word2vec: More details</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-skip-gram-model-with-negative-sampling-HW2"><span class="toc-number">3.</span> <span class="toc-text">The skip-gram model with negative sampling(HW2)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-not-capture-co-occurrence-counts-directly"><span class="toc-number">4.</span> <span class="toc-text">Why not capture co-occurrence counts directly?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Method-1-Dimensionality-Reduction-on-X-HW1"><span class="toc-number">4.1.</span> <span class="toc-text">Method 1: Dimensionality Reduction on X(HW1)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hacks-to-X-several-used-in-Rohde-et-al-2005"><span class="toc-number">4.2.</span> <span class="toc-text">Hacks to X (several used in Rohde et al. 2005)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Towards-GloVe-Count-based-vs-direct-prediction"><span class="toc-number">4.3.</span> <span class="toc-text">Towards GloVe: Count based vs. direct prediction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Encoding-meaning-in-vector-differences"><span class="toc-number">5.</span> <span class="toc-text">Encoding meaning in vector differences</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#How-to-evaluate-word-vectors"><span class="toc-number">6.</span> <span class="toc-text">How to evaluate word vectors?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Intrinsic-word-vector-evaluation"><span class="toc-number">6.1.</span> <span class="toc-text">Intrinsic word vector evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Analogy-evaluation-and-hyperparameters"><span class="toc-number">6.2.</span> <span class="toc-text">Analogy evaluation and hyperparameters</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word-senses-and-word-sense-ambiguity"><span class="toc-number">7.</span> <span class="toc-text">Word senses and word sense ambiguity</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Improving-Word-Representations-Via-Global-Context-And-Multiple-Word-Prototypes-Huang-et-al-2012"><span class="toc-number">7.1.</span> <span class="toc-text">Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy"><span class="toc-number">7.2.</span> <span class="toc-text">Linear Algebraic Structure of Word Senses, with Applications to Polysemy</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/miao.jpg"></div><div class="author-info__name text-center">Stanleylsx</div><div class="author-info__description text-center">达拉崩巴斑得贝迪卜多比鲁翁</div><div class="follow-button"><a href="https://github.com/stanleylsx" target="_blank" rel="noopener">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">9</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">12</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">1</span></a></div></div></div><div id="content-outer"><div class="plain" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">LSXのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div></div><div class="layout" id="content-inner"><article id="post"><div class="plain" id="post-title">02 Word Vectors 2 and Word Senses</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-13</time><span class="post-meta__separator">|</span><i class="fa fa-inbox" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/cs224n%E5%AD%A6%E4%B9%A0/"> cs224n学习</a><span class="post-meta__separator">|</span><span class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">3k</span><span class="post-meta__separator">|</span><span>阅读时长: 10 分钟</span></span></div><div class="article-container" id="post-content"><h2 id="Review-Main-idea-of-word2vec"><a href="#Review-Main-idea-of-word2vec" class="headerlink" title="Review: Main idea of word2vec"></a>Review: Main idea of word2vec</h2><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image01.png" alt></p>
<script type="math/tex; mode=display">
P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}</script><ul>
<li>随机的单词开始</li>
<li>历整个语料库中的每个单词</li>
<li>使用单词向量预测周围的单词</li>
<li>更新向量以便更好地预测</li>
<li>学习到词之间的相似关系和在整个词向量空间中的语义倾向<h3 id="Word2vec-parameters-and-computations"><a href="#Word2vec-parameters-and-computations" class="headerlink" title="Word2vec parameters and computations"></a>Word2vec parameters and computations</h3></li>
</ul>
<p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image02.png" alt></p>
<ul>
<li>每行代表一个单词的词向量，点乘后得到的分数通过softmax映射为概率分布，并且我们得到的概率分布是对于该中心词而言的上下文中单词的概率分布，该分布于上下文所在的具体位置无关，所以在每个位置的预测都是一样的</li>
<li>我们希望模型对上下文中(相当频繁)出现的所有单词给出一个合理的高概率估计</li>
<li>the, and, that, of 这样的停用词，是每个单词点乘后得到的较大概率的单词(去掉这一部分可以使词向量效果更好)<h2 id="Optimization-Gradient-Descent"><a href="#Optimization-Gradient-Descent" class="headerlink" title="Optimization: Gradient Descent"></a>Optimization: Gradient Descent</h2></li>
<li>Gradient Descent每次使用全部样本进行更新，计算成本巨大</li>
<li>Stochastic Gradient Descent每次只是用单个样本进行更新<h3 id="Stochastic-gradients-with-word-vectors"><a href="#Stochastic-gradients-with-word-vectors" class="headerlink" title="Stochastic gradients with word vectors"></a>Stochastic gradients with word vectors</h3><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image03.png" alt></li>
</ul>
<p>我们正在一个window里面计算SGD，但是在一个window里面最多只出现$2m+1$个次，所以$\triangledown_\theta J_t(\theta)$非常稀疏<br>解决办法 : </p>
<ul>
<li>需要稀疏矩阵更新操作来只更新矩阵U和V中的特定行</li>
<li>需要保留单词向量的散列  </li>
</ul>
<p>如果有数百万个单词向量，并且进行分布式计算，那么重要的是不必到处发送巨大的更新  </p>
<h3 id="Word2vec-More-details"><a href="#Word2vec-More-details" class="headerlink" title="Word2vec: More details"></a>Word2vec: More details</h3><p>为什么两个向量？</p>
<ul>
<li>更容易优化，最后都取平均值</li>
<li>可以每个单词只用一个向量</li>
</ul>
<p>两个模型变体</p>
<ul>
<li>Skip-grams(SG)，输入中心词并预测上下文中的单词</li>
<li>Continuous Bag of Words(CBOW)，输入上下文中的单词并预测中心词  </li>
</ul>
<p>之前一直使用naive的softmax(简单但代价很高的训练方法)，接下来使用负采样方法加快训练速率</p>
<h2 id="The-skip-gram-model-with-negative-sampling-HW2"><a href="#The-skip-gram-model-with-negative-sampling-HW2" class="headerlink" title="The skip-gram model with negative sampling(HW2)"></a>The skip-gram model with negative sampling(HW2)</h2><p>softmax中用于归一化的分母的计算代价太高</p>
<script type="math/tex; mode=display">
P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}</script><p>负采样的基本思路是使用一个true pair(中心词及其上下文窗口中的词)与几个noise pair(中心词与随机词搭配) 形成的样本，训练二元逻辑回归<br>原文中的(最大化)目标函数是 : </p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{T}\sum^T_{t=1}J_t(\theta)</script><script type="math/tex; mode=display">
J_t(\theta)=log\sigma(u^T_ov_c)+\sum^k_{i=1}E_{j\sim P(w)}[log\Sigma(-u^T_jv_c)]</script><p>课程中的目标函数是 : </p>
<script type="math/tex; mode=display">
J_{neg-sample}(O,v_c,U)=-log(\sigma(u^T_jv_c))-\sum^K_{k=1}log(\sigma(u^T_kv_c))</script><ul>
<li>我们希望中心词与真实上下文单词的向量点积更大，中心词与随机单词的点积更小</li>
<li>k是我们负采样的样本数目<script type="math/tex; mode=display">
P(w)=U(w)^{3/4}/Z</script>使用上式作为抽样的分布，$U(w)$是unigram分布，通过$\frac{3}{4}$次方，相对减少常见单词的频率，增大稀有词的概率。$Z$用于生成概率分布。<h2 id="Why-not-capture-co-occurrence-counts-directly"><a href="#Why-not-capture-co-occurrence-counts-directly" class="headerlink" title="Why not capture co-occurrence counts directly?"></a>Why not capture co-occurrence counts directly?</h2>共现矩阵$X$</li>
<li>两个选项 ：windows和full document</li>
<li>Window ：与word2vec类似，在每个单词周围都使用Window，包括语法(POS)和语义信息</li>
</ul>
<p>Word-document共现矩阵的基本假设是在同一篇文章中出现的单词更有可能相互关联。假设单词$i$出现在文章$j$中，则矩阵元素$X_{ij}$加1，当我们处理完数据库中的所有文章后，就得到了矩阵$X$，其大小为$|V|×M$，其中$|V|$为词汇量，而M为文章数。这一构建单词文章co-occurrence matrix的方法也是经典的Latent Semantic Analysis所采用的。</p>
<p>利用某个定长窗口中单词与单词同时出现的次数来产生window-based (word-word) co-occurrence matrix。下面以窗口长度为1来举例，假设我们的数据包含以下几个句子 ：</p>
<ul>
<li>I like deep learning.</li>
<li>I like NLP.</li>
<li>I enjoy flying.</li>
</ul>
<p>则我们可以得到如下的word-word co-occurrence matrix:</p>
<p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image04.png" alt></p>
<p>使用共现次数衡量单词的相似性，但是会随着词汇量的增加而增大矩阵的大小，并且需要很多空间来存储这一高维矩阵，后续的分类模型也会由于矩阵的稀疏性而存在稀疏性问题，使得效果不佳。我们需要对这一矩阵进行降维，获得低维(25-1000)的稠密向量。</p>
<h3 id="Method-1-Dimensionality-Reduction-on-X-HW1"><a href="#Method-1-Dimensionality-Reduction-on-X-HW1" class="headerlink" title="Method 1: Dimensionality Reduction on X(HW1)"></a>Method 1: Dimensionality Reduction on X(HW1)</h3><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image05.png" alt><br>使用SVD方法将共现矩阵$X$分解为$UΣV⊤$，$∑$是对角线矩阵，对角线上的值是矩阵的奇异值。$U$,$V$是对应于行和列的正交基。<br>为了减少尺度同时尽量保存有效信息，可保留对角矩阵的最大的k个值，并将矩阵$U$,$V$的相应的行列保留。这是经典的线性代数算法，对于大型矩阵而言，计算代价昂贵。</p>
<h3 id="Hacks-to-X-several-used-in-Rohde-et-al-2005"><a href="#Hacks-to-X-several-used-in-Rohde-et-al-2005" class="headerlink" title="Hacks to X (several used in Rohde et al. 2005)"></a>Hacks to X (several used in Rohde et al. 2005)</h3><p>按比例调整 counts 会很有效  </p>
<ul>
<li>对诸如the、he、has等高频词进行缩放(语法有太多的影响)，可以全部忽略掉，也可以按照$min(X,t),t\approx100$规则忽略</li>
<li>在基于window的计数中，提高更加接近的单词的计数</li>
<li>使用Person相关系数代替计数</li>
</ul>
<blockquote>
<p>Conclusion：对计数进行处理是可以得到有效的词向量的</p>
</blockquote>
<p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image06.png" alt></p>
<h3 id="Towards-GloVe-Count-based-vs-direct-prediction"><a href="#Towards-GloVe-Count-based-vs-direct-prediction" class="headerlink" title="Towards GloVe: Count based vs. direct prediction"></a>Towards GloVe: Count based vs. direct prediction</h3><p>基于计数(使用整个矩阵的全局统计数据来直接估计)</p>
<ul>
<li>优点 ：训练快速、统计数据高效利用</li>
<li>缺点 ：主要用于捕捉单词相似性、对大量数据给予比例失调的重视  </li>
</ul>
<p>转换计数(定义概率分布并试图预测单词)</p>
<ul>
<li>优点 ：提高其他任务的性能、能捕获除了单词相似性以外的复杂的模式</li>
<li>缺点 ：与语料库大小有关的量表、统计数据的低效使用<h2 id="Encoding-meaning-in-vector-differences"><a href="#Encoding-meaning-in-vector-differences" class="headerlink" title="Encoding meaning in vector differences"></a>Encoding meaning in vector differences</h2>将两个流派的想法结合起来，在神经网络中使用计数矩阵<blockquote>
<p>关于Glove的理论分析需要阅读原文，也可以阅读<a href="https://zhuanlan.zhihu.com/p/60208480" target="_blank" rel="noopener">CS224N笔记(二)：GloVe</a></p>
</blockquote>
</li>
</ul>
<p><strong>关键思想</strong> ：共现概率的比值可以对meaning component进行编码</p>
<p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image07.png" alt></p>
<p>重点不是单一的概率大小，重点是他们之间的比值，其中蕴含着meaning component</p>
<p>例如我们想区分热力学上两种不同状态ice冰与蒸汽steam，它们之间的关系可通过与不同的单词 x 的co-occurrence probability 的比值来描述。<br>例如对于solid固态，虽然$P(solid|ice)$与$P(solid|steam)$本身很小，不能透露有效的信息，但是它们的比值$\frac{P(solid|ice)}{P(solid|steam)}$却较大，因为solid更常用来描述ice的状态而不是steam的状态，所以在ice的上下文中出现几率较大<br>对于gas则恰恰相反，而对于water这种描述ice与steam均可或者fashion这种与两者都没什么联系的单词，则比值接近于1。所以相较于单纯的co-occurrence probability，实际上co-occurrence probability的相对比值更有意义</p>
<p>我们如何在词向量空间中以线性meaning component的形式捕获共现概率的比值？<br>log-bilinear 模型 : </p>
<script type="math/tex; mode=display">
w_i\cdot w_j=logP(i|j)</script><p>向量差异 : </p>
<script type="math/tex; mode=display">
w_x\cdot (w_a-w_b)=log\frac{P(x|a)}{P(x|b)}</script><ul>
<li>如果使向量点积等于共现概率的对数，那么向量差异变成了共现概率的比率<script type="math/tex; mode=display">
J=\sum^T_{i,j=1}f(X_{ij})(w^T_i\tilde{w}_j+b_i+\tilde{b}_j-logX_{ij})^2</script></li>
<li>使用平方误差促使点积尽可能得接近共现概率的对数</li>
<li><p>使用$f(x)$对常见单词进行限制</p>
<p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image08.png" alt></p>
</li>
<li><p>优点 : 训练快速、可以扩展到大型语料库、即使是小语料库和小向量，性能也很好</p>
</li>
</ul>
<h2 id="How-to-evaluate-word-vectors"><a href="#How-to-evaluate-word-vectors" class="headerlink" title="How to evaluate word vectors?"></a>How to evaluate word vectors?</h2><p>与NLP的一般评估相关分为内在与外在(intrinsic vs. extrinsic)<br>内在 : </p>
<ul>
<li>对特定/中间子任务进行评估</li>
<li>计算速度快</li>
<li>有助于理解这个系统  </li>
<li>不清楚是否真的有用，除非与实际任务建立了相关性</li>
</ul>
<p>外在 :   </p>
<ul>
<li>对真实任务的评估</li>
<li>计算精确度可能需要很长时间</li>
<li>不清楚子系统是问题所在，是交互问题，还是其他子系统</li>
<li>如果用另一个子系统替换一个子系统可以提高精确度<h3 id="Intrinsic-word-vector-evaluation"><a href="#Intrinsic-word-vector-evaluation" class="headerlink" title="Intrinsic word vector evaluation"></a>Intrinsic word vector evaluation</h3>一个比较常用的内部评估的方法是词向量的类比。在词向量类比中，给定以下形式的不完整类比：  <script type="math/tex; mode=display">a:b :: c:?</script>形如 : <script type="math/tex; mode=display">man:woman :: king:?</script><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image09.png" alt></li>
</ul>
<p>然后内部评估系统计算词向量的最大余弦相似度：</p>
<script type="math/tex; mode=display">
d=arg\underset{i}{max}\frac{(x_b-x_a+x_c)^Tx_i}{||x_b-x_a+x_c||}</script><p>这个指标有直观的解释。理想的情况下，我们希望$x_b-x_a=x_d-x_c$(例如，queen-king=actress-actor)。这就暗含着我们希望$x_b-x_a+x_c=x_d$。因此，我们确定可以最大化两个词向量之间的归一化点积的向量$x_d$即可（即余弦相似度）。<br>它需要面对的问题 :  </p>
<ul>
<li>通过加法后的余弦距离是否能很好地捕捉到直观的语义和句法类比问题来评估单词向量</li>
<li>从搜索中丢弃输入的单词</li>
<li>如果有信息但不是线性的怎么办？</li>
</ul>
<p>下面是Glove可视化效果 ： </p>
<p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image10.png" alt></p>
<p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image11.png" alt></p>
<p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image12.png" alt></p>
<blockquote>
<p>可以使用数据集评估语法和语义上的效果</p>
<h3 id="Analogy-evaluation-and-hyperparameters"><a href="#Analogy-evaluation-and-hyperparameters" class="headerlink" title="Analogy evaluation and hyperparameters"></a>Analogy evaluation and hyperparameters</h3><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image13.png" alt></p>
<ul>
<li>300是一个很好的词向量维度</li>
<li>不对称上下文(只使用单侧的单词)不是很好，但是这在下游任务重可能不同</li>
<li>window size设为8对Glove向量来说比较好</li>
<li>分析 : window size设为2的时候实际上有效的，并且对于句法分析是更好的，因为句法效果非常局部</li>
</ul>
</blockquote>
<p>关于字嵌入的维度 :<br>利用矩阵摄动理论，揭示了词嵌入维数选择的基本的偏差与方法的权衡，当持续增大词向量维度的时候，词向量的效果不会一直变差并且会保持平稳</p>
<p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image14.png" alt></p>
<ul>
<li>训练时间越长越好</li>
<li>数据集越大越好，并且维基百科数据集比新闻文本数据集要好(因为维基百科就是在解释概念以及他们之间的相互关联，更多的说明性文本显示了事物之间的所有联系，而新闻并不去解释，而只是去阐述一些事件)</li>
</ul>
<h2 id="Word-senses-and-word-sense-ambiguity"><a href="#Word-senses-and-word-sense-ambiguity" class="headerlink" title="Word senses and word sense ambiguity"></a>Word senses and word sense ambiguity</h2><p>大多数单词都是多义的  </p>
<ul>
<li>特别是常见单词</li>
<li>特别是存在已久的单词</li>
</ul>
<h3 id="Improving-Word-Representations-Via-Global-Context-And-Multiple-Word-Prototypes-Huang-et-al-2012"><a href="#Improving-Word-Representations-Via-Global-Context-And-Multiple-Word-Prototypes-Huang-et-al-2012" class="headerlink" title="Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)"></a>Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)</h3><p>将常用词的所有上下文进行聚类，通过该词得到一些清晰的簇，从而将这个常用词分解为多个单词，例如bank_1, bank_2, bank_3<br>虽然这很粗糙，并且有时sensors之间的划分也不是很明确甚至相互重叠</p>
<h3 id="Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy"><a href="#Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy" class="headerlink" title="Linear Algebraic Structure of Word Senses, with Applications to Polysemy"></a>Linear Algebraic Structure of Word Senses, with Applications to Polysemy</h3><p>单词在标准单词嵌入(如word2vec)中的不同含义以线性叠加(加权和)的形式存在，$f$指频率 : </p>
<script type="math/tex; mode=display">
v_{pike}=\alpha_1v_{pike_1}+\alpha_2v_{pike_2}+\alpha_3v_{pike_3}</script><script type="math/tex; mode=display">
\alpha_1=\frac{f_1}{f_1+f_2+f_3}</script><p>令人惊讶的结果，只是加权平均值就已经可以获得很好的效果 : </p>
<ul>
<li>由于从稀疏编码中得到的概念，你实际上可以将感官分离出来(前提是它们相对比较常见)</li>
<li>可以理解为由于单词存在于高维的向量空间之中，不同的纬度所包含的含义是不同的，所以加权平均值并不会损害单词在不同含义所属的纬度上存储的信息</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Stanleylsx</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/">http://yoursite.com/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com">LSXのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/">cs224n课程</a><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/word2vec/">word2vec</a></div><div class="social-share pull-right" data-disabled="google,facebook,twitter,qzone,douban,tencent,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/"><i class="fa fa-chevron-left">  </i><span>03 Word Window Classification, Neural Networks, and PyTorch</span></a></div><div class="next-post pull-right"><a href="/2020/04/06/Introduction%20and%20Word%20Vectors/"><span>01 Introduction and Word Vectors</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'Y3GMXKvryJw9CpeduEyC6hh5-9Nh9j0Va',
  appKey:'9Jc79wdsIqJQfFj1gmJh2vHj',
  placeholder:'Just go go',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10',
  lang: 'zh-cn'
})</script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2020 By Stanleylsx</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>