<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="03 Word Window Classification, Neural Networks, and PyTorch"><meta name="keywords" content="cs224n课程,nlp,ner"><meta name="author" content="Stanleylsx"><meta name="copyright" content="Stanleylsx"><title>03 Word Window Classification, Neural Networks, and PyTorch | LSXのBlog</title><link rel="shortcut icon" href="/umbrella.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?3b6aa10c68000f1e94f4d4b68f24e650";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script><meta name="generator" content="Hexo 4.2.0"><link rel="stylesheet" href="\assets\css\APlayer.min.css" class="aplayer-style-marker">
<script src="\assets\js\APlayer.min.js" class="aplayer-script-marker"></script>
<script src="\assets\js\Meting.min.js" class="meting-script-marker"></script>
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Classification-setup-and-notation"><span class="toc-number">1.</span> <span class="toc-text">Classification setup and notation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-intuition"><span class="toc-number">1.1.</span> <span class="toc-text">Classification intuition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-with-softmax-and-cross-entropy-loss"><span class="toc-number">1.2.</span> <span class="toc-text">Training with softmax and cross-entropy loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Background-What-is-“cross-entropy”-loss-error"><span class="toc-number">1.3.</span> <span class="toc-text">Background: What is “cross entropy” loss&#x2F;error?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-over-a-full-dataset"><span class="toc-number">1.4.</span> <span class="toc-text">Classification over a full dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Traditional-ML-optimization"><span class="toc-number">1.5.</span> <span class="toc-text">Traditional ML optimization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Neural-Network-Classifiers"><span class="toc-number">2.</span> <span class="toc-text">Neural Network Classifiers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Neural-Nets-for-the-Win"><span class="toc-number">2.1.</span> <span class="toc-text">Neural Nets for the Win!</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-difference-with-word-vectors"><span class="toc-number">2.2.</span> <span class="toc-text">Classification difference with word vectors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Neural-computation"><span class="toc-number">2.3.</span> <span class="toc-text">Neural computation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-neuron-can-be-a-binary-logistic-regression-unit"><span class="toc-number">2.4.</span> <span class="toc-text">A neuron can be a binary logistic regression unit</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-neural-network-running-several-logistic-regressions-at-the-same-time"><span class="toc-number">2.5.</span> <span class="toc-text">A neural network &#x3D; running several logistic regressions at the same time</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Matrix-notation-for-a-layer"><span class="toc-number">2.6.</span> <span class="toc-text">Matrix notation for a layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Non-linearities-aka-“f-”-Why-they’re-needed"><span class="toc-number">2.7.</span> <span class="toc-text">Non-linearities (aka “f ”): Why they’re needed</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Named-Entity-Recognition-NER"><span class="toc-number">3.</span> <span class="toc-text">Named Entity Recognition (NER)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Named-Entity-Recognition-on-word-sequences"><span class="toc-number">3.1.</span> <span class="toc-text">Named Entity Recognition on word sequences</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-might-NER-be-hard"><span class="toc-number">3.2.</span> <span class="toc-text">Why might NER be hard?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word-window-classification"><span class="toc-number">4.</span> <span class="toc-text">Word window classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Window-classification-Softmax"><span class="toc-number">4.1.</span> <span class="toc-text">Window classification: Softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Simplest-window-classifier-Softmax"><span class="toc-number">4.2.</span> <span class="toc-text">Simplest window classifier: Softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Slightly-more-complex-Multilayer-Perceptron"><span class="toc-number">4.3.</span> <span class="toc-text">Slightly more complex: Multilayer Perceptron</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Neural-Network-Feed-forward-Computation"><span class="toc-number">4.4.</span> <span class="toc-text">Neural Network Feed-forward Computation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Main-intuition-for-extra-layer"><span class="toc-number">4.5.</span> <span class="toc-text">Main intuition for extra layer</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/miao.jpg"></div><div class="author-info__name text-center">Stanleylsx</div><div class="author-info__description text-center">达拉崩巴斑得贝迪卜多比鲁翁</div><div class="follow-button"><a href="https://github.com/stanleylsx" target="_blank" rel="noopener">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">12</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">20</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(http://ww1.sinaimg.cn/large/0067fiZ7ly1g1cuf0dr09j31hc0u049j.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">LSXのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">03 Word Window Classification, Neural Networks, and PyTorch</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-20</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/cs224n%E5%AD%A6%E4%B9%A0/">cs224n学习</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">2.1k</span><span class="post-meta__separator">|</span><span>阅读时长: 7 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content">
    <div id="aplayer-ZKqMRZCE" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="1358003718" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="true" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#555" data-volume="0.5"></div>
<h2 id="Classification-setup-and-notation"><a href="#Classification-setup-and-notation" class="headerlink" title="Classification setup and notation"></a>Classification setup and notation</h2><p>通常我们有由样本组成的训练数据集</p>
<script type="math/tex; mode=display">
\{x_i,y_i\}_{i=1}^N</script><p>$x_i$是输入，例如单词(索引或是向量)，句子，文档等，维度为$d$<br>$y_i$是我们尝试预测的标签($C$各类别中的一个)，例如 :   </p>
<ul>
<li>类别：感情，命名实体，购买/售出的决定</li>
<li>其他单词</li>
<li>之后：多词序列的<h3 id="Classification-intuition"><a href="#Classification-intuition" class="headerlink" title="Classification intuition"></a>Classification intuition</h3></li>
</ul>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image01.png" alt></p>
<p>训练数据 : $\{x_i,y_i\}_{i=1}^N$<br>简单的说明情况   </p>
<ul>
<li>固定的二维单词向量分类</li>
<li>使用softmax/logistic回归</li>
<li>线性决策边界</li>
</ul>
<p><strong>传统的机器学习/统计学方法：</strong> 假设$x_i$是固定的，训练softmax/logistic回归的权重$W\in \mathbb{R}^{C\times d}$来决定决策边界(超平面)<br>方法 : 对每个$x$，预测</p>
<script type="math/tex; mode=display">
p(y|x)=\frac{exp(W_y\cdot x)}{\sum_{c=1}^Cexp(W_c\cdot x)}</script><p>我们可以将预测函数分为两个步骤：  </p>
<ol>
<li>将$W$的第$y$行和$x$中的对应行相乘得到分数，计算所有的$fc,for  c=1,…,C$<script type="math/tex; mode=display">
W_y\cdot x=\sum^d_{i=1}W_{yi}x_i=f_y</script></li>
<li>使用softmax函数获得归一化的概率<script type="math/tex; mode=display">
p(y|x)=\frac{exp(f_y)}{\sum^C_{c=1}exp(f_c)}</script><h3 id="Training-with-softmax-and-cross-entropy-loss"><a href="#Training-with-softmax-and-cross-entropy-loss" class="headerlink" title="Training with softmax and cross-entropy loss"></a>Training with softmax and cross-entropy loss</h3>对于每个训练样本$(x,y)$，我们的目标是最大化正确类$y$的概率，或者我们可以最小化该类的负对数概率  <script type="math/tex; mode=display">
-logp(y|x)=-log(\frac{exp(f_y)}{})</script><h3 id="Background-What-is-“cross-entropy”-loss-error"><a href="#Background-What-is-“cross-entropy”-loss-error" class="headerlink" title="Background: What is “cross entropy” loss/error?"></a>Background: What is “cross entropy” loss/error?</h3></li>
</ol>
<ul>
<li>交叉熵的概念来源于信息论，衡量两个分布之间的差异</li>
<li>令真实概率分布为$p$</li>
<li>令我们计算的模型概率为$q$</li>
<li>交叉熵为  <script type="math/tex; mode=display">
H(p,q)=-\sum^C_{c=1}p(c)logq(c)</script></li>
<li>假设groud truth(or true or gold or target)的概率分布在正确的类上为1，在其他任何地方为0 ：$p=[0,…,0,1,0,…0]$ </li>
<li>因为p是one-hot向量，所以唯一剩下的项是真实类的负对数概率</li>
</ul>
<h3 id="Classification-over-a-full-dataset"><a href="#Classification-over-a-full-dataset" class="headerlink" title="Classification over a full dataset"></a>Classification over a full dataset</h3><p>在整个数据集$\{x_i,y_i\}_{i=1}^N$上的交叉熵损失函数，是所有样本的交叉熵的均值  </p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{N}\sum^N_{i=1}-log(\frac{e^{f_{yi}}}{\sum^C_{c=1}e^{f_c}})</script><p>我们不使用</p>
<script type="math/tex; mode=display">
f_y=f_y(x)=W_y\cdot x=\sum^d_{j=1}W_{yj}x_j</script><p>我们使用矩阵来表示$f$</p>
<script type="math/tex; mode=display">f=Wx</script><h3 id="Traditional-ML-optimization"><a href="#Traditional-ML-optimization" class="headerlink" title="Traditional ML optimization"></a>Traditional ML optimization</h3><ul>
<li>一般机器学习的参数$\theta$通常只由W的列组成<script type="math/tex; mode=display">
\theta=[W_1,...,W_d]=W(:)^T\in \mathbb{R}^{Cd}</script></li>
<li>因此，我们只通过以下方式更新决策边界<script type="math/tex; mode=display">
\bigtriangledown_{\theta}J(\theta)=[\bigtriangledown_{W_1},...,\bigtriangledown_{W_d}]^T\in \mathbb{R}^{Cd}</script><h2 id="Neural-Network-Classifiers"><a href="#Neural-Network-Classifiers" class="headerlink" title="Neural Network Classifiers"></a>Neural Network Classifiers</h2></li>
</ul>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image02.png" alt></p>
<ul>
<li>单独使用Softmax(≈logistic回归)并不十分强大</li>
<li>Softmax只给出线性决策边界  <ul>
<li>这可能是相当有限的，当问题很复杂时是无用的</li>
<li>纠正这些错误不是很酷吗?</li>
</ul>
</li>
</ul>
<h3 id="Neural-Nets-for-the-Win"><a href="#Neural-Nets-for-the-Win" class="headerlink" title="Neural Nets for the Win!"></a>Neural Nets for the Win!</h3><p>神经网络可以学习更复杂的函数和非线性决策边界</p>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image03.png" alt></p>
<p>更高级的分类需要</p>
<ul>
<li>词向量</li>
<li>更深层次的深层神经网络<h3 id="Classification-difference-with-word-vectors"><a href="#Classification-difference-with-word-vectors" class="headerlink" title="Classification difference with word vectors"></a>Classification difference with word vectors</h3>一般在NLP深度学习中</li>
<li>我们学习了矩阵$W$和词向量$x$</li>
<li>我们学习传统参数和表示</li>
<li>词向量是对one-hot向量的重新表示，在中间层向量空间中移动它们，以便使用(线性)softmax分类器通过x = Le层进行分类  <ul>
<li>即将词向量理解为一层神经网络，输入单词的one-hot向量并获得单词的词向量表示，并且我们需要对其进行更新。其中，$Vd$是数量很大的参数<script type="math/tex; mode=display">
\bigtriangledown_{\theta}J(\theta)=[\bigtriangledown_{W_1},...,\bigtriangledown_{dardvark},...\bigtriangledown_{W_d}]^T\in \mathbb{R}^{Cd+Vd}</script><h3 id="Neural-computation"><a href="#Neural-computation" class="headerlink" title="Neural computation"></a>Neural computation</h3></li>
</ul>
</li>
</ul>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image04.png" alt></p>
<h3 id="A-neuron-can-be-a-binary-logistic-regression-unit"><a href="#A-neuron-can-be-a-binary-logistic-regression-unit" class="headerlink" title="A neuron can be a binary logistic regression unit"></a>A neuron can be a binary logistic regression unit</h3><p>$f$为非线性激活函数(例如sigmoid函数)，$w$为权重向量，$b$为偏置向量，$h$为隐藏层变量对应的向量，$x$为输入向量</p>
<script type="math/tex; mode=display">
h_{w,b}=f(w^Tx+b)</script><script type="math/tex; mode=display">
f(z)=\frac{1}{1+e^{-z}}</script><p>$b$ : 我们可以有一个“总是打开”的特性，它给出一个先验类，或者将它作为一个偏向项分离出来<br>$w,b$是神经元的参数</p>
<h3 id="A-neural-network-running-several-logistic-regressions-at-the-same-time"><a href="#A-neural-network-running-several-logistic-regressions-at-the-same-time" class="headerlink" title="A neural network = running several logistic regressions at the same time"></a>A neural network = running several logistic regressions at the same time</h3><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image05.png" alt></p>
<p>如果我们输入一个向量通过一系列逻辑回归函数，那么我们得到一个输出向量，但是我们不需要提前决定这些逻辑回归试图预测的变量是什么。</p>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image06.png" alt></p>
<p>我们可以输入另一个logistic回归函数。损失函数将指导中间隐藏变量应该是什么，以便更好地预测下一层的目标。我们当然可以使用更多层的神经网络。</p>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image07.png" alt></p>
<h3 id="Matrix-notation-for-a-layer"><a href="#Matrix-notation-for-a-layer" class="headerlink" title="Matrix notation for a layer"></a>Matrix notation for a layer</h3><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image08.png" alt></p>
<p>我们有</p>
<script type="math/tex; mode=display">a_1=f(W_11x_1+W_12x_2+W_13x_3+b_1)</script><script type="math/tex; mode=display">a_2=f(W_21x_1+W_22x_2+W_23x_3+b_2)</script><p>通过矩阵表示的运算有</p>
<script type="math/tex; mode=display">z=Wx+b</script><script type="math/tex; mode=display">a=f(z)</script><p>激活函数$f$在运算时是element-wise逐元素的</p>
<script type="math/tex; mode=display">f([z_1,z_2,z_3])=[f(z_1),f(z_2),f(z_3)]</script><h3 id="Non-linearities-aka-“f-”-Why-they’re-needed"><a href="#Non-linearities-aka-“f-”-Why-they’re-needed" class="headerlink" title="Non-linearities (aka “f ”): Why they’re needed"></a>Non-linearities (aka “f ”): Why they’re needed</h3><p>例如：函数近似，如回归或分类</p>
<ul>
<li>没有非线性，深度神经网络只能做线性变换</li>
<li>多个线性变换可以组成一个的线性变换$W_1W_2x=Wx$，因为线性变换是以某种方式旋转和拉伸空间，多次的旋转和拉伸可以融合为一次线性变换</li>
<li>对于非线性函数而言，使用更多的层，他们可以近似更复杂的函数</li>
</ul>
<h2 id="Named-Entity-Recognition-NER"><a href="#Named-Entity-Recognition-NER" class="headerlink" title="Named Entity Recognition (NER)"></a>Named Entity Recognition (NER)</h2><ul>
<li>任务：例如，查找和分类文本中的名称</li>
</ul>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image09.png" alt></p>
<ul>
<li>可能的用途 : <ul>
<li>跟踪文档中提到的特定实体（组织、个人、地点、歌曲名、电影名等）</li>
<li>对于问题回答，答案通常是命名实体</li>
<li>许多需要的信息实际上是命名实体之间的关联</li>
<li>同样的技术可以扩展到其他 slot-filling 槽填充分类</li>
</ul>
</li>
<li>通常后面是命名实体链接/规范化到知识库</li>
</ul>
<h3 id="Named-Entity-Recognition-on-word-sequences"><a href="#Named-Entity-Recognition-on-word-sequences" class="headerlink" title="Named Entity Recognition on word sequences"></a>Named Entity Recognition on word sequences</h3><p>我们通过在上下文中对单词进行分类，然后将实体提取为单词子序列来预测实体</p>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image10.png" alt></p>
<h3 id="Why-might-NER-be-hard"><a href="#Why-might-NER-be-hard" class="headerlink" title="Why might NER be hard?"></a>Why might NER be hard?</h3><ul>
<li><p>很难计算出实体的边界</p>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image11.png" alt><br>第一个实体是 “First National Bank” 还是 “National Bank”</p>
</li>
<li>很难知道某物是否是一个实体，是一所名为“Future School” 的学校，还是这是一所未来的学校？</li>
<li><p>很难知道未知/新奇实体的类别</p>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image12.png" alt><br>“Zig Ziglar” ? 一个人</p>
</li>
<li><p>实体类是模糊的，依赖于上下文</p>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image13.png" alt><br>这里的“Charles Schwab” 是 PER 不是 ORG</p>
</li>
</ul>
<h2 id="Word-window-classification"><a href="#Word-window-classification" class="headerlink" title="Word window classification"></a>Word window classification</h2><ul>
<li>思想：在<strong>相邻词的上下文窗口</strong>中对一个词进行分类</li>
<li>例如，上下文中一个单词的命名实体分类(人、地点、组织、NONE)</li>
<li>在上下文中对单词进行分类的一个简单方法可能是对窗口中的单词向量进行<strong>平均</strong>，并对平均向量进行分类(问题是这会丢失单词位置信息)</li>
</ul>
<h3 id="Window-classification-Softmax"><a href="#Window-classification-Softmax" class="headerlink" title="Window classification: Softmax"></a>Window classification: Softmax</h3><ul>
<li>训练softmax分类器对中心词进行分类，方法是在一个窗口内<strong>将中心词周围的词向量串联起来</strong></li>
<li>例子：在这句话的上下文中对“Paris”进行分类，窗口长度为2</li>
</ul>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image14.png" alt></p>
<ul>
<li>结果$x_{window}=x\in \mathbb{R}^{5d}$是一个列向量</li>
</ul>
<h3 id="Simplest-window-classifier-Softmax"><a href="#Simplest-window-classifier-Softmax" class="headerlink" title="Simplest window classifier: Softmax"></a>Simplest window classifier: Softmax</h3><p>对于$x=x_{window}$，我们可以使用与之前相同的softmax分类器</p>
<p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image15.png" alt></p>
<ul>
<li>如何更新向量？</li>
<li>简而言之：求导和优化</li>
</ul>
<h3 id="Slightly-more-complex-Multilayer-Perceptron"><a href="#Slightly-more-complex-Multilayer-Perceptron" class="headerlink" title="Slightly more complex: Multilayer Perceptron"></a>Slightly more complex: Multilayer Perceptron</h3><ul>
<li>在我们的softmax分类器中引入一个附加的非线性层。</li>
<li>Multilayer Perceptron(mlp)是更复杂的神经系统的基本构件!</li>
<li>假设我们要对中心词是否为一个地点，进行分类</li>
<li>与word2vec类似，我们将遍历语料库中的所有位置。但这一次，它将受到监督，只有一些位置能够得到高分，在他们的中心有一个实际的NER Location的位置是“真实的”位置会获得高分<h3 id="Neural-Network-Feed-forward-Computation"><a href="#Neural-Network-Feed-forward-Computation" class="headerlink" title="Neural Network Feed-forward Computation"></a>Neural Network Feed-forward Computation</h3>使用神经激活$a$简单地给出一个非标准化的分数<script type="math/tex; mode=display">score(x)=U^Ta \in R</script>我们用一个三层神经网络计算一个窗口的得分<script type="math/tex; mode=display">s=score("museums in Paris are amazing”)</script><script type="math/tex; mode=display">s=U^Tf(Wx+b)</script><script type="math/tex; mode=display">x\in \mathbb{R}^{20 \times 1},W\in \mathbb{R}^{8 \times 20},U\in \mathbb{R}^{8 \times 1}</script><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image16.png" alt></li>
</ul>
<h3 id="Main-intuition-for-extra-layer"><a href="#Main-intuition-for-extra-layer" class="headerlink" title="Main intuition for extra layer"></a>Main intuition for extra layer</h3><p>中间层学习输入词向量之间的<strong>非线性交互</strong><br>例如：只有当“museum”是第一个向量时，“in”放在第二个位置才重要</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Stanleylsx</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/">http://yoursite.com/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com">LSXのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/">cs224n课程</a><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/ner/">ner</a></div><div class="social-share pull-right" data-disabled="google,facebook,twitter,qzone,douban,tencent,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/"><i class="fa fa-chevron-left">  </i><span>04 Matrix Calculus and Backpropagation</span></a></div><div class="next-post pull-right"><a href="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/"><span>常用图像分类模型和相关的知识点</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'Y3GMXKvryJw9CpeduEyC6hh5-9Nh9j0Va',
  appKey:'9Jc79wdsIqJQfFj1gmJh2vHj',
  placeholder:'Just go go',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10',
  lang: 'zh-cn'
})</script></div></div><footer class="footer-bg" style="background-image: url(http://ww1.sinaimg.cn/large/0067fiZ7ly1g1cuf0dr09j31hc0u049j.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 By Stanleylsx</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>