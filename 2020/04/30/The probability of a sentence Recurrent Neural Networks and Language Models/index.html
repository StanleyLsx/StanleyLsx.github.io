<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>06 The probability of a sentence? Recurrent Neural Networks and Language Models | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Language Modeling 语言模型就是预测一个句子中下一个词的概率分布。如上图所示，假设给定一个句子前缀是the students opened their，语言模型预测这个句子片段下一个词是books、laptops、exams、minds或者其他任意一个词的概率。形式化表示就是计算。  P(x^{(t+1)}|x^{(t)},...,x^{(1)})$x^{(t+1)}$表示第$t+">
<meta property="og:type" content="article">
<meta property="og:title" content="06 The probability of a sentence? Recurrent Neural Networks and Language Models">
<meta property="og:url" content="http://yoursite.com/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Language Modeling 语言模型就是预测一个句子中下一个词的概率分布。如上图所示，假设给定一个句子前缀是the students opened their，语言模型预测这个句子片段下一个词是books、laptops、exams、minds或者其他任意一个词的概率。形式化表示就是计算。  P(x^{(t+1)}|x^{(t)},...,x^{(1)})$x^{(t+1)}$表示第$t+">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image01.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image02.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image03.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image04.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image05.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image06.png">
<meta property="article:published_time" content="2020-04-30T15:54:16.022Z">
<meta property="article:modified_time" content="2020-04-30T16:20:17.433Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="cs224n课程">
<meta property="article:tag" content="nlp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image01.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-The probability of a sentence Recurrent Neural Networks and Language Models" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/" class="article-date">
  <time datetime="2020-04-30T15:54:16.022Z" itemprop="datePublished">2020-04-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs224n%E5%AD%A6%E4%B9%A0/">cs224n学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      06 The probability of a sentence? Recurrent Neural Networks and Language Models
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h2><p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image01.png" alt></p>
<p>语言模型就是预测一个句子中下一个词的概率分布。如上图所示，假设给定一个句子前缀是the students opened their，语言模型预测这个句子片段下一个词是books、laptops、exams、minds或者其他任意一个词的概率。形式化表示就是计算。</p>
<script type="math/tex; mode=display">
P(x^{(t+1)}|x^{(t)},...,x^{(1)})</script><p>$x^{(t+1)}$表示第$t+1$个位置(时刻)的词是$x$，$x$可以是词典$V$中任意的一个词。</p>
<p>例如有一段文本$x^{(1)}$,…,$x^{(T)}$，则这段文本的概率(根据语言模型)为</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(x^{(1)},...,x^{(T)})&=P(x^{(1)})\times P(x^{(2)}|x^{(1)})\times...\times P(x^{(T)}|x^{(T-1)},...,x^{(1)})
\\&=\prod ^T_{t=1}P(x^{(T)}|x^{(T-1)},...,x^{(1)})
\end{aligned}</script><p>语言模型可以用在输入法中预测下一个将要输入的词，在谷歌搜索中输入前几个关键词，搜索引擎会自动预测接下来可能的几个词。网上有很多智能AI自动生成新闻、诗歌等等。可以说语言模型是很多NLP任务的基础模块，具有非常重要的作用。</p>
<h3 id="n-gram-Language-Models"><a href="#n-gram-Language-Models" class="headerlink" title="n-gram Language Models"></a>n-gram Language Models</h3><p>在前-深度学习时代，人们使用n-gram方法来学习语言模型。对于一个句子，n-gram表示句子中连续的n个词，n-gram对于n=1,2,3,4的结果是：</p>
<ul>
<li>unigrams: “the”, “students”, “opened”, ”their”</li>
<li>bigrams: “the students”, “students opened”, “opened their”</li>
<li>trigrams: “the students opened”, “students opened their”</li>
<li>4-grams: “the students opened their”</li>
</ul>
<p>n-gram方法有一个前提假设，即假设每个词出现的概率只和前n-1个词有关。n-gram的计算方法就是，统计语料库中出现$x^{(t)},…,x^{(t-n+2)}$的次数作为分母，以及在这个基础上再接一个词$x^{(t+1)}$的次数$x^{(t+1)},x^{(t)},…,x^{(t-n+2)}$作为分子，用后者除以前者来近似这个条件概率。</p>
<p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image02.png" alt></p>
<p>举个例子，假设完整的句子是as the proctor started the clock, the students opened their，需要预测下一个词的概率分布。对于4-gram方法，则只有students opened their对下一个词有影响，前面的词都没有影响。然后我们统计训练集语料库中发现，分母students opened their出现1000次，其后接books即students opened their books出现了400次，所以P(books|students opened their)=400/1000=0.4。类似的，可以算出下一个词为exams的概率是0.1。所以4-gram方法认为下一个词是books的概率更大。</p>
<p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image03.png" alt></p>
<p>n-gram方法在统计语料库中的n-gram时，对词的顺序是有要求的，即必须要和给定的n-gram的顺序一样才能对频数加1，比如这个例子中只有出现和students opened their顺序一样才行，如果是their students opened则不行。</p>
<p>n-gram方法虽然能够有效，比如对于上面的例子，预测出books和exams看起来和前面几个词搭配得很好；但是，它有不少的问题，还是上面的例子，其实考虑更前面的词proctor以及clock的话，这很明显是考试场景，后面出现exams的概率应该比books更高才对。</p>
<p>具体来说，n-gram方法有以下不足：</p>
<ul>
<li>考虑的状态有限。n-gram只能看到前n-1个词，无法建模长距离依赖关系，上面就是一个很好的例子。</li>
<li>稀疏性问题。对于一个稀有的(不常见的)词w，如果他的词组没有在语料库中出现，则分子为0，但w很有可能是正确的，概率至少不是0。比如students opened their petri dishes，对于学生物的学生来说是有可能的，但如果students opened their petri dishes没有在语料库中出现的话，petri dishes的概率就被预测为0了，这是不合理的。当然这个问题可以通过对词典中所有可能的词组频率+1平滑来部分解决。</li>
<li>更严重的稀疏性问题，如果分母的词组频率在语料库中是0，那么所有词w对应的分子的词组频率就是0了，根本就没法计算概率。这种情况只能使用back-off策略，即如果4-gram太过于稀疏了，则降到3-gram，分母只统计opened their的频率。一般的，虽然n-gram中的n越大，语言模型预测越准确，但其稀疏性越严重。n其实就相当于维度，我们知道在空间中，维度越高越稀疏，高维空间非常稀疏。对于n-gram，一般取n&lt;=5。</li>
<li>存储问题，需要存储所有n-gram的频率，如果n越大，这种n-gram的组合越多，所以存储空间呈幂次上升。</li>
</ul>
<p>下面是一个更直观的trigram稀疏性问题的例子，由于语料库中统计到的today the company和today the bank的词组频率相同，导致company和bank算出来的概率相等，无法区分。就是因为这两个trigram在预料中出现都比较少，很稀疏，导致统计数据难以把他们区分开来。</p>
<p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image04.png" alt></p>
<h3 id="A-fixed-window-neural-Language-Model"><a href="#A-fixed-window-neural-Language-Model" class="headerlink" title="A fixed-window neural Language Model"></a>A fixed-window neural Language Model</h3><p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image05.png" alt></p>
<p>window-based neural model在第三讲中被用于NER问题，方法是对一个词开一个小窗口，然后利用词向量和全连接网络识别词的类别。仿照这个方法，也可以用基于窗口的方法来学习语言模型。</p>
<p><img src="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image06.png" alt></p>
<p>超越n-gram语言模型的改进有： </p>
<ul>
<li>没有稀疏性问题，它不要求语料库中出现n-gram的词组，它仅仅是把每个独立的单词的词向量组合起来。只要有词向量，就有输入，至少整个模型能顺利跑通。</li>
<li>不需要观察到所有的n-grams，节省存储空间，只需要存储每个独立的词的词向量。</li>
</ul>
<p>存在的问题： </p>
<ul>
<li>固定窗口太小，受限于窗口大小，不能感知远距离的关系。</li>
<li>扩大窗口就需要扩大权重矩阵$W$，导致网络变得复杂。</li>
<li>输入$e^{(1)},…,e^{(4)}$对应$W$的不同列，每个$e$对应的权重完全是独立的，没有共享关系，导致训练效率比较低。</li>
</ul>
<p>我们需要一个神经结构，可以处理任何长度的输入</p>
<h3 id="RNN-Language-Model"><a href="#RNN-Language-Model" class="headerlink" title="RNN Language Model"></a>RNN Language Model</h3><h3 id="Evaluating-Language-Models"><a href="#Evaluating-Language-Models" class="headerlink" title="Evaluating Language Models"></a>Evaluating Language Models</h3><h3 id="Why-should-we-care-about-Language-Modeling"><a href="#Why-should-we-care-about-Language-Modeling" class="headerlink" title="Why should we care about Language Modeling?"></a>Why should we care about Language Modeling?</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/" data-id="ck9n0798900049sv80he21wol" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" rel="tag">cs224n课程</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">05 Linguistic Structure Dependency Parsing</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs224n%E5%AD%A6%E4%B9%A0/">cs224n学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" rel="tag">cs224n课程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" style="font-size: 10px;">cs224n课程</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/">06 The probability of a sentence? Recurrent Neural Networks and Language Models</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/">05 Linguistic Structure Dependency Parsing</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/">04 Matrix Calculus and Backpropagation</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/">03 Word Window Classification, Neural Networks, and PyTorch</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Introduction%20and%20Word%20Vectors/">01 Introduction and Word Vectors</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>