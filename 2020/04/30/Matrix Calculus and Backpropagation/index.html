<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>04 Matrix Calculus and Backpropagation | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Gradients该部分介绍了雅可比矩阵、链式求导法则，最后推算出下图的输出得分$s$对$W$和$b$的求导结果。   根据链式求导法则 :  \frac{\partial s}{\partial b}&#x3D;\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial b}&#x3D;u^Tdiag(f&#39;">
<meta property="og:type" content="article">
<meta property="og:title" content="04 Matrix Calculus and Backpropagation">
<meta property="og:url" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Gradients该部分介绍了雅可比矩阵、链式求导法则，最后推算出下图的输出得分$s$对$W$和$b$的求导结果。   根据链式求导法则 :  \frac{\partial s}{\partial b}&#x3D;\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial b}&#x3D;u^Tdiag(f&#39;">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image01.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image02.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image03.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image04.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image05.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image06.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image07.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image08.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image09.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image10.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image11.png">
<meta property="og:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image12.png">
<meta property="article:published_time" content="2020-04-30T15:50:12.656Z">
<meta property="article:modified_time" content="2020-04-30T15:52:00.183Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="cs224n课程">
<meta property="article:tag" content="nlp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image01.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Matrix Calculus and Backpropagation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/" class="article-date">
  <time datetime="2020-04-30T15:50:12.656Z" itemprop="datePublished">2020-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      04 Matrix Calculus and Backpropagation
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h2><p>该部分介绍了雅可比矩阵、链式求导法则，最后推算出下图的输出得分$s$对$W$和$b$的求导结果。</p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image01.png" alt></p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image02.png" alt></p>
<p>根据链式求导法则 : </p>
<script type="math/tex; mode=display">\frac{\partial s}{\partial b}=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial b}=u^Tdiag(f'(z))I=u^T \cdot f'(z)</script><script type="math/tex; mode=display">\frac{\partial s}{\partial W}=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial W}</script><p>令$\delta=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}=u^T \cdot f’(z)$，$\delta$是局部误差符号，则</p>
<script type="math/tex; mode=display">\frac{\partial s}{\partial W}=\delta\frac{\partial z}{\partial W}=\delta^T x^T</script><script type="math/tex; mode=display">\frac{\partial s}{\partial b}=\delta\frac{\partial z}{\partial b}=\delta</script><h3 id="Derivative-with-respect-to-Matrix-Output-shape"><a href="#Derivative-with-respect-to-Matrix-Output-shape" class="headerlink" title="Derivative with respect to Matrix: Output shape"></a>Derivative with respect to Matrix: Output shape</h3><p>$W \in \mathbb{R}^{n \times m},\frac{\partial s}{\partial W}$的形状是啥?<br>我们遵循导数的形状是参数的形状的规则，可以看到它是一个$n \times m$的雅可比矩阵</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
\frac{\partial s}{\partial W_{11}} & \cdots  & \frac{\partial s}{\partial W_{1m}}\\ 
\vdots  & \ddots  & \vdots \\ 
\frac{\partial s}{\partial W_{n1}} & \cdots  & \frac{\partial s}{\partial W_{nm}}
\end{bmatrix}</script><h3 id="Why-the-Transposes"><a href="#Why-the-Transposes" class="headerlink" title="Why the Transposes?"></a>Why the Transposes?</h3><p>为什么$\frac{\partial s}{\partial W}=\delta^T x^T$中$\delta$是转置?</p>
<script type="math/tex; mode=display">
\frac{\partial s}{\partial W}=\delta^T x^T=\begin{bmatrix}
\delta_1 \\ 
\vdots \\ 
\delta_n
\end{bmatrix}
[x_1,...,x_m]=\begin{bmatrix}
\delta_1x_1 & \cdots  & \delta_1x_m\\ 
\vdots  & \ddots  & \vdots \\ 
\delta_nx_1 & \cdots  & \delta_nx_m
\end{bmatrix}</script><h2 id="Deriving-local-input-gradient-in-backprop"><a href="#Deriving-local-input-gradient-in-backprop" class="headerlink" title="Deriving local input gradient in backprop"></a>Deriving local input gradient in backprop</h2><p>通过神经网络展开的图计算$\frac{\partial s}  {\partial W}$</p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image03.png" alt></p>
<script type="math/tex; mode=display">\frac{\partial s}{\partial W}=\delta\frac{\partial z}{\partial W}=\delta  \frac{\partial}{\partial W}Wx+b</script><p>我们只考虑$W_{ij}$的导数，$W_{ij}$只对$z_i$有贡献，例如$W_{23}$只对$z_2$有贡献，对$z_1$没有贡献，可推导出 ： </p>
<script type="math/tex; mode=display">
\frac{\partial z_i}{\partial W_{ij}}=\delta  \frac{\partial}{\partial W_{ij}}W_ix+b_i=\frac{\partial}{\partial W_{ij}}\sum^d_{k=1}W_{ik}x_k=x_j</script><h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><p>前向传播(从左至右计算)</p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image04.png" alt></p>
<script type="math/tex; mode=display">s=u^Th</script><script type="math/tex; mode=display">h=f(z)</script><script type="math/tex; mode=display">z=Wx+b</script><script type="math/tex; mode=display">x(input)</script><p>后向传播(从右至左传递导数)</p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image05.png" alt></p>
<h3 id="Backpropagation-Single-Node"><a href="#Backpropagation-Single-Node" class="headerlink" title="Backpropagation: Single Node"></a>Backpropagation: Single Node</h3><p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image06.png" alt></p>
<ul>
<li>节点接收“上游梯度”</li>
<li>目标是传递正确的“下游梯度”</li>
<li>每个节点都有<strong>局部梯度local gradient</strong>，它输出的梯度是与它的输入有关</li>
<li>[downstream gradient] = [upstream gradient] x [local gradient]</li>
</ul>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image07.png" alt></p>
<ul>
<li>多个输入对应多个局部梯度<h3 id="An-Example"><a href="#An-Example" class="headerlink" title="An Example"></a>An Example</h3></li>
</ul>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image08.png" alt></p>
<p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image09.png" alt></p>
<script type="math/tex; mode=display">\frac{\partial f}{\partial x}=\frac{\partial f}{\partial a}\frac{\partial a}{\partial x}=2</script><script type="math/tex; mode=display">\frac{\partial f}{\partial y}=\frac{\partial f}{\partial a}\frac{\partial a}{\partial y}+\frac{\partial f}{\partial b}\frac{\partial b}{\partial y}=2+3=5</script><script type="math/tex; mode=display">\frac{\partial f}{\partial z}=0</script><h3 id="Efficiency-compute-all-gradients-at-once"><a href="#Efficiency-compute-all-gradients-at-once" class="headerlink" title="Efficiency: compute all gradients at once"></a>Efficiency: compute all gradients at once</h3><p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image10.png" alt></p>
<ul>
<li>绿的线的部分导数可以共用以此减少计算</li>
</ul>
<h3 id="Back-Prop-in-General-Computation-Graph"><a href="#Back-Prop-in-General-Computation-Graph" class="headerlink" title="Back-Prop in General Computation Graph"></a>Back-Prop in General Computation Graph</h3><p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image11.png" alt></p>
<ol>
<li>Fprop：按拓扑排序顺序访问节点<ul>
<li>计算给定父节点的节点的值</li>
</ul>
</li>
<li>Bprop：<ul>
<li>初始化输出梯度为 1</li>
<li>以相反的顺序方位节点，使用节点的后继的梯度来计算每个节点的梯度</li>
<li>$\{y_1,y_2,…,y_n\}$是$x$的后继</li>
<li>$\frac{\partial z}{\partial x}=\sum^n_1\frac{\partial z}{\partial y_i}\frac{\partial y_i}{\partial x}$</li>
<li>正确地说，Fprop 和 Bprop 的计算复杂度是一样的</li>
<li>一般来说，我们的网络有固定的层结构，所以我们可以使用矩阵和雅可比矩阵</li>
</ul>
</li>
</ol>
<h3 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a>Automatic Differentiation</h3><p><img src="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/image12.png" alt></p>
<ul>
<li>梯度计算可以从 Fprop 的符号表达式中自动推断</li>
<li>每个节点类型需要知道如何计算其输出，以及如何在给定其输出的梯度后计算其输入的梯度</li>
<li>现代DL框架(Tensorflow, Pytoch)为您做反向传播，但主要是令作者手工计算层/节点的局部导数</li>
</ul>
<h2 id="Notes-03-Neural-Networks-Backpropagation"><a href="#Notes-03-Neural-Networks-Backpropagation" class="headerlink" title="Notes 03 Neural Networks, Backpropagation"></a>Notes 03 Neural Networks, Backpropagation</h2><h3 id="Neural-Networks-Foundations"><a href="#Neural-Networks-Foundations" class="headerlink" title="Neural Networks: Foundations"></a>Neural Networks: Foundations</h3><p>该部分涉及到DL中的W和b的更新，已经掌握</p>
<h3 id="Neural-Networks-Tips-and-Tricks"><a href="#Neural-Networks-Tips-and-Tricks" class="headerlink" title="Neural Networks: Tips and Tricks"></a>Neural Networks: Tips and Tricks</h3><p>此部分已经在我的hit里面的<a href="https://github.com/StanleyLsx/image_classification" target="_blank" rel="noopener">image_classification</a>中学习到</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/" data-id="ck9myomw90001hov8fw9i92av" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" rel="tag">cs224n课程</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          05 Linguistic Structure Dependency Parsing
        
      </div>
    </a>
  
  
    <a href="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">03 Word Window Classification, Neural Networks, and PyTorch</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" rel="tag">cs224n课程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/cs224n%E8%AF%BE%E7%A8%8B/" style="font-size: 10px;">cs224n课程</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/30/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/">06 The probability of a sentence? Recurrent Neural Networks and Language Models</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Linguistic%20Structure%20Dependency%20Parsing/">05 Linguistic Structure Dependency Parsing</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Matrix%20Calculus%20and%20Backpropagation/">04 Matrix Calculus and Backpropagation</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/">03 Word Window Classification, Neural Networks, and PyTorch</a>
          </li>
        
          <li>
            <a href="/2020/04/30/Introduction%20and%20Word%20Vectors/">01 Introduction and Word Vectors</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>