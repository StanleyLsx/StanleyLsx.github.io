<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>拉格朗日函数的对偶问题与SVM</title>
      <link href="/2020/05/15/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E4%B8%8ESVM/"/>
      <url>/2020/05/15/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E4%B8%8ESVM/</url>
      
        <content type="html"><![CDATA[<h2 id="拉格朗日函数的对偶问题"><a href="#拉格朗日函数的对偶问题" class="headerlink" title="拉格朗日函数的对偶问题"></a>拉格朗日函数的对偶问题</h2><h3 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h3><p>针对有等式和不等式约束的最优化原始问题：</p><script type="math/tex; mode=display">\underset{x}{min}f(x)\\s.t.\quad h_i(x)=0,i=1,2,...,m\\s.t.\quad g_j(x)\leq0,j=1,2,...,n \tag{1}</script><p>文章<a href="https://stanleylsx.github.io/2020/05/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95%E4%B8%8EL1%E5%92%8CL2%E6%AD%A3%E5%88%99%E9%A1%B9/" target="_blank" rel="noopener">拉格朗日乘子法与L1和L2正则项</a>中给出了它的拉格朗日方程和对应的KTT条件： </p><script type="math/tex; mode=display">L(x,\alpha,\beta)=f(x)+\sum^m_{i=1}\alpha_ih_i(x)+\sum^n_{j=1}\beta_jg_j(x) \tag{2}</script><p>$\alpha_i$和$\beta_j$为拉格朗日乘子，根据不等式约束的拉格朗日乘子条件知道$\beta_j\geq0$。  </p><p>对于(2)式，假设给定一个不符合原始问题(1)约束条件的$x$(即存在$i$使得$h_i(x)\neq0$，或者存在$j$使得$g_j(x)&gt;0$)，若存在$h_i(x)\neq0$，可令$\alpha_i \rightarrow\infty$，若存在$g_j(x)&gt;0$，可令$\beta_j \rightarrow\infty$，则一定有：</p><script type="math/tex; mode=display">\underset{\alpha,\beta;\alpha_i\geq0}{max}[f(x)+\sum^m_{i=1}\alpha_ih_i(x)+\sum^n_{j=1}\beta_jg_j(x) ]\rightarrow\infty \tag{3}</script><p>给定一个符合原始问题(1)约束条件的$x$，即$h_i(x)=0$且$g_j(x)\leq0$，一定会存在：</p><script type="math/tex; mode=display">\underset{\alpha,\beta;\alpha_i\geq0}{max}[f(x)+\sum^m_{i=1}\alpha_ih_i(x)+\sum^n_{j=1}\beta_jg_j(x) ]=f(x) \tag{4}</script><blockquote><p>此时$h_i(x)=0$，对于$g_j(x)$它最大值在它为0的时候取到</p></blockquote><p>由此看到只有当所有$x$必须满足约束条件时，问题才会有解，否则，(3)式可令结果为无穷导致问题无解。</p><p>则根据(1)公式和(4)得到等价问题，它们有相同的解：</p><script type="math/tex; mode=display">\underset{x}{min}\underset{\alpha,\beta;\beta_j\geq0}{max}L(x,\alpha,\beta) \tag{5}</script><p>定义原始问题的最优值：</p><script type="math/tex; mode=display">\underset{x}{min}\underset{\alpha,\beta;\beta_j\geq0}{max}L(x,\alpha,\beta)=p^{*} \tag{6}</script><h3 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h3><p>为了找到对偶的问题，则定义一个对偶函数，令</p><script type="math/tex; mode=display">D(\alpha,\beta)=\underset{x}{min}L(x,\alpha,\beta)  \tag{7}</script><p>有了对偶函数就可给出对偶问题了，与原始问题的形式非常类似，只是把min和max交换了一下，加上限制条件：</p><script type="math/tex; mode=display">\underset{\alpha,\beta;\alpha_i\geq0}{max}D(\alpha,\beta)=\underset{\alpha,\beta;\beta_j\geq0}{max}\underset{x}{min}L(x,\alpha,\beta) \\s.t. \quad \beta_j\geq0,j=1,2...,m\tag{8}</script><p>$\alpha$和$\beta$称为对偶变量，定义对偶函数的最优值</p><script type="math/tex; mode=display">\underset{\alpha,\beta;\beta_j\geq0}{max}D(\alpha,\beta)=d^{*} \tag{9}</script><p>若原始问题与对偶问题都有最优解，则：</p><script type="math/tex; mode=display">d^{*}=\underset{\alpha,\beta;\beta_j\geq0}{max}\underset{x}{min}L(x,\alpha,\beta) \leq \underset{x}{min}\underset{\alpha,\beta;\beta_j\geq0}{max}L(x,\alpha,\beta)=p^{*} \tag{10}</script><ul><li>当$d^*\leq p^*$，称为“弱对偶性(weak duality)”成立，对于所有优化问题都成立，即使原始问题非凸；</li><li>当$d^*=p^*$，则称为“强对偶性(strong duality)”成立。</li></ul><p>强对偶是一个非常好的性质，因为在强对偶成立的情况下，可以通过求解对偶问题来得到原始问题的解，在SVM中就是这样做的。当然并不是所有的对偶问题都满足强对偶性，在SVM中是直接假定了强对偶性的成立，其实只要满足一些条件，强对偶性是成立的，比如说Slater条件与KKT条件。</p><blockquote><p><strong>Slater条件</strong>:对于原始问题及其对偶问题，假设函数$f(x)$和$g_j(x)$是凸函数，$h_i(x)=0$为仿射函数，且不等式约束$g_j(x)$是严格可行的(即存在$x$，对所有$j$有$g_j(x)&lt;0$)，则存在$x^{<em>},\alpha^{</em>},\beta^{<em>}$，使$x$是原始问题的解，$\alpha^{</em>},\beta^{*}$是对偶问题的解，并且:</p><script type="math/tex; mode=display">p^{*}=d^{*}=L(x^{*},\alpha^{*},\beta^{*})</script><p>也就是说如果原始问题是凸优化问题并且满足 Slater 条件的话，那么强对偶性成立。需要注意的是，这里只是指出了强对偶成立的一种情况，并不是唯一的情况。</p><blockquote><p> 假如有$\vec{x}\in\mathbb{R}^n,\vec{b}\in\mathbb{R}^m$，矩阵$A_{m\times n}$，那么：</p><script type="math/tex; mode=display">\vec{x}\rightarrow A\vec{x}+\vec{b}</script><p>被成为$\mathbb{R}^n \rightarrow \mathbb{R}^m$的仿射变换，这一过程被称为仿射函数。</p><script type="math/tex; mode=display">f(x)=Ax+b,x\in \mathbb{R}^n</script><p>比如最简单的$a_1x_1+a_2x_2+\cdots +a_nx_n+b$<br>就是一个仿射函数。仿射函数有一个非常重要的性质，那就是它既凹又凸。</p></blockquote></blockquote><h2 id="支持向量机原理"><a href="#支持向量机原理" class="headerlink" title="支持向量机原理"></a>支持向量机原理</h2>]]></content>
      
      
      <categories>
          
          <category> 机器学习基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 拉格朗日函数 </tag>
            
            <tag> 支持向量机 </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>拉格朗日乘子法与L1和L2正则项</title>
      <link href="/2020/05/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95%E4%B8%8EL1%E5%92%8CL2%E6%AD%A3%E5%88%99%E9%A1%B9/"/>
      <url>/2020/05/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95%E4%B8%8EL1%E5%92%8CL2%E6%AD%A3%E5%88%99%E9%A1%B9/</url>
      
        <content type="html"><![CDATA[<h2 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h2><p>本节内容直接摘自<a href="https://www.cnblogs.com/ooon/p/5721119.html" target="_blank" rel="noopener">约束优化方法之拉格朗日乘子法与KKT条件</a>，这篇文章是我觉得阐述的最清晰的一篇关于KTT条件的，对着敲一遍不失为一个比较好的加深记忆的办法。<br>拉格朗日乘子法可以用解决带有约束条件的最优化问题，约束条件又分为等式约束和不等式约束。对于等式约束，可以直接应用拉格朗日乘子法求得最优解，对于含有不等式约束的最优化问题，可以转化为在满足KTT条件下的拉格朗日乘子法求解。拉格朗日乘子法求得的并不一定是最优解，只有在凸优化的情况下，才能保证得到的是最优解，所以本文称拉格朗日乘子法得到的为可行解，其实就是局部极小值。</p><h3 id="无约束优化"><a href="#无约束优化" class="headerlink" title="无约束优化"></a>无约束优化</h3><p>首先考虑一个不带任何约束的优化问题，对于变量$x\in\mathbb{R}^N$的函数$f(x)$，无约束优化问题如下：</p><script type="math/tex; mode=display">\underset{x}{min}f(x)</script><p>该问题很好解，根据Fermat定理，直接找到使目标函数得0的点即可即$\bigtriangledown_xf(x)=0$，如果没有解析解的话，可以使用梯度下降或牛顿方法等迭代的手段来使$x$沿负梯度方向逐步逼近极小值点。</p><h3 id="等式约束优化"><a href="#等式约束优化" class="headerlink" title="等式约束优化"></a>等式约束优化</h3><p>当目标函数加上约束条件之后，问题就变成如下形式：</p><script type="math/tex; mode=display">\underset{x}{min}f(x)</script><script type="math/tex; mode=display">s.t.\quad h_i(x)=0,i=1,2,...,m</script><p>约束条件会将解的范围限定在一个可行域，此时不一定能找到使得$\bigtriangledown_xf(x)=0$的点，只需找到在可行域内使得$f(x)$最小的值即可，常用的方法即为拉格朗日乘子法，该方法首先引入Lagrange Multiplier $\alpha\in\mathbb{R}^m$，构建拉格朗日等式如下：</p><script type="math/tex; mode=display">L(x,\alpha)=f(x)+\sum^m_{i=1}\alpha_ih_i(x)</script><p>求解方法如下：首先基于等式对$\alpha$和$x$求：</p><script type="math/tex; mode=display">\left\{\begin{matrix}\bigtriangledown_xL(x,\alpha)=0\\\bigtriangledown_{\alpha}L(x,\alpha)=0\end{matrix}\right.</script><p>令导数为0，求得$x$，$\alpha$的值后，将$x$带入$f(x)$即为在约束条件$h_i(x)$下的可行解。这样做的意义是什么呢? 接下来看一个直观的示例，对于二维情况下的目标函数是$f(x,y)$，平面中画出$f(x,y)$的等高线，如下图的虚线所示，并只给出一个约束等式$h(x,y)=0$，如下图的绿线所示，目标函数$f(x,y)$与约束$g(x,y)$只有三种情况，相交、相切或者没有交集，没交集肯定不是解，只有相交或者相切可能是解，但相交得到的一定不是最优值，因为相交意味着肯定还存在其它的等高线在该条等高线的内部或者外部，使得新的等高线与目标函数的交点的值更大或者更小，这就意味着只有等高线与目标函数的曲线相切的时候，才可能得到可行解。</p><p><img src="/2020/05/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95%E4%B8%8EL1%E5%92%8CL2%E6%AD%A3%E5%88%99%E9%A1%B9/image01.png" alt></p><p>因此给出结论：拉格朗日乘子法取得极值的必要条件是目标函数与约束函数相切，这时两者的法向量是平行的，即：</p><script type="math/tex; mode=display">\bigtriangledown_{x}f(x)-\alpha\bigtriangledown_{x}h(x)=0</script><p>所以只要满足上述等式，且满足之前的约束$h_i(x)=0,i=1,2,…,m$，即可得到解，联立起来，正好得到就是拉格朗日乘子法。</p><h3 id="不等式约束优化"><a href="#不等式约束优化" class="headerlink" title="不等式约束优化"></a>不等式约束优化</h3><p>当约束加上不等式之后，情况变得更加复杂，首先来看一个简单的情况，给定如下不等式约束问题：</p><script type="math/tex; mode=display">\underset{x}{min}f(x)</script><script type="math/tex; mode=display">s.t.\quad g(x)\leq 0</script><p>对应拉格朗日式子与图形分别如下所示：</p><script type="math/tex; mode=display">L(x,\lambda)=f(x)+\lambda g(x)</script><p>这时的可行解必须落在约束区域$g(x)$之内，下图给出了目标函数的等高线与约束：</p><p><img src="/2020/05/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95%E4%B8%8EL1%E5%92%8CL2%E6%AD%A3%E5%88%99%E9%A1%B9/image02.png" alt></p><p>由图可见可行解$x$只能在$g(x)&lt;0$或者$g(x)=0$的区域里取得：</p><ul><li>当可行解$x$落在$g(x)&lt;0$的区域内，此时直接极小化$f(x)$即可；</li><li>当可行解$x$落在$g(x)=0$即边界上，此时等价于等式约束优化问题.</li></ul><p>当约束区域包含目标函数原有的可行解时，此时加上约束可行解扔落在约束区域内部，对应$g(x)&lt;0$的情况，这时约束条件不起作用；当约束区域不包含目标函数原有的可行解时，此时加上约束后可行解落在边界$g(x)=0$上。下图分别描述了两种情况，右图表示加上约束可行解会落在约束区域的边界上。</p><p><img src="/2020/05/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95%E4%B8%8EL1%E5%92%8CL2%E6%AD%A3%E5%88%99%E9%A1%B9/image03.png" alt></p><p>以上两种情况就是说，要么可行解落在约束边界上即得$g(x)=0$，要么可行解落在约束区域内部，此时约束不起作用，另$\lambda=0$消去约束即可，所以无论哪种情况都会得到：</p><script type="math/tex; mode=display">\lambda g(x)=0</script><p>还有一个问题是$\lambda$的取值，在等式约束优化中，约束函数与目标函数的梯度只要满足平行即可，而在不等式约束中则不然，若$\lambda\neq0$，这便说明可行解$x$是落在约束区域的边界上的，这时可行解应尽量靠近无约束时的解，所以在约束边界上，目标函数的负梯度方向应该远离约束区域朝向无约束时的解，此时正好可得约束函数的梯度方向与目标函数的负梯度方向应相同：</p><script type="math/tex; mode=display">-\bigtriangledown_xf(x)=\lambda\bigtriangledown_xg(x)</script><p>上式需要满足的要求是拉格朗日乘子$\lambda&gt;0$，这个问题可以举一个形象的例子，假设你去爬山，目标是山顶，但有一个障碍挡住了通向山顶的路，所以只能沿着障碍爬到尽可能靠近山顶的位置，然后望着山顶叹叹气，这里山顶便是目标函数的可行解，障碍便是约束函数的边界，此时的梯度方向一定是指向山顶的，与障碍的梯度同向，下图描述了这种情况:</p><p><img src="/2020/05/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95%E4%B8%8EL1%E5%92%8CL2%E6%AD%A3%E5%88%99%E9%A1%B9/image04.png" alt></p><p>可见<strong>对于不等式约束，只要满足一定的条件，依然可以使用拉格朗日乘子法解决，这里的条件便是KKT条件</strong>。接下来给出形式化的KKT条件首先给出形式化的不等式约束优化问题：</p><script type="math/tex; mode=display">\underset{x}{min}f(x)</script><script type="math/tex; mode=display">s.t.\quad h_i(x)=0,i=1,2,...,m</script><script type="math/tex; mode=display">s.t.\quad g_j(x)\leq0,j=1,2,...,n</script><p>列出无约束优化问题的拉格朗日方程得到： </p><script type="math/tex; mode=display">L(x,\alpha,\beta)=f(x)+\sum^m_{i=1}\alpha_ih_i(x)+\sum^n_{j=1}\beta_jg_j(x)</script><p>经过之前的分析，便得知加上不等式约束后可行解$x$需要满足的就是以下的KKT条件：</p><script type="math/tex; mode=display">\begin{aligned}\bigtriangledown_xL(x,\alpha,\beta)&=0 &(1)\\ \beta_jg_j(x)&=0,j=1,2,...,n &(2)\\ h_i(x)&=0,i=1,2,...,m &(3)\\ g_j(x)&\leq0,j=1,2,...,n &(4)\\ \beta_j&\geq0,j=1,2,...,n &(5)\end{aligned}</script><p>满足KKT条件后极小化拉格朗日公式即可得到在不等式约束条件下的可行解。KKT条件看起来很多，其实很好理解:<br><strong>(1)</strong>:拉格朗日取得可行解的必要条件；<br><strong>(2)</strong>:这就是以上分析的一个比较有意思的约束，称作松弛互补条件；<br><strong>(3)~(4)</strong>:初始的约束条件；<br><strong>(5)</strong>:不等式约束的拉格朗日乘子需满足的条件。<br>主要的KKT条件便是(3)和(5)，只要满足这俩个条件便可直接用拉格朗日乘子法。</p><h2 id="L1和L2正则"><a href="#L1和L2正则" class="headerlink" title="L1和L2正则"></a>L1和L2正则</h2><p>现在，我们用拉格朗日乘子法的解空间来理解L1和L2正则，有时候，相对于约束参数而言，L1又叫lasso回归，L2又叫岭回归(Ridge Regression)。<br>现在有一个损失函数$L(y,\hat{y})$，引入L1正则： </p><script type="math/tex; mode=display">J(w)=L(y,\hat{y};w)+\lambda\sum^n_{i=1}|w_i| \tag{6}</script><p>引入L2正则： </p><script type="math/tex; mode=display">J(w)=L(y,\hat{y};w)+\frac{1}{2}\lambda\sum^n_{i=1}w_i^2 \tag{7}</script><p>可以看到他们就是在求等式约束时拉格朗日公式的普通形式。观察两维，即取$w=1,2$，(6)式的正则项为$|w_1|+|w_2|$它和损失函数组成下右图，(7)式的正则项为$w_1^2+w_2^2$，它和损失函数组成下面左图。</p><p><img src="/2020/05/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95%E4%B8%8EL1%E5%92%8CL2%E6%AD%A3%E5%88%99%E9%A1%B9/image05.png" alt></p><p>现在我们讨论可行解落在约束边界上的情况，比较清晰的看出来，对于L1正则，受到约束的最优解$w^*$往往在棱形的顶点取到，而顶点在某一个维度上的投影为0(图中对应$w_1$这个轴，而$w_2$这个维度被选择了出来)，而对于L2正则，受到约束的最优解$w^*$可以在圆上的任意一点取到。<br>接下来就可以看图说说L1和L2的特点了，L1会趋向于产生少量的特征，而其他的特征都是0，故L1正则化导出的稀疏性质已被广泛用于特征选择，而L2会选择更多的特征，这些特征都会接近于0，更常见于做正则化。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>   <a href="https://www.cnblogs.com/ooon/p/5721119.html" target="_blank" rel="noopener">约束优化方法之拉格朗日乘子法与KKT条件</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 拉格朗日乘子法 </tag>
            
            <tag> L1正则化 </tag>
            
            <tag> L2正则化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>09 Question Answering, the Default Final Project, and an introduction to Transformer architectures</title>
      <link href="/2020/05/12/Question%20Answering,%20the%20Default%20Final%20Project,%20and%20an%20introduction%20to%20Transformer%20architectures/"/>
      <url>/2020/05/12/Question%20Answering,%20the%20Default%20Final%20Project,%20and%20an%20introduction%20to%20Transformer%20architectures/</url>
      
        <content type="html"><![CDATA[<h2 id="Motivation-Question-answering"><a href="#Motivation-Question-answering" class="headerlink" title="Motivation: Question answering"></a>Motivation: Question answering</h2><p>问答系统(Question Answering, QA)是指对检索到的文档进行阅读理解，抽取出能回答问题的答案</p><h2 id="Stanford-Question-Answering-Dataset-SQuAD"><a href="#Stanford-Question-Answering-Dataset-SQuAD" class="headerlink" title="Stanford Question Answering Dataset(SQuAD)"></a>Stanford Question Answering Dataset(SQuAD)</h2><p>2015~2016年，几个大规模QA标注数据集的发表，极大的推动了这个领域的发展。这其中比较有名的数据集是斯坦福大学发布的Stanford Question Answering Dataset(SQuAD)。</p><p><img src="/2020/05/12/Question%20Answering,%20the%20Default%20Final%20Project,%20and%20an%20introduction%20to%20Transformer%20architectures/image01.png" alt></p><ul><li>文章是来自维基百科的一段文本，系统需要回答问题，在文章中找出答案</li><li>答案直接直接来自于文章，也就是提取式问答(给出答案所在的收尾位置之间的单词序列)</li><li>SQuAD由100k个examples构造而成</li></ul><p><img src="/2020/05/12/Question%20Answering,%20the%20Default%20Final%20Project,%20and%20an%20introduction%20to%20Transformer%20architectures/image02.png" alt></p><p>为了增加数据集的难度，斯坦福后续推出升级版本SQuAD2.0数据集中增加了没有答案的问题，可以理解为假问题，对系统造成干扰。此时就要求系统判断这个问题能否从文章描述中获得答案，如果没有答案的话，就不输出任何回答\<No answer\></No></p><h2 id="Stanford-Attentive-Reader"><a href="#Stanford-Attentive-Reader" class="headerlink" title="Stanford Attentive Reader"></a>Stanford Attentive Reader</h2><h3 id="Stanford-Attentive-Reader-1"><a href="#Stanford-Attentive-Reader-1" class="headerlink" title="Stanford Attentive Reader++"></a>Stanford Attentive Reader++</h3><h2 id="BiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension"><a href="#BiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension" class="headerlink" title="BiDAF: Bi-Directional Attention Flow for Machine Comprehension"></a>BiDAF: Bi-Directional Attention Flow for Machine Comprehension</h2><h2 id="Recent-more-advanced-architectures"><a href="#Recent-more-advanced-architectures" class="headerlink" title="Recent, more advanced architectures"></a>Recent, more advanced architectures</h2><h3 id="Dynamic-CoattentionNetworks-for-Question-Answering"><a href="#Dynamic-CoattentionNetworks-for-Question-Answering" class="headerlink" title="Dynamic CoattentionNetworks for Question Answering"></a>Dynamic CoattentionNetworks for Question Answering</h3><h3 id="Coattention-Encoder"><a href="#Coattention-Encoder" class="headerlink" title="Coattention Encoder"></a>Coattention Encoder</h3><h3 id="FusionNet-Huang-Zhu-Shen-Chen-2017"><a href="#FusionNet-Huang-Zhu-Shen-Chen-2017" class="headerlink" title="FusionNet(Huang, Zhu, Shen, Chen 2017)"></a>FusionNet(Huang, Zhu, Shen, Chen 2017)</h3><h3 id="Multi-level-inter-attention"><a href="#Multi-level-inter-attention" class="headerlink" title="Multi-level inter-attention"></a>Multi-level inter-attention</h3><h2 id="ELMo-and-BERT-preview"><a href="#ELMo-and-BERT-preview" class="headerlink" title="ELMo and BERT preview"></a>ELMo and BERT preview</h2><p>此部分内容之前已经在<a href="https://blog.csdn.net/sunhua93/article/details/102764783" target="_blank" rel="noopener">彻底搞懂BERT</a>文章中学习过</p>]]></content>
      
      
      <categories>
          
          <category> cs224n学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs224n课程 </tag>
            
            <tag> nlp </tag>
            
            <tag> QA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>08 Machine Translation, Sequence-to-sequence and Attention</title>
      <link href="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/"/>
      <url>/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/</url>
      
        <content type="html"><![CDATA[<h2 id="Section-1-Pre-Neural-Machine-Translation"><a href="#Section-1-Pre-Neural-Machine-Translation" class="headerlink" title="Section 1: Pre-Neural Machine Translation"></a>Section 1: Pre-Neural Machine Translation</h2><p>机器翻译(MT)是将一个句子$x$从一种语言(源语言)转换为另一种语言(目标语言)的句子$y$的任务。   </p><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image01.png" alt></p><h3 id="1990s-2010s-Statistical-Machine-Translation"><a href="#1990s-2010s-Statistical-Machine-Translation" class="headerlink" title="1990s-2010s: Statistical Machine Translation"></a>1990s-2010s: Statistical Machine Translation</h3><p>统计机器翻译的核心是基于概率模型的。加入我们要将法语翻译成英语，我们需要找到最好的英语句子$y$，对于给定的法语句子$x$，求： </p><script type="math/tex; mode=display">argmax_yP(y|x)</script><p>通过贝叶斯规则可以将其拆解为两个部分分别进行学习</p><script type="math/tex; mode=display">argmax_yP(x|y)P(y)</script><p>其中$P(x|y)$专注于翻译模型，翻译好局部的短语或者单词，使得翻译更加准确无误，它可以并行的从数据中进行学习。$P(y)$专注于语言模型，用来学习整个句子$y$的概率，使得翻译出来的句子更加的流利通顺，它需要从自身语言的数据中进行学习</p><h3 id="What-is-alignment"><a href="#What-is-alignment" class="headerlink" title="What is alignment?"></a>What is alignment?</h3><p>SMT进一步把$P(x|y)$分解成$P(x,a|y)$，其中$a$表示一个对齐alignment，可以认为是两种语言之间单词和单词或短语和短语的一个对齐关系。</p><ul><li>有些词是没有对应词的</li></ul><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image02.png" alt></p><ul><li>对齐本身就很复杂，存在1对1，多对1，1对多，多对多等情况</li></ul><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image03.png" alt></p><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image04.png" alt></p><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image05.png" alt></p><ul><li>有些词很丰富</li></ul><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image06.png" alt></p><h3 id="Decoding-for-SMT"><a href="#Decoding-for-SMT" class="headerlink" title="Decoding for SMT"></a>Decoding for SMT</h3><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image07.png" alt></p><p>如何计算$argmax_y$?  </p><ul><li>我们可以列举所有可能的$y$并计算概率?这样的话代价太大</li><li>使用启发式搜索算法搜索最佳翻译，丢弃概率过低的假设</li><li>这个过程称为解码</li></ul><p>解码可以使用Viterbi解码法，假设模型为独立的，则有： </p><script type="math/tex; mode=display">p(x,a|y)=\prod^{l_x}_{j=1}p(x_j|f_{a(j)})</script><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image08.png" alt></p><p>SMT是一个巨大的研究领域，有很多的子模块，需要很多的人工干预和特征工程。</p><h2 id="Section-2-Neural-Machine-Translation"><a href="#Section-2-Neural-Machine-Translation" class="headerlink" title="Section 2: Neural Machine Translation"></a>Section 2: Neural Machine Translation</h2><h3 id="What-is-Neural-Machine-Translation"><a href="#What-is-Neural-Machine-Translation" class="headerlink" title="What is Neural Machine Translation?"></a>What is Neural Machine Translation?</h3><ul><li>神经机器翻译是利用单个神经网络进行机器翻译的一种方法</li><li>神经网络架构称为sequence-to-sequence(又名seq2seq)，它包含两个RNNs<h3 id="Neural-Machine-Translation-NMT"><a href="#Neural-Machine-Translation-NMT" class="headerlink" title="Neural Machine Translation(NMT)"></a>Neural Machine Translation(NMT)</h3><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image09.png" alt></li></ul><p>Seq2seq两个RNN组成，左边的红色部分称为Encoder RNN，它负责对源语言进行编码，右边的绿色部分称为Decoder RNN，它负责对目标语言进行解码。<br>Encoder RNN可以是任意一个RNN，比如朴素RNN、LSTM或者GRU。Encoder RNN负责对源语言进行编码，学习源语言的隐含特征。Encoder RNN的最后一个神经元的隐状态作为Decoder RNN的初始隐状态。<br>Decoder RNN是一个条件语言模型，一方面它是一个语言模型，即用来生成目标语言，另一方面，它的初始隐状态是基于Encoder RNN的输出，所以称Decoder RNN是条件语言模型。Decoder RNN在预测的时候，需要把上一个神经元的输出作为下一个神经元的输入，不断的预测下一个词，直到预测输出了结束标志符END而停止预测。Encoder RNN的输入是源语言的word embeding，Decoder RNN的输入是目标语言的word embeding。</p><p>Seq2seq不仅仅对MT有用，许多NLP任务可以按照顺序进行表达： </p><ul><li>摘要(长文本$\rightarrow$短文本)</li><li>对话(前一句话$\rightarrow$下一句话)</li><li>解析(输入文本$\rightarrow$输出解析为序列)</li><li>代码生成(自然语言$\rightarrow$Python代码)</li></ul><p>Seq2seq作为一个条件语言模型，它直接计算$P(y|x)$，在生成$y$的过程中，始终有$x$作为条件</p><script type="math/tex; mode=display">P(y|x)=P(y_1|x)P(y_2|y_1,x)P(y_3|y_1,y_2,x)...P(y_T|y_1,...,y_{T-1},x)</script><p>上式中最后一项为，给定到目前为止的目标词和源句$x$，下一个目标词的概率</p><h3 id="Training-a-Neural-Machine-Translation-system"><a href="#Training-a-Neural-Machine-Translation-system" class="headerlink" title="Training a Neural Machine Translation system"></a>Training a Neural Machine Translation system</h3><p>seq2seq的训练过程如下图所示，训练的时候，我们同时需要源语言和翻译好的目标语言，分别作为Encoder RNN和Deocder RNN的输入。Decoder RNN在训练阶段，每一个时刻的输入是提供的正确翻译词，输出是预测的下一个时刻词的概率分布，比如在$t=4$时刻，预测输出是$\hat{y}_4$<br>，而正确答案是”with”，根据交叉熵损失函数，$J_4=-logP(“with”)$，总的损失函数就是所有时刻的损失均值。</p><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image10.png" alt></p><p>seq2seq的预测过程，实际上是一个贪心的预测过程，即在Decoder RNN的每一步都贪心选择概率$\hat{y}_t$最大的那个词。如下图，贪心只能保证每一步最优，不能保证全局最优，贪心算法没有办法回溯，但是如果每个时刻都穷举所有可能的话，时间复杂度为$O(V^T)$，太高了</p><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image11.png" alt></p><h3 id="Beam-search-decoding"><a href="#Beam-search-decoding" class="headerlink" title="Beam search decoding"></a>Beam search decoding</h3><p>Beam search搜索策略是贪心策略和穷举策略的一个折中方案，它在预测的每一步，都保留Top-k高概率的词，作为下一个时间步的输入。k称为beam size，k越大，得到更好结果的可能性更大，但计算消耗也越大。这里的Top-k高概率不仅仅指当前时刻$\hat{y}_t$<br>的最高概率，而是截止目前这条路径上的累计概率之和</p><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image12.png" alt></p><h4 id="Beam-search-decoding-example"><a href="#Beam-search-decoding-example" class="headerlink" title="Beam search decoding: example"></a>Beam search decoding: example</h4><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image13.png" alt></p><p>假设Beam size: $k = 2$，蓝色的数字是$score(y_1…,y_t)=\sum^t_{i=1}logP_{LM}(y_i|y_1,…,y_{i-1},x)$的结果： </p><ul><li>第一步：计算每个分支下下一个单词的概率分布</li><li>第二步：取每个分支的前2个单词并计算分数</li><li>第三步：对于每一次的2个假设，找出最前面的2个单词进行分支保留</li></ul><blockquote><p>在$t=2$这个时刻，保留分数最高的hit和was两个单词</p></blockquote><h4 id="Beam-search-decoding-finishing-up"><a href="#Beam-search-decoding-finishing-up" class="headerlink" title="Beam search decoding: finishing up"></a>Beam search decoding: finishing up</h4><p>在beam search的过程中，不同路径预测输出结束标志符<END>的时间点可能不一样，有些路径可能提前结束了，称为完全路径，暂时把这些完全路径放一边，其他路径接着beam search。beam search的停止条件有很多种，可以设置一个最大的搜索时间步数，也可以设置收集到的最多的完全路径数。<br>当beam search结束时，需要从n条完全路径中选一个打分最高的路径作为最终结果。由于不同路径的长度不一样，而beam search打分是累加项，累加越多打分越低，所以需要用长度对打分进行归一化。  </END></p><script type="math/tex; mode=display">\frac{1}{t}\sum^t_{i=1}logP_{LM}(y_i|y_1,...,y_{i-1},x)</script><p>那么，为什么不在beam search的过程中就直接用下面的归一化打分来比较呢？因为在树搜索的过程中，每一时刻比较的两条路径的长度是一样的，即分母是一样的，所以归一化打分和非归一化打分的大小关系是一样的，即在beam search的过程中就没必要对打分进行归一化。</p><h3 id="Advantages-and-Disadvantages-of-NMT"><a href="#Advantages-and-Disadvantages-of-NMT" class="headerlink" title="Advantages and Disadvantages of NMT"></a>Advantages and Disadvantages of NMT</h3><h4 id="Advantages-of-NMT"><a href="#Advantages-of-NMT" class="headerlink" title="Advantages of NMT"></a>Advantages of NMT</h4><ul><li>性能更好，翻译更流程，RNN擅长语言模型，能更好的利用上下文关系，能利用短语相似性</li><li>模型更简单，只需要一个神经网络，端到端训练即可，简单方便</li><li>不需要很多的人工干预和特征工程，对于不同的语言，网络结构保持一样，只需要换一下词向量<h4 id="Disadvantages-of-NMT"><a href="#Disadvantages-of-NMT" class="headerlink" title="Disadvantages of NMT"></a>Disadvantages of NMT</h4></li><li>难以解释，难以调试，难以解释出错原因</li><li>难以控制，比如难以加入一些人工的规则<h3 id="How-do-we-evaluate-Machine-Translation"><a href="#How-do-we-evaluate-Machine-Translation" class="headerlink" title="How do we evaluate Machine Translation?"></a>How do we evaluate Machine Translation?</h3>BLEU(Bilingual Evaluation Understudy)衡量机器翻译结果和人类的翻译结果(标注答案)的n-gram的overlap，overlap越多，打分越高。BLEU很有用，但不完美，因为一个句子有很多不同的翻译办法，所以一个好的翻译可以得到一个糟糕的BLEU score，因为它与人工翻译的n-gram重叠较低。</li></ul><h2 id="Section-3-Attention"><a href="#Section-3-Attention" class="headerlink" title="Section 3: Attention"></a>Section 3: Attention</h2><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image14.png" alt></p><p>最后介绍提升机器翻译性能的一大利器——注意力机制Attention。首先回顾一下朴素的seq2seq模型，我们用Encoder RNN的最后一个神经元的隐状态作为Decoder RNN的初始隐状态，也就是说Encoder的最后一个隐状态向量需要承载源句子的所有信息，成为整个模型的“信息”瓶颈。<br>个人认为，关于attention的原理和应用阐述的最好的文章是这篇<a href="https://blog.csdn.net/sunhua93/article/details/102764783" target="_blank" rel="noopener">彻底搞懂BERT</a>。</p><h3 id="Sequence-to-sequence-with-attention"><a href="#Sequence-to-sequence-with-attention" class="headerlink" title="Sequence-to-sequence with attention"></a>Sequence-to-sequence with attention</h3><p>Attention机制就是为了解决这个“信息”瓶颈而设计的。Attention把Decoder RNN的每个隐层和Encoder RNN的每个隐层直接连起来了，由于有这个捷径，Encoder RNN的最后一个隐状态不再是“信息”瓶颈，信息还可以通过Attention的很多“直连”线路进行传输。<br>具体来说，在$t$时刻，Decoder第$t$时刻的隐状态$s_t$和Encoder所有时刻的隐状态$h_1,…,h_N$做点积，得到N个标量Attention score，作为Encoder每个隐状态的权重，然后使用softmax对这些权重进行归一化，得到Attention distribution。这个Attention distribution相当于Decoder在时刻对Encoder所有隐状态的注意力分布，如下图所示，翻译到”he”这个时刻的注意力主要分布在Encoder的第2和第4个词上，这就是SMT的对齐操作，Attention自动学习到了这种对齐操作，只不过是soft alignment。接下来，对Encoder所有隐状态使用Attention distribution进行加权平均，得到Attention output$a_t$。把$a_t$和该时刻的隐状态$s_t$拼起来再进行非线性变换得到输出$\hat{y}_2$<br>。有时，也可以把上一时刻的Attention output和当前的输入词向量拼起来作为一个新的输入，输入到Decoder RNN中。</p><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image15.png" alt></p><h3 id="Attention-in-equations"><a href="#Attention-in-equations" class="headerlink" title="Attention: in equations"></a>Attention: in equations</h3><ul><li>在$t$时刻上，我们有编码器隐藏状态$h_1,…,h_N\in\mathbb{R}^h$</li><li>我们有解码器隐藏状态$s_t\in\mathbb{R}^h$</li><li>我们得到这一步的注意分数<script type="math/tex; mode=display">e^t=[s_t^Th_1,...,s_t^Th_N]\in\mathbb{R}^N</script></li><li>我们使用softmax得到这一步的注意分布$\alpha^t$<script type="math/tex; mode=display">\alpha^t=softmax(e^t)\in\mathbb{R}^N</script></li><li>我们使用$\alpha^t$来获得编码器隐藏状态的加权和，得到注意力输出$a_t$<script type="math/tex; mode=display">a_t = \sum^N_{i=1}\alpha_i^th_i\in\mathbb{R}^h</script></li><li>最后，我们将注意力输出$a_t$与解码器隐藏状态连接起来，并按照非注意seq2seq模型继续进行<script type="math/tex; mode=display">[a_t;s_t]\in\mathbb{R}^{2h}</script><h3 id="Attention-is-great"><a href="#Attention-is-great" class="headerlink" title="Attention is great"></a>Attention is great</h3></li><li>提高NMT性能，让Decoder更多的关注于时刻$t$需要翻译的词</li><li>解决了NMT的“信息”瓶颈问题</li><li>有助于缓解梯度消失问题，因为Decoder的每一时间步都和所有Encoder的隐状态相连了</li><li>有助于增加NMT的可解释性，解释为什么时刻$t$翻译输出了某个词，可以通过查看Attentioin distribution来解释<h3 id="There-are-several-attention-variants"><a href="#There-are-several-attention-variants" class="headerlink" title="There are several attention variants"></a>There are several attention variants</h3>现在，Attention已经变成了深度学习的一个的通用技术，并不局限于seq2seq和机器翻译。Attention更一般的过程： </li></ul><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image16.png" alt></p><h3 id="Attention-variants"><a href="#Attention-variants" class="headerlink" title="Attention variants"></a>Attention variants</h3><p>Attention还有很多变种，都是在query和values怎样相乘得到Attention scores上做文章</p><p><img src="/2020/05/10/Machine%20Translation,%20Seq2Seq%20and%20Attention/image17.png" alt></p><ul><li>基本的点乘注意力</li><li>乘法注意力</li><li>加法注意力</li></ul>]]></content>
      
      
      <categories>
          
          <category> cs224n学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs224n课程 </tag>
            
            <tag> nlp </tag>
            
            <tag> seq2seq </tag>
            
            <tag> 机器翻译 </tag>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>07 Vanishing Gradients and Fancy RNNs</title>
      <link href="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/"/>
      <url>/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/</url>
      
        <content type="html"><![CDATA[<h2 id="Vanishing-gradient-intuition"><a href="#Vanishing-gradient-intuition" class="headerlink" title="Vanishing gradient intuition"></a>Vanishing gradient intuition</h2><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image01.png" alt></p><p>RNN在不同时都使用共享参数$W$，导致$t+n$时刻的损失对$t$时刻的参数的偏导数存在$W$的指数形式，一旦$W$很小就会导致梯度消失问题。当这些梯度很小的时候，反向传播的越深入，梯度信号就会变得越来越小</p><h2 id="Why-is-vanishing-gradient-a-problem"><a href="#Why-is-vanishing-gradient-a-problem" class="headerlink" title="Why is vanishing gradient a problem?"></a>Why is vanishing gradient a problem?</h2><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image02.png" alt></p><p>在nlp任务中，梯度消失会使得参数更新更多的受到临近词的影响，那些和当前$t$时刻较远的词对当前的参数更新影响很小。上图中$h^{(1)}$对$J^{(2)}(\theta)$的影响就比对$J^{(4)}(\theta)$的影响大。久而久之，因为梯度消失，我们就不知道$t$时刻是真的对$t+n$时刻没影响还是因为梯度消失导致我们没学习到这种影响</p><h2 id="Effect-of-vanishing-gradient-on-RNN-LM"><a href="#Effect-of-vanishing-gradient-on-RNN-LM" class="headerlink" title="Effect of vanishing gradient on RNN-LM"></a>Effect of vanishing gradient on RNN-LM</h2><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image03.png" alt><br>由于梯度的消失，RNN-LMs更善于从顺序近因学习而不是语法近因 ，所以他们犯这种错误的频率比我们希望的要高，假设我们需要预测句子The writer of the books下一个单词，由于梯度消失，books对下一个词的影响比writer对下一个词的影响更大，导致模型错误的预测成了are</p><h2 id="Why-is-exploding-gradient-a-problem"><a href="#Why-is-exploding-gradient-a-problem" class="headerlink" title="Why is exploding gradient a problem?"></a>Why is exploding gradient a problem?</h2><p>如果梯度爆炸，则根据梯度下降的更新公式，参数会一瞬间更新非常大，导致网络震荡，甚至出现Inf或NaN的情况</p><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image04.png" alt>  </p><h2 id="Gradient-clipping-solution-for-exploding-gradient"><a href="#Gradient-clipping-solution-for-exploding-gradient" class="headerlink" title="Gradient clipping: solution for exploding gradient"></a>Gradient clipping: solution for exploding gradient</h2><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image05.png" alt>  </p><ul><li>梯度裁剪 ：如果梯度的范数大于某个阈值，在应用SGD更新之前将其缩小  </li></ul><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image06.png" alt> </p><p>左图是没有梯度裁剪的情况，由于RNN的梯度爆炸问题，导致快接近局部极小值时，梯度很大，参数突然爬上悬崖，然后又飞到右边一个随机的区域，绕过了中间的局部极小值。右图是增加了梯度裁剪之后，更新步伐变小，参数稳定在局部极小值附近</p><p>梯度爆炸相对好解决，想要解决梯度消失，就要靠一个具有独立记忆的RNN  </p><h2 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short-Term Memory (LSTM)"></a>Long Short-Term Memory (LSTM)</h2><ul><li>LSTM在1997年提出用于解决梯度消失问题</li><li>信息被遗忘/写入/读取的选择由三个对应的门控制</li></ul><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image07.png" alt>  </p><p>LSTM的隐层不仅包含隐状态$h_t$，还专门开辟了一个cell来保存过去的“记忆”$c_t$，LSTM希望用$c_t$来传递很久以前的信息，以达到长距离依赖的目的。所以LSTM隐层神经元的输入是上一时刻的隐状态$h_{t-1}$和记忆$c_{t-1}，输出是当前时刻的隐状态$h_t$和希望传递给下一个时刻的记忆$c_t$。</p><ul><li>遗忘门：控制上一个单元状态的保存与遗忘</li><li>输入门：控制写入单元格的新单元内容的哪些部分</li><li>输出门：控制单元的哪些内容输出到隐藏状态</li><li>单元状态：删除(“忘记”)上次单元状态中的一些内容，并写入(“输入”)一些新的单元内容</li><li>隐藏状态：从单元中读取(“output”)一些内容</li><li>Sigmoid函数：所有的门的值都在0到1之间</li><li>通过逐元素的乘积来应用门</li><li>这些是长度相同的向量</li></ul><p>我们有一个输入序列$x^{(t)}$，我们将计算一个隐藏状态$h^{(t)}$和单元状态$c^{(t)}$的序列。在$t$时刻时为了调控遗忘哪些记忆，写入哪些新记忆，LSTM设置了两个门，分别是遗忘门$f^{(t)}$和输入门$i^{(t)}$，它们都是上一时刻的隐状态$h_{t-1}$和当前时刻的输入$x^{(t)}$经过Sigmoid函数输出的结果。 </p><ul><li>$f^{(t)}$控制遗忘哪些记忆：$f^{(t)}\cdot c^{(t-1)}$</li><li>$i^{(t)}$控制写入哪些新记忆：$i^{(t)}\cdot\tilde{c}^{(t)}$，其中$\tilde{c}^{(t)}$即为期望写入的新记忆，它是隐状态$h_{t-1}$和当前时刻的输入$x^{(t)}$经过Tanh函数输出的结果</li></ul><p>最终，新时刻$t$的记忆就是这两部分的组合</p><ul><li>输出门$o^{(t)}$控制哪些记忆需要输出到下一个隐状态$h^{(t)}$，$o^{(t)}$也是隐状态$h_{t-1}$和当前时刻的输入$x^{(t)}$经过Sigmoid函数输出的结果。</li></ul><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image08.png" alt>  </p><h3 id="How-does-LSTM-solve-vanishing-gradients"><a href="#How-does-LSTM-solve-vanishing-gradients" class="headerlink" title="How does LSTM solve vanishing gradients?"></a>How does LSTM solve vanishing gradients?</h3><p>LSTM解决梯度消失最直接的方法就是，遗忘门选择不遗忘，每一时刻遗忘门$f^{(t)}$都选择记住前一时刻的记忆$c^{(t-1)}$，然后直接传递给下一时刻。那么，所有前$t-1$时刻的记忆都会被完整的传递给第$t$时刻，从而对时刻的输出产生影响。LSTM依然存在梯度消失或梯度爆炸问题，但是这两种情况得以缓解，且LSTM性能不错。</p><h3 id="LSTMs-real-world-success"><a href="#LSTMs-real-world-success" class="headerlink" title="LSTMs: real-world success"></a>LSTMs: real-world success</h3><p>2013-2015年，LSTM开始实现最先进的结果，成功的任务包括：手写识别、语音识别、机器翻译、解析、图像字幕。现在(2020年)，其他方法(如Transformers)在某些任务上变得更加主导。</p><h2 id="Gated-Recurrent-Units-GRU"><a href="#Gated-Recurrent-Units-GRU" class="headerlink" title="Gated Recurrent Units (GRU)"></a>Gated Recurrent Units (GRU)</h2><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image09.png" alt>  </p><ul><li>更新门：控制隐藏状态的哪些部分被更新，哪些部分被保留</li><li>重置门：控制之前隐藏状态的哪些部分被用于计算新内容</li></ul><blockquote><p>原课件没有提供GRU的结构示意图，找了一个网上的，上图中的$z_t$对应下面的$u^{(t)}$，其他的将下角标改为上角标对待</p></blockquote><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image10.png" alt>  </p><p>GRU没有cell，但依然保留了门来控制信息的传递。当前时刻的隐状态$h^{(t)}$等于上一时刻的隐状态$h^{(t-1)}$和新写入的隐状态$\tilde{h}^{(t)}$<br>的加权平均，更新门$u^{(t)}$来控制它们之间的比例，$u^{(t)}$是上一个时刻的隐状态$h_{t-1}$和当前时刻的输入$x^{(t)}$经过Sigmoid函数输出的结果。新写入的隐状态$\tilde{h}^{(t)}$通过一个重置门$r^{(t)}$来控制，它也是$h_{t-1}$和$x^{(t)}$经过Sigmoid函数输出的结果。</p><blockquote><p>GRU中的更新门$u^{(t)}$类似于LSTM中的输出门$o^{(t)}$，重置门$r^{(t)}$类似于LSTM中的遗忘门$f^{(t)}$和输入门$i^{(t)}$的组合，GRU中新写入的隐状态$\tilde{h}^{(t)}$类似于LSTM中的记忆$c^{(t)}$</p></blockquote><h2 id="LSTM-vs-GRU"><a href="#LSTM-vs-GRU" class="headerlink" title="LSTM vs GRU"></a>LSTM vs GRU</h2><ul><li>研究人员提出了许多门控RNN变体，其中LSTM和GRU的应用最为广泛</li><li>最大的区别是GRU计算速度更快，参数更少</li><li>没有确凿的证据表明其中一个总是比另一个表现得更好</li><li>LSTM是一个很好的默认选择(特别是当您的数据具有非常长的依赖关系，或者您有很多训练数据时)</li><li>可以根据经验法则：从LSTM开始，但是如果你想要更有效率，就切换到GRU</li></ul><h2 id="Is-vanishing-exploding-gradient-just-a-RNN-problem"><a href="#Is-vanishing-exploding-gradient-just-a-RNN-problem" class="headerlink" title="Is vanishing/exploding gradient just a RNN problem?"></a>Is vanishing/exploding gradient just a RNN problem?</h2><p>对于所有的神经结构(包括前馈和卷积)都是一个问题，尤其是对于深度结构，一些通用解决方法如下：</p><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>因为梯度是在传递的过程中逐渐减小并消失的，如果跨越好几层直接进行连接，能保持远距离信息。  </p><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image11.png" alt>  </p><h3 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h3><p>粗暴的把跨越多层之间的很多神经元都连起来，进一步减弱梯度消失问题。</p><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image12.png" alt>  </p><h3 id="HighwayNet"><a href="#HighwayNet" class="headerlink" title="HighwayNet"></a>HighwayNet</h3><p>灵感来自LSTM，但适用于深度前馈/卷积网络，类似于剩余连接，但标识连接与转换层由动态门控制。</p><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image13.png" alt>  </p><p>虽然所有神经网络都存在梯度消失的问题，但RNN的这个问题更严重，因为它连乘的是相同的权重矩阵W，而且RNN针对的是序列问题，往往更深。</p><h3 id="Bidirectional-RNNs"><a href="#Bidirectional-RNNs" class="headerlink" title="Bidirectional RNNs"></a>Bidirectional RNNs</h3><p>假设我们在对句子进行情感分类，如下图所示。对于terribly这个词，常规RNN，terribly的梯度只能看到左边的信息，看不到右边的信息，因为网络是从左到右的。单独看terribly或者从左往右看，在没有看到exciting时，可能认为terribly是贬义词，但是如果跟右边的exciting结合的话，则意思变为强烈的褒义词，所以有必要同时考虑左边和右边的信息。</p><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image14.png" alt>  </p><p>双向RNN包含两个RNN，一个从左往右，一个从右往左，两个RNN的参数是独立的。最后把两个RNN的输出拼接起来作为整体输出。那么，对于terribly这个词，它的梯度能同时看到左边和右边的信息。</p><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image15.png" alt>  </p><p>由于双向RNN对于某个时刻$t$，既需要知道时刻前的信息(Forward RNN)，又需要知道时刻$t$之后的信息(Backward RNN)，所以双向RNN无法用于学习语言模型，因为语言模型只知道时刻之前的信息，下一时刻的词需要模型来预测。对于包含完整序列的NLP问题，双向RNN应该是默认选择，它通常比单向RNN效果更好。</p><h3 id="Multi-layer-RNNs"><a href="#Multi-layer-RNNs" class="headerlink" title="Multi-layer RNNs"></a>Multi-layer RNNs</h3><p><img src="/2020/05/05/Vanishing%20Gradients%20and%20Fancy%20RNNs/image16.png" alt>  </p><p>前面展示的RNN从时间的维度上来说可以认为是多层的，但是RNN还可以从另一个维度来增加层数。将上一层(RNN layer 1)的输出作为下一层(RNN layer 2)的输入，不断堆叠下去，变成一个多层RNN。通常来说，深度越大，性能越好，但是他的缺点是RNN无法并行化，计算代价过大，所以不会过深</p>]]></content>
      
      
      <categories>
          
          <category> cs224n学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs224n课程 </tag>
            
            <tag> nlp </tag>
            
            <tag> LSTM </tag>
            
            <tag> GRU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>06 The probability of a sentence? Recurrent Neural Networks and Language Models</title>
      <link href="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/"/>
      <url>/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/</url>
      
        <content type="html"><![CDATA[<h2 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h2><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image01.png" alt></p><p>语言模型就是预测一个句子中下一个词的概率分布。如上图所示，假设给定一个句子前缀是the students opened their，语言模型预测这个句子片段下一个词是books、laptops、exams、minds或者其他任意一个词的概率。形式化表示就是计算。</p><script type="math/tex; mode=display">P(x^{(t+1)}|x^{(t)},...,x^{(1)})</script><p>$x^{(t+1)}$表示第$t+1$个位置(时刻)的词是$x$，$x$可以是词典$V$中任意的一个词。</p><p>例如有一段文本$x^{(1)}$,…,$x^{(T)}$，则这段文本的概率(根据语言模型)为</p><script type="math/tex; mode=display">\begin{aligned}P(x^{(1)},...,x^{(T)})&=P(x^{(1)})\times P(x^{(2)}|x^{(1)})\times...\times P(x^{(T)}|x^{(T-1)},...,x^{(1)})\\&=\prod ^T_{t=1}P(x^{(T)}|x^{(T-1)},...,x^{(1)})\end{aligned}</script><p>语言模型可以用在输入法中预测下一个将要输入的词，在谷歌搜索中输入前几个关键词，搜索引擎会自动预测接下来可能的几个词。网上有很多智能AI自动生成新闻、诗歌等等。可以说语言模型是很多NLP任务的基础模块，具有非常重要的作用。</p><h3 id="n-gram-Language-Models"><a href="#n-gram-Language-Models" class="headerlink" title="n-gram Language Models"></a>n-gram Language Models</h3><p>在前-深度学习时代，人们使用n-gram方法来学习语言模型。对于一个句子，n-gram表示句子中连续的n个词，n-gram对于n=1,2,3,4的结果是：</p><ul><li>unigrams: “the”, “students”, “opened”, ”their”</li><li>bigrams: “the students”, “students opened”, “opened their”</li><li>trigrams: “the students opened”, “students opened their”</li><li>4-grams: “the students opened their”</li></ul><p>n-gram方法有一个前提假设，即假设每个词出现的概率只和前n-1个词有关。n-gram的计算方法就是，统计语料库中出现$x^{(t)},…,x^{(t-n+2)}$的次数作为分母，以及在这个基础上再接一个词$x^{(t+1)}$的次数$x^{(t+1)},x^{(t)},…,x^{(t-n+2)}$作为分子，用后者除以前者来近似这个条件概率。</p><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image02.png" alt></p><p>举个例子，假设完整的句子是as the proctor started the clock, the students opened their，需要预测下一个词的概率分布。对于4-gram方法，则只有students opened their对下一个词有影响，前面的词都没有影响。然后我们统计训练集语料库中发现，分母students opened their出现1000次，其后接books即students opened their books出现了400次，所以P(books|students opened their)=400/1000=0.4。类似的，可以算出下一个词为exams的概率是0.1。所以4-gram方法认为下一个词是books的概率更大。</p><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image03.png" alt></p><p>n-gram方法在统计语料库中的n-gram时，对词的顺序是有要求的，即必须要和给定的n-gram的顺序一样才能对频数加1，比如这个例子中只有出现和students opened their顺序一样才行，如果是their students opened则不行。</p><p>n-gram方法虽然能够有效，比如对于上面的例子，预测出books和exams看起来和前面几个词搭配得很好；但是，它有不少的问题，还是上面的例子，其实考虑更前面的词proctor以及clock的话，这很明显是考试场景，后面出现exams的概率应该比books更高才对。</p><p>具体来说，n-gram方法有以下不足：</p><ul><li>考虑的状态有限。n-gram只能看到前n-1个词，无法建模长距离依赖关系，上面就是一个很好的例子。</li><li>稀疏性问题。对于一个稀有的(不常见的)词w，如果他的词组没有在语料库中出现，则分子为0，但w很有可能是正确的，概率至少不是0。比如students opened their petri dishes，对于学生物的学生来说是有可能的，但如果students opened their petri dishes没有在语料库中出现的话，petri dishes的概率就被预测为0了，这是不合理的。当然这个问题可以通过对词典中所有可能的词组频率+1平滑来部分解决。</li><li>更严重的稀疏性问题，如果分母的词组频率在语料库中是0，那么所有词w对应的分子的词组频率就是0了，根本就没法计算概率。这种情况只能使用back-off策略，即如果4-gram太过于稀疏了，则降到3-gram，分母只统计opened their的频率。一般的，虽然n-gram中的n越大，语言模型预测越准确，但其稀疏性越严重。n其实就相当于维度，我们知道在空间中，维度越高越稀疏，高维空间非常稀疏。对于n-gram，一般取n&lt;=5。</li><li>存储问题，需要存储所有n-gram的频率，如果n越大，这种n-gram的组合越多，所以存储空间呈幂次上升。</li></ul><p>下面是一个更直观的trigram稀疏性问题的例子，由于语料库中统计到的today the company和today the bank的词组频率相同，导致company和bank算出来的概率相等，无法区分。就是因为这两个trigram在预料中出现都比较少，很稀疏，导致统计数据难以把他们区分开来。</p><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image04.png" alt></p><h3 id="A-fixed-window-neural-Language-Model"><a href="#A-fixed-window-neural-Language-Model" class="headerlink" title="A fixed-window neural Language Model"></a>A fixed-window neural Language Model</h3><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image05.png" alt></p><p>window-based neural model在第三讲中被用于NER问题，方法是对一个词开一个小窗口，然后利用词向量和全连接网络识别词的类别。仿照这个方法，也可以用基于窗口的方法来学习语言模型。</p><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image06.png" alt></p><p>超越n-gram语言模型的改进有： </p><ul><li>没有稀疏性问题，它不要求语料库中出现n-gram的词组，它仅仅是把每个独立的单词的词向量组合起来。只要有词向量，就有输入，至少整个模型能顺利跑通。</li><li>不需要观察到所有的n-grams，节省存储空间，只需要存储每个独立的词的词向量。</li></ul><p>存在的问题： </p><ul><li>固定窗口太小，受限于窗口大小，不能感知远距离的关系。</li><li>扩大窗口就需要扩大权重矩阵$W$，导致网络变得复杂。</li><li>输入$e^{(1)},…,e^{(4)}$对应$W$的不同列，每个$e$对应的权重完全是独立的，没有共享关系，导致训练效率比较低。</li></ul><p>我们需要一个神经结构，可以处理任何长度的输入</p><h3 id="RNN-Language-Model"><a href="#RNN-Language-Model" class="headerlink" title="RNN Language Model"></a>RNN Language Model</h3><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image07.png" alt></p><p>RNN输入可以是任意长度，特别适合对顺序敏感的序列问题进行建模。对于$t$时刻的输入词$x^{(t)}$来说，首先把它转为词向量$e^{(t)}$作为RNN的输入，然后对于隐藏层，它的输入来自于$t$时刻的输入变换$W_ee^{(t)}$和上一时刻的隐藏层变换$W_hh^{(t-1)}$，这两部分组合起来再做一个非线性变换，得到当前层的隐状态$h^{(t)}$，最后，隐状态再接一个softmax层，得到该时刻的输出概率分布$\hat{y}^{(t)}$<br>整个网络中，不同时刻的权重矩阵$W$是共享的，不同的是不同时刻的输入$x^{(t)}$和隐状态$h^{(t)}$，词向量$e$可以是pre-trained得到的，然后固定不动，也可以根据实际任务进行fine-tune，甚至可以完全随机，在实际任务中现场学习。常规任务中，最好还是用pre-trained的词向量，如果数据量很大的话，再考虑fine-tune  </p><p>RNN的<strong>优点</strong>：</p><ul><li>可以处理任意长度的输入</li><li>状态$t$可以感知很久以前的状态</li><li>模型大小是固定的，因为不同时刻的参数都是共享的</li></ul><p>RNN的<strong>缺点</strong>：</p><ul><li>训练起来很慢，因为后续状态需要用到前面的状态，是串行的，难以并行计算</li><li>虽然理论上$t$时刻可以感知很久以前的状态，但实际上很难，因为梯度消失的问题</li></ul><h4 id="Training-a-RNN-Language-Model"><a href="#Training-a-RNN-Language-Model" class="headerlink" title="Training a RNN Language Model"></a>Training a RNN Language Model</h4><ul><li>训练RNN依然是梯度下降，输入RNN-LM，计算每个时刻$t$的输出分布，即预测到目前为止给定的每个单词的概率分布。</li><li>RNN在$t$时刻的输出是预测$t+1$个词的概率分布$\hat{y}^{(t)}$，而训练集中第$t+1$个词是已知的单词，所以真实答案$y^{(t)}$其实是一个one-hot向量，在第$x^{(t+1)}$的位置为1，其它位置为0，所以如果是交叉熵损失函数的话，表达式如下图中间的等式。<script type="math/tex; mode=display">J^{(t)}(\theta)=crossentropy(y^{(t)},\hat{y}^{(t)})=-\sum_{w\in V}y_w^{(t)}log\hat{y}_w^{(t)}=-log\hat{y}_{x_{t+1}}^{(t)}</script></li><li>RNN整体的损失就是所有时刻的损失均值。<script type="math/tex; mode=display">J(\theta)=\frac{1}{T}\sum^T_{t=1}J^{(t)}(\theta)=\frac{1}{T}\sum^T_{t=1}-log\hat{y}_{x_{t+1}}^{(t)}</script></li></ul><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image08.png" alt></p><p>因为完整的语料库通常是非常大，比如上百万篇文章，这么长的输入，训练起来就很费劲，所以输入RNN的往往是以句子或者单篇文档为单位，然后使用SGD，小batch进行批量训练。</p><h4 id="Backpropagation-for-RNNs"><a href="#Backpropagation-for-RNNs" class="headerlink" title="Backpropagation for RNNs"></a>Backpropagation for RNNs</h4><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image09.png" alt><br>对于一个多变量函数$f(x,y)$和两个单变量函数$x(t)$和$y(t)$，其链式法则如下：</p><script type="math/tex; mode=display">\frac{d}{dt}f(x(t),y(t))=\frac{\partial f}{\partial x}\frac{dx}{dt}+\frac{\partial f}{\partial y}\frac{dy}{dt}</script><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image10.png" alt></p><p>关于重复的权重矩阵$W_h$的偏导数$J^{(t)}(\theta)$是每次其出现时的梯度的总和</p><script type="math/tex; mode=display">\frac{\partial J^{(t)}}{\partial W_h}=\sum^t_{i=1}\frac{\partial J^{(t)}}{\partial W_h}|_{(i)}</script><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image11.png" alt></p><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image12.png" alt></p><p>反向传播用时间步长计算累加梯度$i=t,…,0$。这个算法叫做“backpropagation through time”(BPTT)</p><h4 id="Generating-text-with-a-RNN-Language-Model"><a href="#Generating-text-with-a-RNN-Language-Model" class="headerlink" title="Generating text with a RNN Language Model"></a>Generating text with a RNN Language Model</h4><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image13.png" alt></p><p>就像n-gram语言模型一样，可以使用RNN语言模型通过重复采样来生成文本。注意上一步的输出正好是下一步的输入，这样语义才连贯.</p><ul><li>相比n-gram更流畅，语法正确，但总体上仍然很不连贯</li><li>食谱的例子中，生成的文本并没有记住文本的主题是什么</li><li>哈利波特的例子中，甚至有体现出了人物的特点，并且引号的开闭也没有出现问题（符号也有被神经元或者隐藏状态跟踪）</li><li>RNN是否可以和手工规则结合？可以使用Beam Serach，但是可能很难做到</li></ul><h3 id="Evaluating-Language-Models"><a href="#Evaluating-Language-Models" class="headerlink" title="Evaluating Language Models"></a>Evaluating Language Models</h3><p>语言模型的评价指标是perplexity——困惑度，其计算公式如下。</p><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image14.png" alt></p><p>这等于交叉熵损失$J(θ)$的指数。困惑度和损失函数是正相关的，损失越低，则困惑度也越小，模型性能越好</p><p><img src="/2020/05/03/The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/image15.png" alt></p><h3 id="Why-should-we-care-about-Language-Modeling"><a href="#Why-should-we-care-about-Language-Modeling" class="headerlink" title="Why should we care about Language Modeling?"></a>Why should we care about Language Modeling?</h3><ul><li>RNNs can be used for tagging(词性标注, 命名实体识别)</li><li>RNNs can be used for sentence classification(情感分类)</li><li>Enbeddin编码(使用最终隐层状态或者使用所有隐层状态的逐元素最值或均值)</li><li>RNNs can be used as an encoder module(机器翻译、问答等多中任务)</li><li>RNN-LMs can be used to generate text(语音辨识, 机器翻译, 摘要生成)</li></ul><p>RNN可保证上述的任务的基本实现。</p>]]></content>
      
      
      <categories>
          
          <category> cs224n学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs224n课程 </tag>
            
            <tag> nlp </tag>
            
            <tag> RNN </tag>
            
            <tag> n-gram </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>05 Linguistic Structure Dependency Parsing</title>
      <link href="/2020/04/26/Linguistic%20Structure%20Dependency%20Parsing/"/>
      <url>/2020/04/26/Linguistic%20Structure%20Dependency%20Parsing/</url>
      
        <content type="html"><![CDATA[<p>对于句法结构(syntactic structure)分析，主要有两种方式：Constituency Parsing与Dependency Parsing</p><h2 id="Constituency-Parsing"><a href="#Constituency-Parsing" class="headerlink" title="Constituency Parsing"></a>Constituency Parsing</h2><p>Constituency Parsing主要用phrase structure grammer即短语语法来不断的将词语整理成嵌套的组成成分，又被称为context-free grammers，简写做CFG<br>其主要步骤是先对每个词做词性分析part of speech, 简称POS，然后再将其组成短语，再将短语不断递归构成更大的短语</p><p><img src="/2020/04/26/Linguistic%20Structure%20Dependency%20Parsing/image01.png" alt></p><p>例如，对于 the cuddly cat by the door, 先做POS分析，the是限定词，用Det(Determiner)表示，cuddly是形容词，用Adj(Adjective)代表，cat和door是名词，用N(Noun)表示, by是介词，用P(Preposition)表示。<br>然后the cuddly cat构成名词短语NP(Noun Phrase)，这里由Det(the)+Adj(cuddly)+N(cat)构成，by the door构成介词短语PP(Preposition Phrase), 这里由P(by)+NP(the door)构成。<br>最后，整个短语the cuddly cat by the door 是NP，由NP（the cuddly cat）+ PP(by the door)构成。</p><h2 id="Dependency-Parsing"><a href="#Dependency-Parsing" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h2><p>Dependency Structure展示了词语之前的依赖关系,通常用箭头表示其依存关系，有时也会在箭头上标出其具体的语法关系，如是主语还是宾语关系等。<br>Dependency Structure有两种表现形式，一种是直接在句子上标出依存关系箭头及语法关系，如 ：  </p><p><img src="/2020/04/26/Linguistic%20Structure%20Dependency%20Parsing/image02.png" alt></p><p>另一种是将其做成树状机构（Dependency Tree Graph）</p><p>Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas<br>堪萨斯州共和党参议员布朗巴克(Brownback)提交了有关港口和移民的法案</p><p><img src="/2020/04/26/Linguistic%20Structure%20Dependency%20Parsing/image03.png" alt></p><p>Dependency Parsing可以看做是给定输入句子$S=w_0w_1…w_n$（其中$w_0$常常是fake ROOT，使得句子中每一个词都依赖于另一个节点）构建对应的Dependency Tree Graph的任务。而这个树如何构建呢？一个有效的方法是Transition-based Dependency Parsing</p><h3 id="Transition-based-Dependency-Parsing"><a href="#Transition-based-Dependency-Parsing" class="headerlink" title="Transition-based Dependency Parsing"></a>Transition-based Dependency Parsing</h3><p>Transition-based Dependency Parsing可以看做是状态机，对于$S=w_0w_1…w_n$，其状态由三部分构成$(\sigma,\beta,A)$<br>$\sigma$是$S$中若干$w_i$构成的堆(stack)<br>$\beta$是$S$中若干$w_i$构成的缓冲(buffer)<br>$A$是$w_i$之间的关系依存弧构成的集合，每一条边的形式是$(w_i,r,w_j)$，其中$r$描述了节点的依存关系(如动宾关系等)<br>初始状态时，$\sigma$仅包含ROOT$w_0$，$\beta$包含了所有的单词$w_1…w_n$，而$A$是空集$\varnothing$。最终的目标是$\sigma$包含ROOTT$w_0$，$\beta$清空，而$A$包含了所有关系， $A$就是我们想要的描述Dependency的结果<br>其含义是对于$S$中所有的单词都找到了相互的关系<br>状态之间的转移有三类 ： </p><ul><li>移除在缓冲区的第一个单词，然后将其放在堆的顶部（前提条件：缓冲区不能为空）。</li><li>LEFT-ARC：向依存弧集合$A$中加入一个依存弧$(w_j,r,w_i)$，其中$w_i$是堆顶的第二个单词，$w_j$堆顶部的单词。从堆栈中移除$w_i$。</li><li>RIGHT-ARC:向依存弧集合$A$中加入一个依存弧$(w_i,r,w_j)，其中$w_i$是堆顶的第二个单词，$w_j$堆顶部的单词。从堆栈中移除$w_j$。<br>我们不断的进行上述三类操作，直到从初始态达到最终态。    </li></ul><p>在每个状态下如何选择哪种操作呢？  </p><ul><li>当我们考虑到LEFT-ARC与RIGHT-ARC各有|R|(|R|为r的类的个数)种类别，我们可以将其看做是类别数为2|R|+1的分类问题。</li><li>每一个stack+buffer的状态相当于输入，3种操作相当于输出，把这个问题建模成分类问题。于是Nivre等人对每一个stack+buffer的状态，人工抽取出很多的特征，然后使用logistic或者svm进行分类。但是，当时的特征设计都是0/1状态的，特征向量很稀疏；特征又多，抽取特征很花时间。</li></ul><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>当我们有了Dependency Parsing的模型后，我们如何对其准确性进行评估呢？<br>我们有两个metric，一个是<strong>LAS(labeled attachment score)</strong>即只有arc的箭头方向以及语法关系均正确时才算正确，以及<strong>UAS(unlabeled attachment score)</strong>即只要arc的箭头方向正确即可。</p><p><img src="/2020/04/26/Linguistic%20Structure%20Dependency%20Parsing/image04.png" alt></p><h3 id="Neural-Dependency-Parsing"><a href="#Neural-Dependency-Parsing" class="headerlink" title="Neural Dependency Parsing"></a>Neural Dependency Parsing</h3><p>传统的Transition-based Dependency Parsing对特征工程要求较高，我们可以用神经网络来减少人力劳动</p><p>对于Neural Dependency Parser，其输入特征通常包含三种：</p><ul><li>stack和buffer中的单词及其dependent word。</li><li>单词的Part-of-Speech tag。</li><li>描述语法关系的arc label。</li></ul><p><img src="/2020/04/26/Linguistic%20Structure%20Dependency%20Parsing/image05.png" alt></p><p>当神经网络火了之后，人们自然想到了用神经网络替代logistic或svm，提出了新的句法分析器。他们对于每一个stack+buffer的状态，抽取出words、POS tags和arc labels三种不同类型的特征，都用词向量来表示。然后输入只有一个隐层的全连接网络，效果立马超过了之前所有人工设计的特征和方法。基于这个工作，后续又有很多改进版本。</p><p><img src="/2020/04/26/Linguistic%20Structure%20Dependency%20Parsing/image06.png" alt></p><p>利用这样简单的前置神经网络，我们就可以减少特征工程并提高准确度，当然，RNN模型也可以应用到Dependency Parsing任务中。</p>]]></content>
      
      
      <categories>
          
          <category> cs224n学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs224n课程 </tag>
            
            <tag> nlp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>04 Matrix Calculus and Backpropagation</title>
      <link href="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/"/>
      <url>/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/</url>
      
        <content type="html"><![CDATA[<h2 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h2><p>该部分介绍了雅可比矩阵、链式求导法则，最后推算出下图的输出得分$s$对$W$和$b$的求导结果。</p><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image01.png" alt></p><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image02.png" alt></p><p>根据链式求导法则 : </p><script type="math/tex; mode=display">\frac{\partial s}{\partial b}=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial b}=u^Tdiag(f'(z))I=u^T \cdot f'(z)</script><script type="math/tex; mode=display">\frac{\partial s}{\partial W}=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial W}</script><p>令$\delta=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}=u^T \cdot f’(z)$，$\delta$是局部误差符号，则</p><script type="math/tex; mode=display">\frac{\partial s}{\partial W}=\delta\frac{\partial z}{\partial W}=\delta^T x^T</script><script type="math/tex; mode=display">\frac{\partial s}{\partial b}=\delta\frac{\partial z}{\partial b}=\delta</script><h3 id="Derivative-with-respect-to-Matrix-Output-shape"><a href="#Derivative-with-respect-to-Matrix-Output-shape" class="headerlink" title="Derivative with respect to Matrix: Output shape"></a>Derivative with respect to Matrix: Output shape</h3><p>$W \in \mathbb{R}^{n \times m},\frac{\partial s}{\partial W}$的形状是啥?<br>我们遵循导数的形状是参数的形状的规则，可以看到它是一个$n \times m$的雅可比矩阵</p><script type="math/tex; mode=display">\begin{bmatrix}\frac{\partial s}{\partial W_{11}} & \cdots  & \frac{\partial s}{\partial W_{1m}}\\ \vdots  & \ddots  & \vdots \\ \frac{\partial s}{\partial W_{n1}} & \cdots  & \frac{\partial s}{\partial W_{nm}}\end{bmatrix}</script><h3 id="Why-the-Transposes"><a href="#Why-the-Transposes" class="headerlink" title="Why the Transposes?"></a>Why the Transposes?</h3><p>为什么$\frac{\partial s}{\partial W}=\delta^T x^T$中$\delta$是转置?</p><script type="math/tex; mode=display">\frac{\partial s}{\partial W}=\delta^T x^T=\begin{bmatrix}\delta_1 \\ \vdots \\ \delta_n\end{bmatrix}[x_1,...,x_m]=\begin{bmatrix}\delta_1x_1 & \cdots  & \delta_1x_m\\ \vdots  & \ddots  & \vdots \\ \delta_nx_1 & \cdots  & \delta_nx_m\end{bmatrix}</script><h2 id="Deriving-local-input-gradient-in-backprop"><a href="#Deriving-local-input-gradient-in-backprop" class="headerlink" title="Deriving local input gradient in backprop"></a>Deriving local input gradient in backprop</h2><p>通过神经网络展开的图计算$\frac{\partial s}  {\partial W}$</p><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image03.png" alt></p><script type="math/tex; mode=display">\frac{\partial s}{\partial W}=\delta\frac{\partial z}{\partial W}=\delta  \frac{\partial}{\partial W}Wx+b</script><p>我们只考虑$W_{ij}$的导数，$W_{ij}$只对$z_i$有贡献，例如$W_{23}$只对$z_2$有贡献，对$z_1$没有贡献，可推导出 ： </p><script type="math/tex; mode=display">\frac{\partial z_i}{\partial W_{ij}}=\delta  \frac{\partial}{\partial W_{ij}}W_ix+b_i=\frac{\partial}{\partial W_{ij}}\sum^d_{k=1}W_{ik}x_k=x_j</script><h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><p>前向传播(从左至右计算)</p><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image04.png" alt></p><script type="math/tex; mode=display">s=u^Th</script><script type="math/tex; mode=display">h=f(z)</script><script type="math/tex; mode=display">z=Wx+b</script><script type="math/tex; mode=display">x(input)</script><p>后向传播(从右至左传递导数)</p><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image05.png" alt></p><h3 id="Backpropagation-Single-Node"><a href="#Backpropagation-Single-Node" class="headerlink" title="Backpropagation: Single Node"></a>Backpropagation: Single Node</h3><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image06.png" alt></p><ul><li>节点接收“上游梯度”</li><li>目标是传递正确的“下游梯度”</li><li>每个节点都有<strong>局部梯度local gradient</strong>，它输出的梯度是与它的输入有关</li><li>[downstream gradient] = [upstream gradient] x [local gradient]</li></ul><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image07.png" alt></p><ul><li>多个输入对应多个局部梯度<h3 id="An-Example"><a href="#An-Example" class="headerlink" title="An Example"></a>An Example</h3></li></ul><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image08.png" alt></p><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image09.png" alt></p><script type="math/tex; mode=display">\frac{\partial f}{\partial x}=\frac{\partial f}{\partial a}\frac{\partial a}{\partial x}=2</script><script type="math/tex; mode=display">\frac{\partial f}{\partial y}=\frac{\partial f}{\partial a}\frac{\partial a}{\partial y}+\frac{\partial f}{\partial b}\frac{\partial b}{\partial y}=2+3=5</script><script type="math/tex; mode=display">\frac{\partial f}{\partial z}=0</script><h3 id="Efficiency-compute-all-gradients-at-once"><a href="#Efficiency-compute-all-gradients-at-once" class="headerlink" title="Efficiency: compute all gradients at once"></a>Efficiency: compute all gradients at once</h3><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image10.png" alt></p><ul><li>绿的线的部分导数可以共用以此减少计算</li></ul><h3 id="Back-Prop-in-General-Computation-Graph"><a href="#Back-Prop-in-General-Computation-Graph" class="headerlink" title="Back-Prop in General Computation Graph"></a>Back-Prop in General Computation Graph</h3><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image11.png" alt></p><ol><li>Fprop：按拓扑排序顺序访问节点<ul><li>计算给定父节点的节点的值</li></ul></li><li>Bprop：<ul><li>初始化输出梯度为 1</li><li>以相反的顺序方位节点，使用节点的后继的梯度来计算每个节点的梯度</li><li>$\{y_1,y_2,…,y_n\}$是$x$的后继</li><li>$\frac{\partial z}{\partial x}=\sum^n_1\frac{\partial z}{\partial y_i}\frac{\partial y_i}{\partial x}$</li><li>正确地说，Fprop 和 Bprop 的计算复杂度是一样的</li><li>一般来说，我们的网络有固定的层结构，所以我们可以使用矩阵和雅可比矩阵</li></ul></li></ol><h3 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a>Automatic Differentiation</h3><p><img src="/2020/04/21/Matrix%20Calculus%20and%20Backpropagation/image12.png" alt></p><ul><li>梯度计算可以从 Fprop 的符号表达式中自动推断</li><li>每个节点类型需要知道如何计算其输出，以及如何在给定其输出的梯度后计算其输入的梯度</li><li>现代DL框架(Tensorflow, Pytoch)为您做反向传播，但主要是令作者手工计算层/节点的局部导数</li></ul><h2 id="Notes-03-Neural-Networks-Backpropagation"><a href="#Notes-03-Neural-Networks-Backpropagation" class="headerlink" title="Notes 03 Neural Networks, Backpropagation"></a>Notes 03 Neural Networks, Backpropagation</h2><h3 id="Neural-Networks-Foundations"><a href="#Neural-Networks-Foundations" class="headerlink" title="Neural Networks: Foundations"></a>Neural Networks: Foundations</h3><p>该部分涉及到DL中的W和b的更新，已经掌握</p><h3 id="Neural-Networks-Tips-and-Tricks"><a href="#Neural-Networks-Tips-and-Tricks" class="headerlink" title="Neural Networks: Tips and Tricks"></a>Neural Networks: Tips and Tricks</h3><p>此部分已经在我的文章<a href="https://stanleylsx.github.io/2020/05/12/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9" target="_blank" rel="noopener">常用图像分类模型和相关的知识点</a>中学习</p>]]></content>
      
      
      <categories>
          
          <category> cs224n学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs224n课程 </tag>
            
            <tag> nlp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>03 Word Window Classification, Neural Networks, and PyTorch</title>
      <link href="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/"/>
      <url>/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/</url>
      
        <content type="html"><![CDATA[<h2 id="Classification-setup-and-notation"><a href="#Classification-setup-and-notation" class="headerlink" title="Classification setup and notation"></a>Classification setup and notation</h2><p>通常我们有由样本组成的训练数据集</p><script type="math/tex; mode=display">\{x_i,y_i\}_{i=1}^N</script><p>$x_i$是输入，例如单词(索引或是向量)，句子，文档等，维度为$d$<br>$y_i$是我们尝试预测的标签($C$各类别中的一个)，例如 :   </p><ul><li>类别：感情，命名实体，购买/售出的决定</li><li>其他单词</li><li>之后：多词序列的<h3 id="Classification-intuition"><a href="#Classification-intuition" class="headerlink" title="Classification intuition"></a>Classification intuition</h3></li></ul><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image01.png" alt></p><p>训练数据 : $\{x_i,y_i\}_{i=1}^N$<br>简单的说明情况   </p><ul><li>固定的二维单词向量分类</li><li>使用softmax/logistic回归</li><li>线性决策边界</li></ul><p><strong>传统的机器学习/统计学方法：</strong> 假设$x_i$是固定的，训练softmax/logistic回归的权重$W\in \mathbb{R}^{C\times d}$来决定决策边界(超平面)<br>方法 : 对每个$x$，预测</p><script type="math/tex; mode=display">p(y|x)=\frac{exp(W_y\cdot x)}{\sum_{c=1}^Cexp(W_c\cdot x)}</script><p>我们可以将预测函数分为两个步骤：  </p><ol><li>将$W$的第$y$行和$x$中的对应行相乘得到分数，计算所有的$fc,for  c=1,…,C$<script type="math/tex; mode=display">W_y\cdot x=\sum^d_{i=1}W_{yi}x_i=f_y</script></li><li>使用softmax函数获得归一化的概率<script type="math/tex; mode=display">p(y|x)=\frac{exp(f_y)}{\sum^C_{c=1}exp(f_c)}</script><h3 id="Training-with-softmax-and-cross-entropy-loss"><a href="#Training-with-softmax-and-cross-entropy-loss" class="headerlink" title="Training with softmax and cross-entropy loss"></a>Training with softmax and cross-entropy loss</h3>对于每个训练样本$(x,y)$，我们的目标是最大化正确类$y$的概率，或者我们可以最小化该类的负对数概率  <script type="math/tex; mode=display">-logp(y|x)=-log(\frac{exp(f_y)}{})</script><h3 id="Background-What-is-“cross-entropy”-loss-error"><a href="#Background-What-is-“cross-entropy”-loss-error" class="headerlink" title="Background: What is “cross entropy” loss/error?"></a>Background: What is “cross entropy” loss/error?</h3></li></ol><ul><li>交叉熵的概念来源于信息论，衡量两个分布之间的差异</li><li>令真实概率分布为$p$</li><li>令我们计算的模型概率为$q$</li><li>交叉熵为  <script type="math/tex; mode=display">H(p,q)=-\sum^C_{c=1}p(c)logq(c)</script></li><li>假设groud truth(or true or gold or target)的概率分布在正确的类上为1，在其他任何地方为0 ：$p=[0,…,0,1,0,…0]$ </li><li>因为p是one-hot向量，所以唯一剩下的项是真实类的负对数概率</li></ul><h3 id="Classification-over-a-full-dataset"><a href="#Classification-over-a-full-dataset" class="headerlink" title="Classification over a full dataset"></a>Classification over a full dataset</h3><p>在整个数据集$\{x_i,y_i\}_{i=1}^N$上的交叉熵损失函数，是所有样本的交叉熵的均值  </p><script type="math/tex; mode=display">J(\theta)=\frac{1}{N}\sum^N_{i=1}-log(\frac{e^{f_{yi}}}{\sum^C_{c=1}e^{f_c}})</script><p>我们不使用</p><script type="math/tex; mode=display">f_y=f_y(x)=W_y\cdot x=\sum^d_{j=1}W_{yj}x_j</script><p>我们使用矩阵来表示$f$</p><script type="math/tex; mode=display">f=Wx</script><h3 id="Traditional-ML-optimization"><a href="#Traditional-ML-optimization" class="headerlink" title="Traditional ML optimization"></a>Traditional ML optimization</h3><ul><li>一般机器学习的参数$\theta$通常只由W的列组成<script type="math/tex; mode=display">\theta=[W_1,...,W_d]=W(:)^T\in \mathbb{R}^{Cd}</script></li><li>因此，我们只通过以下方式更新决策边界<script type="math/tex; mode=display">\bigtriangledown_{\theta}J(\theta)=[\bigtriangledown_{W_1},...,\bigtriangledown_{W_d}]^T\in \mathbb{R}^{Cd}</script><h2 id="Neural-Network-Classifiers"><a href="#Neural-Network-Classifiers" class="headerlink" title="Neural Network Classifiers"></a>Neural Network Classifiers</h2></li></ul><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image02.png" alt></p><ul><li>单独使用Softmax(≈logistic回归)并不十分强大</li><li>Softmax只给出线性决策边界  <ul><li>这可能是相当有限的，当问题很复杂时是无用的</li><li>纠正这些错误不是很酷吗?</li></ul></li></ul><h3 id="Neural-Nets-for-the-Win"><a href="#Neural-Nets-for-the-Win" class="headerlink" title="Neural Nets for the Win!"></a>Neural Nets for the Win!</h3><p>神经网络可以学习更复杂的函数和非线性决策边界</p><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image03.png" alt></p><p>更高级的分类需要</p><ul><li>词向量</li><li>更深层次的深层神经网络<h3 id="Classification-difference-with-word-vectors"><a href="#Classification-difference-with-word-vectors" class="headerlink" title="Classification difference with word vectors"></a>Classification difference with word vectors</h3>一般在NLP深度学习中</li><li>我们学习了矩阵$W$和词向量$x$</li><li>我们学习传统参数和表示</li><li>词向量是对one-hot向量的重新表示，在中间层向量空间中移动它们，以便使用(线性)softmax分类器通过x = Le层进行分类  <ul><li>即将词向量理解为一层神经网络，输入单词的one-hot向量并获得单词的词向量表示，并且我们需要对其进行更新。其中，$Vd$是数量很大的参数<script type="math/tex; mode=display">\bigtriangledown_{\theta}J(\theta)=[\bigtriangledown_{W_1},...,\bigtriangledown_{dardvark},...\bigtriangledown_{W_d}]^T\in \mathbb{R}^{Cd+Vd}</script><h3 id="Neural-computation"><a href="#Neural-computation" class="headerlink" title="Neural computation"></a>Neural computation</h3></li></ul></li></ul><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image04.png" alt></p><h3 id="A-neuron-can-be-a-binary-logistic-regression-unit"><a href="#A-neuron-can-be-a-binary-logistic-regression-unit" class="headerlink" title="A neuron can be a binary logistic regression unit"></a>A neuron can be a binary logistic regression unit</h3><p>$f$为非线性激活函数(例如sigmoid函数)，$w$为权重向量，$b$为偏置向量，$h$为隐藏层变量对应的向量，$x$为输入向量</p><script type="math/tex; mode=display">h_{w,b}=f(w^Tx+b)</script><script type="math/tex; mode=display">f(z)=\frac{1}{1+e^{-z}}</script><p>$b$ : 我们可以有一个“总是打开”的特性，它给出一个先验类，或者将它作为一个偏向项分离出来<br>$w,b$是神经元的参数</p><h3 id="A-neural-network-running-several-logistic-regressions-at-the-same-time"><a href="#A-neural-network-running-several-logistic-regressions-at-the-same-time" class="headerlink" title="A neural network = running several logistic regressions at the same time"></a>A neural network = running several logistic regressions at the same time</h3><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image05.png" alt></p><p>如果我们输入一个向量通过一系列逻辑回归函数，那么我们得到一个输出向量，但是我们不需要提前决定这些逻辑回归试图预测的变量是什么。</p><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image06.png" alt></p><p>我们可以输入另一个logistic回归函数。损失函数将指导中间隐藏变量应该是什么，以便更好地预测下一层的目标。我们当然可以使用更多层的神经网络。</p><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image07.png" alt></p><h3 id="Matrix-notation-for-a-layer"><a href="#Matrix-notation-for-a-layer" class="headerlink" title="Matrix notation for a layer"></a>Matrix notation for a layer</h3><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image08.png" alt></p><p>我们有</p><script type="math/tex; mode=display">a_1=f(W_11x_1+W_12x_2+W_13x_3+b_1)</script><script type="math/tex; mode=display">a_2=f(W_21x_1+W_22x_2+W_23x_3+b_2)</script><p>通过矩阵表示的运算有</p><script type="math/tex; mode=display">z=Wx+b</script><script type="math/tex; mode=display">a=f(z)</script><p>激活函数$f$在运算时是element-wise逐元素的</p><script type="math/tex; mode=display">f([z_1,z_2,z_3])=[f(z_1),f(z_2),f(z_3)]</script><h3 id="Non-linearities-aka-“f-”-Why-they’re-needed"><a href="#Non-linearities-aka-“f-”-Why-they’re-needed" class="headerlink" title="Non-linearities (aka “f ”): Why they’re needed"></a>Non-linearities (aka “f ”): Why they’re needed</h3><p>例如：函数近似，如回归或分类</p><ul><li>没有非线性，深度神经网络只能做线性变换</li><li>多个线性变换可以组成一个的线性变换$W_1W_2x=Wx$，因为线性变换是以某种方式旋转和拉伸空间，多次的旋转和拉伸可以融合为一次线性变换</li><li>对于非线性函数而言，使用更多的层，他们可以近似更复杂的函数</li></ul><h2 id="Named-Entity-Recognition-NER"><a href="#Named-Entity-Recognition-NER" class="headerlink" title="Named Entity Recognition (NER)"></a>Named Entity Recognition (NER)</h2><ul><li>任务：例如，查找和分类文本中的名称</li></ul><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image09.png" alt></p><ul><li>可能的用途 : <ul><li>跟踪文档中提到的特定实体（组织、个人、地点、歌曲名、电影名等）</li><li>对于问题回答，答案通常是命名实体</li><li>许多需要的信息实际上是命名实体之间的关联</li><li>同样的技术可以扩展到其他 slot-filling 槽填充分类</li></ul></li><li>通常后面是命名实体链接/规范化到知识库</li></ul><h3 id="Named-Entity-Recognition-on-word-sequences"><a href="#Named-Entity-Recognition-on-word-sequences" class="headerlink" title="Named Entity Recognition on word sequences"></a>Named Entity Recognition on word sequences</h3><p>我们通过在上下文中对单词进行分类，然后将实体提取为单词子序列来预测实体</p><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image10.png" alt></p><h3 id="Why-might-NER-be-hard"><a href="#Why-might-NER-be-hard" class="headerlink" title="Why might NER be hard?"></a>Why might NER be hard?</h3><ul><li><p>很难计算出实体的边界</p><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image11.png" alt><br>第一个实体是 “First National Bank” 还是 “National Bank”</p></li><li>很难知道某物是否是一个实体，是一所名为“Future School” 的学校，还是这是一所未来的学校？</li><li><p>很难知道未知/新奇实体的类别</p><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image12.png" alt><br>“Zig Ziglar” ? 一个人</p></li><li><p>实体类是模糊的，依赖于上下文</p><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image13.png" alt><br>这里的“Charles Schwab” 是 PER 不是 ORG</p></li></ul><h2 id="Word-window-classification"><a href="#Word-window-classification" class="headerlink" title="Word window classification"></a>Word window classification</h2><ul><li>思想：在<strong>相邻词的上下文窗口</strong>中对一个词进行分类</li><li>例如，上下文中一个单词的命名实体分类(人、地点、组织、NONE)</li><li>在上下文中对单词进行分类的一个简单方法可能是对窗口中的单词向量进行<strong>平均</strong>，并对平均向量进行分类(问题是这会丢失单词位置信息)</li></ul><h3 id="Window-classification-Softmax"><a href="#Window-classification-Softmax" class="headerlink" title="Window classification: Softmax"></a>Window classification: Softmax</h3><ul><li>训练softmax分类器对中心词进行分类，方法是在一个窗口内<strong>将中心词周围的词向量串联起来</strong></li><li>例子：在这句话的上下文中对“Paris”进行分类，窗口长度为2</li></ul><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image14.png" alt></p><ul><li>结果$x_{window}=x\in \mathbb{R}^{5d}$是一个列向量</li></ul><h3 id="Simplest-window-classifier-Softmax"><a href="#Simplest-window-classifier-Softmax" class="headerlink" title="Simplest window classifier: Softmax"></a>Simplest window classifier: Softmax</h3><p>对于$x=x_{window}$，我们可以使用与之前相同的softmax分类器</p><p><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image15.png" alt></p><ul><li>如何更新向量？</li><li>简而言之：求导和优化</li></ul><h3 id="Slightly-more-complex-Multilayer-Perceptron"><a href="#Slightly-more-complex-Multilayer-Perceptron" class="headerlink" title="Slightly more complex: Multilayer Perceptron"></a>Slightly more complex: Multilayer Perceptron</h3><ul><li>在我们的softmax分类器中引入一个附加的非线性层。</li><li>Multilayer Perceptron(mlp)是更复杂的神经系统的基本构件!</li><li>假设我们要对中心词是否为一个地点，进行分类</li><li>与word2vec类似，我们将遍历语料库中的所有位置。但这一次，它将受到监督，只有一些位置能够得到高分，在他们的中心有一个实际的NER Location的位置是“真实的”位置会获得高分<h3 id="Neural-Network-Feed-forward-Computation"><a href="#Neural-Network-Feed-forward-Computation" class="headerlink" title="Neural Network Feed-forward Computation"></a>Neural Network Feed-forward Computation</h3>使用神经激活$a$简单地给出一个非标准化的分数<script type="math/tex; mode=display">score(x)=U^Ta \in R</script>我们用一个三层神经网络计算一个窗口的得分<script type="math/tex; mode=display">s=score("museums in Paris are amazing”)</script><script type="math/tex; mode=display">s=U^Tf(Wx+b)</script><script type="math/tex; mode=display">x\in \mathbb{R}^{20 \times 1},W\in \mathbb{R}^{8 \times 20},U\in \mathbb{R}^{8 \times 1}</script><img src="/2020/04/20/Word%20Window%20Classification,%20Neural%20Networks,%20and%20PyTorch/image16.png" alt></li></ul><h3 id="Main-intuition-for-extra-layer"><a href="#Main-intuition-for-extra-layer" class="headerlink" title="Main intuition for extra layer"></a>Main intuition for extra layer</h3><p>中间层学习输入词向量之间的<strong>非线性交互</strong><br>例如：只有当“museum”是第一个向量时，“in”放在第二个位置才重要</p>]]></content>
      
      
      <categories>
          
          <category> cs224n学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs224n课程 </tag>
            
            <tag> nlp </tag>
            
            <tag> ner </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用图像分类模型和相关的知识点</title>
      <link href="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<h2 id="SourceCode"><a href="#SourceCode" class="headerlink" title="SourceCode"></a>SourceCode</h2><p>和这篇文章对应的源码存放在我的github下的<a href="https://github.com/StanleyLsx/image_classification.git" target="_blank" rel="noopener">Image classification</a>仓库。</p><p>可以选择直接clone到本地： </p><ul><li>git clone <a href="https://github.com/StanleyLsx/image_classification.git" target="_blank" rel="noopener">https://github.com/StanleyLsx/image_classification.git</a></li></ul><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>cifar-10数据集</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/alexnet_structure.jpg" alt></p><p>论文：《ImageNet Classification with Deep Convolutional Neural Networks》<br>意义：相比传统结构有了更高的分类准确度，引爆深度学习，首次使用Relu。<br>结构：2-GPU并行结构；1、2、5卷积层后跟随max-pooling层；两个全连接使用dropout；总共8层神经网络。</p><h2 id="VggNet"><a href="#VggNet" class="headerlink" title="VggNet"></a>VggNet</h2><p>论文：《Very Deep Convolutional Networks for Large-Scale Image Recognition》<br>意义：分类问题第二，物体检测第一(ImageNet2014)。<br>结构：更深的网络结构；使用3x3的卷积核和1x1的卷积核；每经过一个pooling层，通道数目翻倍。<br>两个3x3卷积层视野率等于一个5x5卷积核，多一次线性变换且参数数量降低28%，1x1的卷积核可以看做在对应通道上的非线性变换，有通道降维的作用。  </p><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/vggnet_structure.jpg" alt>  </p><p>实现的代码基于Cifar数据集，实现了简单层次的vggnet，两次卷积加一个池化层并重复三次并且最后全连接层只实现了一层。</p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/resnet_structure_3.png" alt>   </p><p>论文：《Deep Residual Learning for Image Recognition》<br>意义：ILSVRC2015分类比赛冠军，解决深层次网络训练问题。<br>结构：加入恒等变换子结构，identity部分是恒等变换，F(x)为残差学习，学习使得F(x)趋向0，从而忽略深度。<br>不同的ResNet有不同结构，如图ResNet-34和ResNet-101是两种常用结构。</p><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/resnet_structure_1.png" alt>     </p><p>所有的网络结构可以通用描述为：</p><ul><li>先用一个步长为2的卷积层。</li><li>经过一个3x3的max_pooling层</li><li>经过残差结构</li><li>没有中间的全连接层，直接到输出。</li></ul><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/resnet_structure_2.png" alt>  </p><p>上图表格中有更多的结构，从各个结构可以看出ResNet强化了卷积层，弱化了全连接层，维持了参数平衡。<br>特点：残差结构使得网络需要的学习的知识变少，容易学习；残差结构使得每一层的数据分布接近，容易学习。</p><p>代码中同样使用Cifer数据集，由于数据集图片较小，所以在输入到残差结构前的卷积层步长设定为1，且没有经过3x3的max_pooling层。<br>每个残差块都由两个3x3卷积核组成，总共有三个残差层，残差块的个数分别为2、3、2。  </p><h2 id="InceptionNet"><a href="#InceptionNet" class="headerlink" title="InceptionNet"></a>InceptionNet</h2><p>论文：《Rethinking the Inception Architecture for Computer Vision》<br>意义：主要是工程的优化，使得同样的参数数量训练更加的效率。一方面解决更深的网络过拟合，另外一方面解决更深的网络有更大计算量的问题。<br>结构：主要是v1~v4四个结构。  </p><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/inceptionnet.png" alt></p><h3 id="V1结构"><a href="#V1结构" class="headerlink" title="V1结构"></a>V1结构</h3><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/inceptionnet_v1_structure.png" alt>  </p><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/inceptionnet_v1_structure_1.png" alt>  </p><p>采用分组卷积，组与组之间的数据在分组计算时候不会交叉。一层上同时使用多种卷积核，看到各层的feature；不同组之间的feature不交叉计算，减少了计算量。</p><h3 id="V2结构"><a href="#V2结构" class="headerlink" title="V2结构"></a>V2结构</h3><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/inceptionnet_v2_structure.png" alt>  </p><p>引入3x3的卷积核做同等卷积替换，两个3x3卷积核的视野域和一个5x5的相同。  </p><h3 id="V3结构"><a href="#V3结构" class="headerlink" title="V3结构"></a>V3结构</h3><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/inceptionnet_v3_structure.png" alt>  </p><p>进一步的做同等卷积替换，一个3x3的卷积核的视野域等同于一个1x3的卷积核加上一个3x1卷积核。  </p><h3 id="V4结构"><a href="#V4结构" class="headerlink" title="V4结构"></a>V4结构</h3><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/inceptionnet_v4_structure.png" alt>  </p><p>使用和ResNet同样的思想，引入skip connection，可解决深层次网络训练问题。  </p><p>项目代码中基于v1结构实现了简单的InceptionNet，受限于数据集图片的大小，层次不深，各层的步长和核大小被调整。<br>结构为conv1(3x3/1)-&gt;max_pooling1(2x2/2)-&gt;inception_2a-&gt;inception_2b-&gt;max_pooling2(2x2/2)-&gt;inception_3a-&gt;inception_3a-&gt;max_pooling2(2x2/2)-&gt;dense，每个inception都会增加通道的数目，但是图片的大小维持不变。</p><h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><p>论文：《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications》<br>意义：引入深度可分离的卷积，进一步降低参数。 </p><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/mobilenet_structure_1.png" alt>  </p><p>下图设定输入输出通道数为300，对于一个普通的3x3卷积，它需要3*3*300*300个参数。<br>若分组卷积，对分组卷积来看它的参数为3*3*100*100*3。参数降低1/3。  </p><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/mobilenet_structure_2.png" alt>    </p><p><img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/mobilenet_structure_3.png" alt>     </p><p>MobileNet将分组卷积做到极致，如上图所示，每一个3x3卷积核只管一个通道。<br>代码实现和InceptionNet结构差不多，要注意的是在通过深度可分离卷积块的时候(separable_X)，将通道分割开分别送入一个3x3的卷积核，再把它们的输出拼接起来。  </p><h2 id="Cnn-Tricks"><a href="#Cnn-Tricks" class="headerlink" title="Cnn Tricks"></a>Cnn Tricks</h2><h3 id="Activation"><a href="#Activation" class="headerlink" title="Activation"></a>Activation</h3><ul><li><p><strong>Sigmoid</strong><br>  <img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/sigmoid.png" alt></p><script type="math/tex; mode=display">f(x)=\frac{1}{1+e^x}</script><p>  特点：输入非常大或非常小时没有梯度；输出均值非0；exp计算比较复杂。</p></li><li><p><strong>Tanh</strong><br>  <img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/tanh.png" alt></p><script type="math/tex; mode=display">f(x)=tanh(x)</script><p>  特点：输入非常大或非常小时没有梯度；输出均值为0；计算复杂。</p></li><li><p><strong>ReLU</strong><br>  <img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/ReLU.png" alt></p><script type="math/tex; mode=display">f(x)=max(0,x)</script><p>  特点：梯度不会过小；计算量小；收敛速度快；输出均值非0；Dead ReLU:非常大的梯度流过神经元时不会再有激活现象。</p></li><li><p><strong>Leaky ReLU</strong><br>  <img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/Leaky_ReLU.png" alt></p><script type="math/tex; mode=display">f(x)=max(0.1x,x)</script><p>  特点：解决Dead ReLU问题。</p></li><li><p><strong>ELU</strong><br>  <img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/ELU.png" alt></p><script type="math/tex; mode=display">f(x)=\left\{\begin{matrix}x,\ x\geq 0\\ \alpha(e^x-1),\ x<0\end{matrix}\right.</script><p>  特点：均值更接近于0；小于0时计算量大。</p></li><li><p><strong>Maxout</strong><br>  <img src="/2020/04/16/%E5%B8%B8%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/maxout.png" alt></p><script type="math/tex; mode=display">f(x)=max(w_1^Tx+b_1,w_2^Tx+b_2,...,w_n^T+b_n)</script><p>  特点：ReLU泛化版本；无Dead ReLU；两倍的参数数量。</p></li></ul><h3 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h3><ul><li><p><strong>SGD随机梯度下降</strong></p><script type="math/tex; mode=display">g_{t}=\bigtriangledown f(\theta_{t-1})</script><script type="math/tex; mode=display">\bigtriangleup\theta_{t}=-\eta \ast g_{t}</script><p>  容易陷入局部极值；容易陷入saddle point；选择合适的learning rate比较困难；每个分量学习率相同。</p></li><li><p><strong>Momentum动量梯度下降</strong></p><script type="math/tex; mode=display">v_{t}=\eta*v_{t-1}+g_{t}</script><script type="math/tex; mode=display">\bigtriangleup\theta_{t}=-\eta*m_{t}</script><p>  开始训练时，积累动量，加速训练；局部极值附近震荡时，梯度为0，由于动量存在，跳出陷阱；梯度改变方向时，能够缓解震荡。</p></li><li><p><strong>Adagrad</strong></p><script type="math/tex; mode=display">n_{t}=n_{t-1}+g_{t}^{2}</script><script type="math/tex; mode=display">\bigtriangleup \theta_{t}=-\frac{\eta}{\sqrt{n_{t}+\varepsilon}}*g_{t}</script><p>  $-\frac{\eta}{\sqrt{n_{t}+\varepsilon}}$为约束项，约束项前期较小，放大梯度，后期较大，缩小梯度；梯度随着训练次数降低；每个分量有不同的学习率。学习率设置过大时，约束项过于敏感；后期，约束项累计太大，会提前结束训练。</p></li><li><p><strong>RMSProp</strong></p><script type="math/tex; mode=display">E \left\|g^{2}\right\|_{t}=\rho*E \left \|g^{2}\right\|_{t-1}+(1-\rho)*g_{t}^{2}</script><script type="math/tex; mode=display">\bigtriangleup \theta_{t}=-\frac{\eta}{\sqrt{E\left\|g^{2}\right\|_{t}+\varepsilon }}*g_{t}</script><p>  适合处理非平稳目标，对于RNN效果很好；由累积平方梯度变为平均平方梯度；解决了Adagrad训练后期提前结束的问题。</p></li><li><p><strong>Adam</strong></p><script type="math/tex; mode=display">m_{t}=\beta_{1}*m_{t-1}+(1-\beta_{1})*g_{t},v_{t}=\beta_{2}*v_{t-1}+(1-\beta_{2})*g_{t}^{2}</script><script type="math/tex; mode=display">\hat{m}_t=\frac{m_t}{1-\beta_1^t},\hat{v}_t=\frac{v_t}{1-\beta_2^t}</script><script type="math/tex; mode=display">\bigtriangleup\theta_t=-\frac{\hat{m}_t}{\sqrt{\hat{n}_t}+\varepsilon}*\eta</script><p>  默认参数设置为$\beta_1=0.9,\beta_2=0.999,learning_rate=e^{-3}$；不同分量有不同学习率；冲量优势和学习率自适应优势的组合；善于处理非平稳目标。</p></li></ul><h3 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h3><p>包含图像的归一化，图像的变形(反转、拉伸、裁剪)，色彩调节(对比度，亮度)，多尺度的处理图像使得数据更丰富以期更好的效果。</p><h3 id="Fine-Tune"><a href="#Fine-Tune" class="headerlink" title="Fine Tune"></a>Fine Tune</h3><p>预训练好的模型上进行微调。</p>]]></content>
      
      
      <categories>
          
          <category> 图像分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像分类 </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02 Word Vectors 2 and Word Senses</title>
      <link href="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/"/>
      <url>/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/</url>
      
        <content type="html"><![CDATA[<h2 id="Review-Main-idea-of-word2vec"><a href="#Review-Main-idea-of-word2vec" class="headerlink" title="Review: Main idea of word2vec"></a>Review: Main idea of word2vec</h2><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image01.png" alt></p><script type="math/tex; mode=display">P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}</script><ul><li>随机的单词开始</li><li>历整个语料库中的每个单词</li><li>使用单词向量预测周围的单词</li><li>更新向量以便更好地预测</li><li>学习到词之间的相似关系和在整个词向量空间中的语义倾向<h3 id="Word2vec-parameters-and-computations"><a href="#Word2vec-parameters-and-computations" class="headerlink" title="Word2vec parameters and computations"></a>Word2vec parameters and computations</h3></li></ul><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image02.png" alt></p><ul><li>每行代表一个单词的词向量，点乘后得到的分数通过softmax映射为概率分布，并且我们得到的概率分布是对于该中心词而言的上下文中单词的概率分布，该分布于上下文所在的具体位置无关，所以在每个位置的预测都是一样的</li><li>我们希望模型对上下文中(相当频繁)出现的所有单词给出一个合理的高概率估计</li><li>the, and, that, of 这样的停用词，是每个单词点乘后得到的较大概率的单词(去掉这一部分可以使词向量效果更好)<h2 id="Optimization-Gradient-Descent"><a href="#Optimization-Gradient-Descent" class="headerlink" title="Optimization: Gradient Descent"></a>Optimization: Gradient Descent</h2></li><li>Gradient Descent每次使用全部样本进行更新，计算成本巨大</li><li>Stochastic Gradient Descent每次只是用单个样本进行更新<h3 id="Stochastic-gradients-with-word-vectors"><a href="#Stochastic-gradients-with-word-vectors" class="headerlink" title="Stochastic gradients with word vectors"></a>Stochastic gradients with word vectors</h3><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image03.png" alt></li></ul><p>我们正在一个window里面计算SGD，但是在一个window里面最多只出现$2m+1$个次，所以$\triangledown_\theta J_t(\theta)$非常稀疏<br>解决办法 : </p><ul><li>需要稀疏矩阵更新操作来只更新矩阵U和V中的特定行</li><li>需要保留单词向量的散列  </li></ul><p>如果有数百万个单词向量，并且进行分布式计算，那么重要的是不必到处发送巨大的更新  </p><h3 id="Word2vec-More-details"><a href="#Word2vec-More-details" class="headerlink" title="Word2vec: More details"></a>Word2vec: More details</h3><p>为什么两个向量？</p><ul><li>更容易优化，最后都取平均值</li><li>可以每个单词只用一个向量</li></ul><p>两个模型变体</p><ul><li>Skip-grams(SG)，输入中心词并预测上下文中的单词</li><li>Continuous Bag of Words(CBOW)，输入上下文中的单词并预测中心词  </li></ul><p>之前一直使用naive的softmax(简单但代价很高的训练方法)，接下来使用负采样方法加快训练速率</p><h2 id="The-skip-gram-model-with-negative-sampling-HW2"><a href="#The-skip-gram-model-with-negative-sampling-HW2" class="headerlink" title="The skip-gram model with negative sampling(HW2)"></a>The skip-gram model with negative sampling(HW2)</h2><p>softmax中用于归一化的分母的计算代价太高</p><script type="math/tex; mode=display">P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}</script><p>负采样的基本思路是使用一个true pair(中心词及其上下文窗口中的词)与几个noise pair(中心词与随机词搭配) 形成的样本，训练二元逻辑回归<br>原文中的(最大化)目标函数是 : </p><script type="math/tex; mode=display">J(\theta)=\frac{1}{T}\sum^T_{t=1}J_t(\theta)</script><script type="math/tex; mode=display">J_t(\theta)=log\sigma(u^T_ov_c)+\sum^k_{i=1}E_{j\sim P(w)}[log\Sigma(-u^T_jv_c)]</script><p>课程中的目标函数是 : </p><script type="math/tex; mode=display">J_{neg-sample}(O,v_c,U)=-log(\sigma(u^T_jv_c))-\sum^K_{k=1}log(\sigma(u^T_kv_c))</script><ul><li>我们希望中心词与真实上下文单词的向量点积更大，中心词与随机单词的点积更小</li><li>k是我们负采样的样本数目<script type="math/tex; mode=display">P(w)=U(w)^{3/4}/Z</script>使用上式作为抽样的分布，$U(w)$是unigram分布，通过$\frac{3}{4}$次方，相对减少常见单词的频率，增大稀有词的概率。$Z$用于生成概率分布。<h2 id="Why-not-capture-co-occurrence-counts-directly"><a href="#Why-not-capture-co-occurrence-counts-directly" class="headerlink" title="Why not capture co-occurrence counts directly?"></a>Why not capture co-occurrence counts directly?</h2>共现矩阵$X$</li><li>两个选项 ：windows和full document</li><li>Window ：与word2vec类似，在每个单词周围都使用Window，包括语法(POS)和语义信息</li></ul><p>Word-document共现矩阵的基本假设是在同一篇文章中出现的单词更有可能相互关联。假设单词$i$出现在文章$j$中，则矩阵元素$X_{ij}$加1，当我们处理完数据库中的所有文章后，就得到了矩阵$X$，其大小为$|V|×M$，其中$|V|$为词汇量，而M为文章数。这一构建单词文章co-occurrence matrix的方法也是经典的Latent Semantic Analysis所采用的。</p><p>利用某个定长窗口中单词与单词同时出现的次数来产生window-based (word-word) co-occurrence matrix。下面以窗口长度为1来举例，假设我们的数据包含以下几个句子 ：</p><ul><li>I like deep learning.</li><li>I like NLP.</li><li>I enjoy flying.</li></ul><p>则我们可以得到如下的word-word co-occurrence matrix:</p><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image04.png" alt></p><p>使用共现次数衡量单词的相似性，但是会随着词汇量的增加而增大矩阵的大小，并且需要很多空间来存储这一高维矩阵，后续的分类模型也会由于矩阵的稀疏性而存在稀疏性问题，使得效果不佳。我们需要对这一矩阵进行降维，获得低维(25-1000)的稠密向量。</p><h3 id="Method-1-Dimensionality-Reduction-on-X-HW1"><a href="#Method-1-Dimensionality-Reduction-on-X-HW1" class="headerlink" title="Method 1: Dimensionality Reduction on X(HW1)"></a>Method 1: Dimensionality Reduction on X(HW1)</h3><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image05.png" alt><br>使用SVD方法将共现矩阵$X$分解为$UΣV⊤$，$∑$是对角线矩阵，对角线上的值是矩阵的奇异值。$U$,$V$是对应于行和列的正交基。<br>为了减少尺度同时尽量保存有效信息，可保留对角矩阵的最大的k个值，并将矩阵$U$,$V$的相应的行列保留。这是经典的线性代数算法，对于大型矩阵而言，计算代价昂贵。</p><h3 id="Hacks-to-X-several-used-in-Rohde-et-al-2005"><a href="#Hacks-to-X-several-used-in-Rohde-et-al-2005" class="headerlink" title="Hacks to X (several used in Rohde et al. 2005)"></a>Hacks to X (several used in Rohde et al. 2005)</h3><p>按比例调整 counts 会很有效  </p><ul><li>对诸如the、he、has等高频词进行缩放(语法有太多的影响)，可以全部忽略掉，也可以按照$min(X,t),t\approx100$规则忽略</li><li>在基于window的计数中，提高更加接近的单词的计数</li><li>使用Person相关系数代替计数</li></ul><blockquote><p>Conclusion：对计数进行处理是可以得到有效的词向量的</p></blockquote><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image06.png" alt></p><h3 id="Towards-GloVe-Count-based-vs-direct-prediction"><a href="#Towards-GloVe-Count-based-vs-direct-prediction" class="headerlink" title="Towards GloVe: Count based vs. direct prediction"></a>Towards GloVe: Count based vs. direct prediction</h3><p>基于计数(使用整个矩阵的全局统计数据来直接估计)</p><ul><li>优点 ：训练快速、统计数据高效利用</li><li>缺点 ：主要用于捕捉单词相似性、对大量数据给予比例失调的重视  </li></ul><p>转换计数(定义概率分布并试图预测单词)</p><ul><li>优点 ：提高其他任务的性能、能捕获除了单词相似性以外的复杂的模式</li><li>缺点 ：与语料库大小有关的量表、统计数据的低效使用<h2 id="Encoding-meaning-in-vector-differences"><a href="#Encoding-meaning-in-vector-differences" class="headerlink" title="Encoding meaning in vector differences"></a>Encoding meaning in vector differences</h2>将两个流派的想法结合起来，在神经网络中使用计数矩阵<blockquote><p>关于Glove的理论分析需要阅读原文，也可以阅读<a href="https://zhuanlan.zhihu.com/p/60208480" target="_blank" rel="noopener">CS224N笔记(二)：GloVe</a></p></blockquote></li></ul><p><strong>关键思想</strong> ：共现概率的比值可以对meaning component进行编码</p><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image07.png" alt></p><p>重点不是单一的概率大小，重点是他们之间的比值，其中蕴含着meaning component</p><p>例如我们想区分热力学上两种不同状态ice冰与蒸汽steam，它们之间的关系可通过与不同的单词 x 的co-occurrence probability 的比值来描述。<br>例如对于solid固态，虽然$P(solid|ice)$与$P(solid|steam)$本身很小，不能透露有效的信息，但是它们的比值$\frac{P(solid|ice)}{P(solid|steam)}$却较大，因为solid更常用来描述ice的状态而不是steam的状态，所以在ice的上下文中出现几率较大<br>对于gas则恰恰相反，而对于water这种描述ice与steam均可或者fashion这种与两者都没什么联系的单词，则比值接近于1。所以相较于单纯的co-occurrence probability，实际上co-occurrence probability的相对比值更有意义</p><p>我们如何在词向量空间中以线性meaning component的形式捕获共现概率的比值？<br>log-bilinear 模型 : </p><script type="math/tex; mode=display">w_i\cdot w_j=logP(i|j)</script><p>向量差异 : </p><script type="math/tex; mode=display">w_x\cdot (w_a-w_b)=log\frac{P(x|a)}{P(x|b)}</script><ul><li>如果使向量点积等于共现概率的对数，那么向量差异变成了共现概率的比率<script type="math/tex; mode=display">J=\sum^T_{i,j=1}f(X_{ij})(w^T_i\tilde{w}_j+b_i+\tilde{b}_j-logX_{ij})^2</script></li><li>使用平方误差促使点积尽可能得接近共现概率的对数</li><li><p>使用$f(x)$对常见单词进行限制</p><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image08.png" alt></p></li><li><p>优点 : 训练快速、可以扩展到大型语料库、即使是小语料库和小向量，性能也很好</p></li></ul><h2 id="How-to-evaluate-word-vectors"><a href="#How-to-evaluate-word-vectors" class="headerlink" title="How to evaluate word vectors?"></a>How to evaluate word vectors?</h2><p>与NLP的一般评估相关分为内在与外在(intrinsic vs. extrinsic)<br>内在 : </p><ul><li>对特定/中间子任务进行评估</li><li>计算速度快</li><li>有助于理解这个系统  </li><li>不清楚是否真的有用，除非与实际任务建立了相关性</li></ul><p>外在 :   </p><ul><li>对真实任务的评估</li><li>计算精确度可能需要很长时间</li><li>不清楚子系统是问题所在，是交互问题，还是其他子系统</li><li>如果用另一个子系统替换一个子系统可以提高精确度<h3 id="Intrinsic-word-vector-evaluation"><a href="#Intrinsic-word-vector-evaluation" class="headerlink" title="Intrinsic word vector evaluation"></a>Intrinsic word vector evaluation</h3>一个比较常用的内部评估的方法是词向量的类比。在词向量类比中，给定以下形式的不完整类比：  <script type="math/tex; mode=display">a:b :: c:?</script>形如 : <script type="math/tex; mode=display">man:woman :: king:?</script><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image09.png" alt></li></ul><p>然后内部评估系统计算词向量的最大余弦相似度：</p><script type="math/tex; mode=display">d=arg\underset{i}{max}\frac{(x_b-x_a+x_c)^Tx_i}{||x_b-x_a+x_c||}</script><p>这个指标有直观的解释。理想的情况下，我们希望$x_b-x_a=x_d-x_c$(例如，queen-king=actress-actor)。这就暗含着我们希望$x_b-x_a+x_c=x_d$。因此，我们确定可以最大化两个词向量之间的归一化点积的向量$x_d$即可（即余弦相似度）。<br>它需要面对的问题 :  </p><ul><li>通过加法后的余弦距离是否能很好地捕捉到直观的语义和句法类比问题来评估单词向量</li><li>从搜索中丢弃输入的单词</li><li>如果有信息但不是线性的怎么办？</li></ul><p>下面是Glove可视化效果 ： </p><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image10.png" alt></p><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image11.png" alt></p><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image12.png" alt></p><blockquote><p>可以使用数据集评估语法和语义上的效果</p><h3 id="Analogy-evaluation-and-hyperparameters"><a href="#Analogy-evaluation-and-hyperparameters" class="headerlink" title="Analogy evaluation and hyperparameters"></a>Analogy evaluation and hyperparameters</h3><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image13.png" alt></p><ul><li>300是一个很好的词向量维度</li><li>不对称上下文(只使用单侧的单词)不是很好，但是这在下游任务重可能不同</li><li>window size设为8对Glove向量来说比较好</li><li>分析 : window size设为2的时候实际上有效的，并且对于句法分析是更好的，因为句法效果非常局部</li></ul></blockquote><p>关于字嵌入的维度 :<br>利用矩阵摄动理论，揭示了词嵌入维数选择的基本的偏差与方法的权衡，当持续增大词向量维度的时候，词向量的效果不会一直变差并且会保持平稳</p><p><img src="/2020/04/13/Word%20Vectors%202%20and%20Word%20Senses/image14.png" alt></p><ul><li>训练时间越长越好</li><li>数据集越大越好，并且维基百科数据集比新闻文本数据集要好(因为维基百科就是在解释概念以及他们之间的相互关联，更多的说明性文本显示了事物之间的所有联系，而新闻并不去解释，而只是去阐述一些事件)</li></ul><h2 id="Word-senses-and-word-sense-ambiguity"><a href="#Word-senses-and-word-sense-ambiguity" class="headerlink" title="Word senses and word sense ambiguity"></a>Word senses and word sense ambiguity</h2><p>大多数单词都是多义的  </p><ul><li>特别是常见单词</li><li>特别是存在已久的单词</li></ul><h3 id="Improving-Word-Representations-Via-Global-Context-And-Multiple-Word-Prototypes-Huang-et-al-2012"><a href="#Improving-Word-Representations-Via-Global-Context-And-Multiple-Word-Prototypes-Huang-et-al-2012" class="headerlink" title="Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)"></a>Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)</h3><p>将常用词的所有上下文进行聚类，通过该词得到一些清晰的簇，从而将这个常用词分解为多个单词，例如bank_1, bank_2, bank_3<br>虽然这很粗糙，并且有时sensors之间的划分也不是很明确甚至相互重叠</p><h3 id="Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy"><a href="#Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy" class="headerlink" title="Linear Algebraic Structure of Word Senses, with Applications to Polysemy"></a>Linear Algebraic Structure of Word Senses, with Applications to Polysemy</h3><p>单词在标准单词嵌入(如word2vec)中的不同含义以线性叠加(加权和)的形式存在，$f$指频率 : </p><script type="math/tex; mode=display">v_{pike}=\alpha_1v_{pike_1}+\alpha_2v_{pike_2}+\alpha_3v_{pike_3}</script><script type="math/tex; mode=display">\alpha_1=\frac{f_1}{f_1+f_2+f_3}</script><p>令人惊讶的结果，只是加权平均值就已经可以获得很好的效果 : </p><ul><li>由于从稀疏编码中得到的概念，你实际上可以将感官分离出来(前提是它们相对比较常见)</li><li>可以理解为由于单词存在于高维的向量空间之中，不同的纬度所包含的含义是不同的，所以加权平均值并不会损害单词在不同含义所属的纬度上存储的信息</li></ul>]]></content>
      
      
      <categories>
          
          <category> cs224n学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs224n课程 </tag>
            
            <tag> nlp </tag>
            
            <tag> word2vec </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01 Introduction and Word Vectors</title>
      <link href="/2020/04/06/Introduction%20and%20Word%20Vectors/"/>
      <url>/2020/04/06/Introduction%20and%20Word%20Vectors/</url>
      
        <content type="html"><![CDATA[<h2 id="Human-language-and-word-meaning"><a href="#Human-language-and-word-meaning" class="headerlink" title="Human language and word meaning"></a>Human language and word meaning</h2><h3 id="How-do-we-represent-the-meaning-of-a-word"><a href="#How-do-we-represent-the-meaning-of-a-word" class="headerlink" title="How do we represent the meaning of a word?"></a>How do we represent the meaning of a word?</h3><ul><li>用一个词、词组等表示的概念</li><li>一个人想用语言、符号等来表达的想法</li><li>表达在作品、艺术等方面的思想</li></ul><p>理解意义的最普遍的语言方式(<strong>linguistic way</strong>) : 语言符号与语言符号的意义的转化</p><h3 id="How-do-we-have-usable-meaning-in-a-computer"><a href="#How-do-we-have-usable-meaning-in-a-computer" class="headerlink" title="How do we have usable meaning in a computer?"></a>How do we have usable meaning in a computer?</h3><h4 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h4><p>它是一个包含同义词集和上位词(“is a”关系)<strong>synonym sets and hypernyms</strong>的列表的辞典  </p><p><img src="/2020/04/06/Introduction%20and%20Word%20Vectors/image01.png" alt>  </p><p>缺点 : 这种表达方式忽略了词在上下文中的语境，缺少新的含义，判断比较主观且无法计算单词间的相似度。</p><h4 id="离散向量"><a href="#离散向量" class="headerlink" title="离散向量"></a>离散向量</h4><p>最常见的入one-hot编码，传统的自然语言处理中，我们把词语看作离散的符号。单词可以通过独热向量(one-hot vectors，只有一个1，其余均为0的稀疏向量) 。向量维度=词汇量(如500,000)。  </p><script type="math/tex; mode=display">motel = [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]</script><script type="math/tex; mode=display">hotel = [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0]</script><p>缺点 :  所有向量是正交的。对于独热向量，没有关于相似性概念，并且向量维度过大。</p><h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><ul><li>一个单词的意思是由经常出现在它附近的单词给出的。</li><li>当一个单词$w$出现在文本中时，它的上下文是出现在其附近的一组单词(在一个固定大小的窗口中)。</li><li>使用$w$的上下文来构建$w$的表示</li></ul><p><img src="/2020/04/06/Introduction%20and%20Word%20Vectors/image02.png" alt></p><h2 id="Word2vec-introduction"><a href="#Word2vec-introduction" class="headerlink" title="Word2vec introduction"></a>Word2vec introduction</h2><p>为每个单词构建一个密集的向量，使其与出现在相似上下文中的单词向量相似，词向量<strong>word vectors</strong>有时被称为词嵌入<strong>word embeddings</strong>或词表示<strong>word representations</strong>  </p><p><img src="/2020/04/06/Introduction%20and%20Word%20Vectors/formula01.png" alt></p><p>Word2vec是一个学习单词向量的框架，它的思路如下 :</p><ul><li>有大量的文本</li><li>固定词汇表中的每个单词都由一个向量表示</li><li>文本中的每个位置$t$，其中有一个中心词$c$和上下文(“外部”)单词$o$</li><li>使用$c$和$o$的词向量的相似性来计算给定$c$的$o$的概率(反之亦然)</li><li>不断调整词向量来最大化这个概率</li></ul><p>下图是窗口大小为$j=2$时的$P(w_{t+j}|w_t)$计算过程，其中center word分别为<strong><em>into</em></strong>和<strong><em>banking</em></strong></p><p><img src="/2020/04/06/Introduction%20and%20Word%20Vectors/image03.png" alt></p><p><img src="/2020/04/06/Introduction%20and%20Word%20Vectors/image04.png" alt></p><h2 id="Word2vec-objective-function"><a href="#Word2vec-objective-function" class="headerlink" title="Word2vec objective function"></a>Word2vec objective function</h2><p>对于每个位置$t=1,…,T$，在大小为m的固定窗口内预测上下文单词，给定中心词$w_j$</p><script type="math/tex; mode=display">L(\theta)=\prod_{t=1}^{T}\prod_{-m\leq j\leq m,j\neq 0}P(w_{t+j}|w_{t};\theta)</script><blockquote><p>其中，$\theta$为所有需要优化的变量</p></blockquote><p>损失函数$J(\theta)$是平均负对数似然</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{T}logL(\theta)=-\frac{1}{T}\prod_{t=1}^{T}\prod_{-m\leq j\leq m,j\neq 0}logP(w_{t+j}|w_{t};\theta)</script><p>其中log形式是方便将连乘转化为求和，负号是希望将极大化似然率转化为极小化损失函数的等价问题。</p><blockquote><p>连乘转求和 : </p><script type="math/tex; mode=display">log\prod_{i}x_{i}=\sum_{i}logx_{i}</script></blockquote><p>上述的$J(\theta)$可以看到我们要让损失函数最小则是让预测更准，即$P(w_{t+j}|w_{t};\theta)$越大</p><p>怎么计算$P(w_{t+j}|w_{t};\theta)$？<br>对于每个单词都是用两个向量，设当单词$w$为中心词的时候的向量为$v_w$，当单词$w$为上下文的某个词的时候向量为$u_w$，$V$为vocab，于是对于一个中心词$c$和一个上下文词$o$有 : </p><script type="math/tex; mode=display">P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}</script><blockquote><p>公式中，向量$u_o$和向量$v_c$进行点乘。向量之间越相似，点乘结果越大，从而归一化后得到的概率值也越大。模型的训练正是为了使得具有相似上下文的单词，具有相似的向量。</p></blockquote><h2 id="Word2vec-prediction-function"><a href="#Word2vec-prediction-function" class="headerlink" title="Word2vec prediction function"></a>Word2vec prediction function</h2><script type="math/tex; mode=display">P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}</script><p>上述公式公式中 :   </p><ul><li>取幂使任何数都为正</li><li>点积比较$o$和$c$的相似性，点积越大则概率越大</li><li>分母对整个词汇表进行标准化，从而给出概率分布</li></ul><p>将公式做实数集的同等映射$\mathbb{R}^n\rightarrow (0,1)^n$<br><strong>softmax function</strong> : </p><script type="math/tex; mode=display">softmax(x_i)=\frac{exp(x_i)}{\sum_{j=1}^nexp(x_j)}=p_i</script><p>将任意值$x_i$映射到概率分布$p_i$</p><ul><li>max : 因为放大了最大的概率</li><li>soft : 因为仍然为较小的$x_i$赋予了一定概率</li><li>在深度学习中常用</li></ul><p><img src="/2020/04/06/Introduction%20and%20Word%20Vectors/image05.png" alt></p><p>随机初始化$u_x\in \mathbb{R}^d$和$v_w\in \mathbb{R}^d$，使用梯度下降法进行公式推导</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial}{\partial v_clog(o|c)}&=\frac{\partial}{\partial v_c}log\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}\\&=\frac{\partial}{\partial v_c}(logexp(u_o^Tv_c)-log\sum_{w\in V}exp(u_w^Tv_c))\\&=\frac{\partial}{\partial v_c}(u_o^Tv_c-log\sum_{w\in V}exp(u_w^Tv_c))\\&=u_o-\frac{\sum_{w\in V}exp(u_w^Tv_c)u_w}{\sum_{w\in V}exp(u_w^Tv_c)}\end{aligned}</script><blockquote><p>偏导数可以 移进求和中，对应上方公式的最后两行的推导 : </p><script type="math/tex; mode=display">\frac{\partial}{\partial x}\sum_iy_i=\sum_i\frac{\partial}{\partial x}y_i</script></blockquote><p>对上述结果重新排列如下 : </p><script type="math/tex; mode=display">\begin{aligned}u_o-\frac{\sum_{w\in V}exp(u_w^Tv_c)u_w}{\sum_{w\in V}exp(u_w^Tv_c)}&=u_o-\sum_{w\in V} \frac{exp(u_w^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}u_w\\&=u_o-\sum_{w\in V}P(w|c)u_w\end{aligned}</script><p>第一项$u_0$是真正的上下文单词，第二项是预测的上下文单词。使用梯度下降法，模型的预测上下文将逐步接近真正的上下文。</p><p>对$u_0$进行偏微分计算(其中$u_o$是$u_w=o$的简写)，可知 : </p><blockquote><script type="math/tex; mode=display">\frac{\partial}{\partial u_o}\sum_{w\in V}u_w^Tv_c=\frac{\partial}{\partial u_o}u_o^Tv_c=v_c</script></blockquote><script type="math/tex; mode=display">\begin{aligned}\frac{\partial}{\partial u_o}logP(o|c)&=\frac{\partial}{\partial u_o}log\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}\\&=\frac{\partial}{\partial u_o}(logexp(u_o^Tv_c)-log\sum_{w\in V}exp(u_w^Tv_c))\\&=\frac{\partial}{\partial u_o}(u_o^Tv_c-log\sum_{w\in V}exp(u_w^Tv_c))\\&=v_c-\frac{\sum_{w\in V}\frac{\partial}{\partial u_o}exp(u_w^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}\\&=v_c-\frac{exp(u_o^Tv_c)v_c}{\sum_{w\in V}exp(u_w^Tv_c)}\\&=v_c-\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}v_c\\&=v_c-P(o|c)v_c\\&=(1-P(o|c))v_c\end{aligned}</script><p>从以上式子可看出，当$P(o|c) \rightarrow1$，即通过中心词$c$我们可以正确预测上下文词$o$，此时不需要调整$u_o$，反之，则要调整。</p><h2 id="Notes-01-Introduction-SVD-and-Word2Vec"><a href="#Notes-01-Introduction-SVD-and-Word2Vec" class="headerlink" title="Notes 01 Introduction, SVD and Word2Vec"></a>Notes 01 Introduction, SVD and Word2Vec</h2><h3 id="How-to-represent-words"><a href="#How-to-represent-words" class="headerlink" title="How to represent words?"></a>How to represent words?</h3><h4 id="Word-Vectors"><a href="#Word-Vectors" class="headerlink" title="Word Vectors"></a>Word Vectors</h4><p>使用词向量编码单词，N维空间足够我们编码语言的所有语义，每一维度都会编码一些我们使用语言传递的信息。简单的one-hot向量无法给出单词间的相似性，我们需要将维度$|V|$减少至一个低纬度的子空间，来获得稠密的词向量，获得词之间的关系</p><h4 id="SVD-Based-Methods"><a href="#SVD-Based-Methods" class="headerlink" title="SVD Based Methods"></a>SVD Based Methods</h4><p>首先遍历一个很大的数据集和统计词的共现计数矩阵$X$，然后对矩阵$X$进行SVD分解得到$USV^T$。然后使用$U$的行来作为字典中所有词的词向量。<br>缺点 : </p><ul><li>增加新的单词和语料库的大小会改变矩阵的维度</li><li>矩阵会非常的稀疏，因为很多词不会共现</li><li>矩阵维度会非常高</li><li>基于 SVD 的方法的计算复杂度很高</li><li>需要对$X$加入一些技巧处理来解决词频的极剧的不平衡<h4 id="Iteration-Based-Methods-Word2vec"><a href="#Iteration-Based-Methods-Word2vec" class="headerlink" title="Iteration Based Methods - Word2vec"></a>Iteration Based Methods - Word2vec</h4>我们创建一个模型，该模型能够一次学习一个迭代，并最终对给定上下文的单词的概率进行编码，而不是像上面的方法存储一些大型数据集的全局信息。<br>我们设计一个模型，该模型的参数就是词向量。然后根据一个目标函数训练模型，在每次模型的迭代计算误差，并遵循一些更新规则，该规则具有惩罚造成错误的模型参数的作用，从而可以学习到词向量。这个方法可以追溯到1986年，我们称这个方法为“反向传播”，模型和任务越简单，训练它的速度就越快。基于迭代的方法一次捕获一个单词的共现情况，而不是像SVD方法那样直接捕获所有的共现计数。<br>已经很多人按照这个思路测试了不同的方法。[Collobert et al., 2011]设计的模型首先将每个单词转换为向量。对每个特定的任务（命名实体识别、词性标注等等），他们不仅训练模型的参数，同时也训练单词向量，计算出了非常好的词向量的同时取得了很好的性能。<br>Word2vec 是一个软件包实际上包含 : </li><li><strong>两个算法</strong> ：continuous bag-of-words（CBOW）和skip-gram。CBOW是根据中心词周围的上下文单词来预测该词的词向量。skip-gram则相反，是根据中心词预测周围上下文的词的概率分布。</li><li><strong>两个训练方法</strong> ：negative sampling和hierarchical softmax。Negative sampling通过抽取负样本来定义目标，hierarchical softmax通过使用一个有效的树结构来计算所有词的概率来定义目标。</li></ul><p>该部分引出了Word2vec相关的知识点，基础知识通过<a href="https://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="noopener">刘建平博客</a>学习过。</p>]]></content>
      
      
      <categories>
          
          <category> cs224n学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs224n课程 </tag>
            
            <tag> nlp </tag>
            
            <tag> word2vec </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
